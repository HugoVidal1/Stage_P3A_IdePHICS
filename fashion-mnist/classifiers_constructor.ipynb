{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from utils import mnist_reader\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 18\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 10]) torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "# Importation des données\n",
    "x_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "x_valid, y_valid = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(x_train, dtype=dtype), torch.tensor(y_train, dtype=dtype), torch.tensor(x_valid, dtype=dtype), torch.tensor(y_valid,dtype=dtype)\n",
    "\n",
    "# Modification du format des données shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros(y_train_raw.shape[0], 10)\n",
    "for i,y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros(y_valid_raw.shape[0], 10)\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "number_of_classes_reduction = False\n",
    "class_binary_reduction = False\n",
    "\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "if class_binary_reduction :\n",
    "    determination_des_classes = True\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "            \n",
    "        # For square images\n",
    "        class_list = [x.reshape(int(np.sqrt(len(x))),int(np.sqrt(len(x)))) for x in class_list]\n",
    "        for i, x in enumerate(class_list) :\n",
    "            plt.subplot(2, len(class_list)//2+1, i+1)\n",
    "            plt.imshow(x, cmap='gray')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "            \n",
    "    classe1 = [0, 2, 4]\n",
    "    classe2 = [5, 7, 9]\n",
    "\n",
    "    # Création des masques pour les échantillons appartenant à ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient à classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient à classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concernés\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Création du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "    \n",
    "if number_of_classes_reduction:\n",
    "    number_of_classes = 5\n",
    "    classes = torch.linspace(0,number_of_classes-1, number_of_classes, dtype = int)\n",
    "    mask_train , mask_valid = y_train[:,classes].sum(1) > 0, y_valid[:,classes].sum(1) > 0\n",
    "    print(mask_train.shape)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train][:, classes]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid][:, classes]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  cpu\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else :\n",
    "#     device = torch.device(\"cpu\")\n",
    "# print(\"Computing on : \", device)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        self.observation_by_class = False\n",
    "        \n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        self.classes_accuracies_trajectories = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True, observation_by_class = False):\n",
    "        \n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        \n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.observation_by_class = observation_by_class\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                if observation_by_class :\n",
    "                    accuracy_by_class = []\n",
    "                    for classe in range(self.number_of_classes): \n",
    "                        correct_class_pred = ((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)) & (torch.argmax(y_valid, dim=1) == classe)).sum()\n",
    "                        total_class_data = (torch.argmax(y_valid, dim=1) == classe).sum()\n",
    "                        class_accuracy = correct_class_pred/total_class_data\n",
    "                        accuracy_by_class.append(class_accuracy.numpy())\n",
    "                    self.classes_accuracies_trajectories.append(accuracy_by_class)  \n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item(), 'Accuracies by class', accuracy_by_class)\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.observation_by_class = False\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        self.classes_accuracies_trajectories = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, observation_by_class = False):\n",
    "        \n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        \n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "       \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                if observation_by_class :\n",
    "                    accuracy_by_class = []\n",
    "                    for classe in range(self.number_of_classes): \n",
    "                        correct_class_pred = ((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)) & (torch.argmax(y_valid, dim=1) == classe)).sum()\n",
    "                        total_class_data = (torch.argmax(y_valid, dim=1) == classe).sum()\n",
    "                        class_accuracy = correct_class_pred/total_class_data\n",
    "                        accuracy_by_class.append(class_accuracy.numpy())\n",
    "                    self.classes_accuracies_trajectories.append(accuracy_by_class)  \n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item(), 'Accuracies by class', accuracy_by_class)\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            #print(output[0:5], z2[0:5], h1[0:5], z1[0:5])\n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            if i == 1000:\n",
    "                break\n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            if i == 1000:\n",
    "                break\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36584c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(784, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57236d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-6, 0, 0, 1e-1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(784, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd3755e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained = binary_classification_three_layer_NN(784, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(784,512,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 102442.\n",
      "Iteration 0 Training loss 0.0940396636724472 Validation loss 0.09468485414981842 Accuracy 0.040300000458955765 Accuracies by class [array(0., dtype=float32), array(0.043, dtype=float32), array(0.266, dtype=float32), array(0.003, dtype=float32), array(0., dtype=float32), array(0.06, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.017, dtype=float32), array(0.014, dtype=float32)]\n",
      "Iteration 10 Training loss 0.08379991352558136 Validation loss 0.08423139899969101 Accuracy 0.15289999544620514 Accuracies by class [array(0., dtype=float32), array(0.992, dtype=float32), array(0.422, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.114, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20 Training loss 0.07413873821496964 Validation loss 0.07525823265314102 Accuracy 0.241799995303154 Accuracies by class [array(0., dtype=float32), array(0.92, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.541, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30 Training loss 0.07058655470609665 Validation loss 0.07218378037214279 Accuracy 0.27549999952316284 Accuracies by class [array(0., dtype=float32), array(0.935, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.837, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 40 Training loss 0.0724896565079689 Validation loss 0.07168867439031601 Accuracy 0.2806999981403351 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.91, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.916, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 50 Training loss 0.0693068727850914 Validation loss 0.0710887685418129 Accuracy 0.2870999872684479 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 60 Training loss 0.07269352674484253 Validation loss 0.070867620408535 Accuracy 0.289000004529953 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 70 Training loss 0.07173898071050644 Validation loss 0.0711606815457344 Accuracy 0.28700000047683716 Accuracies by class [array(0., dtype=float32), array(0.919, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 80 Training loss 0.06768537312746048 Validation loss 0.07062381505966187 Accuracy 0.29170000553131104 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 90 Training loss 0.07055754959583282 Validation loss 0.07082661986351013 Accuracy 0.2903999984264374 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 100 Training loss 0.07374708354473114 Validation loss 0.07082533091306686 Accuracy 0.28940001130104065 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 110 Training loss 0.07222148030996323 Validation loss 0.07075190544128418 Accuracy 0.29109999537467957 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 120 Training loss 0.07250526547431946 Validation loss 0.07111646980047226 Accuracy 0.28760001063346863 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 130 Training loss 0.07201827317476273 Validation loss 0.07100372761487961 Accuracy 0.2883000075817108 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.924, dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 140 Training loss 0.0678393766283989 Validation loss 0.07083908468484879 Accuracy 0.2903999984264374 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 150 Training loss 0.07262644916772842 Validation loss 0.07072422653436661 Accuracy 0.29170000553131104 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 160 Training loss 0.06974378228187561 Validation loss 0.07066997140645981 Accuracy 0.29190000891685486 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 170 Training loss 0.06704492121934891 Validation loss 0.07072950899600983 Accuracy 0.2915000021457672 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 180 Training loss 0.06994453072547913 Validation loss 0.07059799134731293 Accuracy 0.2930000126361847 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 190 Training loss 0.07100217789411545 Validation loss 0.0705283135175705 Accuracy 0.29330000281333923 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 200 Training loss 0.0705416351556778 Validation loss 0.07075753808021545 Accuracy 0.29120001196861267 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 210 Training loss 0.06901794672012329 Validation loss 0.07054246962070465 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 220 Training loss 0.07039889693260193 Validation loss 0.07048150151968002 Accuracy 0.2939000129699707 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 230 Training loss 0.07055006176233292 Validation loss 0.0706929862499237 Accuracy 0.2922999858856201 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 240 Training loss 0.07279904186725616 Validation loss 0.07049710303544998 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 250 Training loss 0.0740320011973381 Validation loss 0.07060946524143219 Accuracy 0.29280000925064087 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 260 Training loss 0.07012053579092026 Validation loss 0.07050496339797974 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 270 Training loss 0.07292956858873367 Validation loss 0.0705554187297821 Accuracy 0.29330000281333923 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 280 Training loss 0.06856425851583481 Validation loss 0.07043532282114029 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 290 Training loss 0.0712558701634407 Validation loss 0.07045741379261017 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 300 Training loss 0.06776291131973267 Validation loss 0.07061079144477844 Accuracy 0.2928999960422516 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 310 Training loss 0.06979651004076004 Validation loss 0.0705440416932106 Accuracy 0.2937000095844269 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 320 Training loss 0.07187316566705704 Validation loss 0.07063599675893784 Accuracy 0.29269999265670776 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 330 Training loss 0.06940259784460068 Validation loss 0.07048087567090988 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 340 Training loss 0.06965712457895279 Validation loss 0.07049255073070526 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 350 Training loss 0.06996721774339676 Validation loss 0.07065919786691666 Accuracy 0.29249998927116394 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 360 Training loss 0.07254710048437119 Validation loss 0.07047682255506516 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 370 Training loss 0.07169549912214279 Validation loss 0.07095751911401749 Accuracy 0.2890999913215637 Accuracies by class [array(0., dtype=float32), array(0.906, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 380 Training loss 0.07064161449670792 Validation loss 0.07070736587047577 Accuracy 0.29109999537467957 Accuracies by class [array(0., dtype=float32), array(0.926, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 390 Training loss 0.07100924849510193 Validation loss 0.07070275396108627 Accuracy 0.290800005197525 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 400 Training loss 0.06948236376047134 Validation loss 0.07044749706983566 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 410 Training loss 0.07086525112390518 Validation loss 0.07042308151721954 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 420 Training loss 0.07335404306650162 Validation loss 0.07061472535133362 Accuracy 0.29120001196861267 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 430 Training loss 0.06970445066690445 Validation loss 0.07098343968391418 Accuracy 0.2881999909877777 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 440 Training loss 0.07043922692537308 Validation loss 0.07042995095252991 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 450 Training loss 0.06889698654413223 Validation loss 0.07056988030672073 Accuracy 0.29319998621940613 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 460 Training loss 0.0730983093380928 Validation loss 0.07034742832183838 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 470 Training loss 0.07155300676822662 Validation loss 0.07053928822278976 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 480 Training loss 0.06938128173351288 Validation loss 0.07043656706809998 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 490 Training loss 0.06879745423793793 Validation loss 0.07050059735774994 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 500 Training loss 0.07037059962749481 Validation loss 0.07038548588752747 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 510 Training loss 0.06958425045013428 Validation loss 0.07043248414993286 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 520 Training loss 0.06684236973524094 Validation loss 0.07057616114616394 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 530 Training loss 0.07095472514629364 Validation loss 0.07055013626813889 Accuracy 0.29350000619888306 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 540 Training loss 0.07258360087871552 Validation loss 0.07061038911342621 Accuracy 0.29269999265670776 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 550 Training loss 0.07278710603713989 Validation loss 0.07137636840343475 Accuracy 0.2856000065803528 Accuracies by class [array(0., dtype=float32), array(0.871, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 560 Training loss 0.0703272670507431 Validation loss 0.07128828763961792 Accuracy 0.2865000069141388 Accuracies by class [array(0., dtype=float32), array(0.88, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 570 Training loss 0.06946184486150742 Validation loss 0.07059723138809204 Accuracy 0.29339998960494995 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 580 Training loss 0.06797429174184799 Validation loss 0.07054905593395233 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 590 Training loss 0.07105555385351181 Validation loss 0.07039043307304382 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 600 Training loss 0.06702501326799393 Validation loss 0.07037945836782455 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 610 Training loss 0.06747307628393173 Validation loss 0.070524200797081 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 620 Training loss 0.07138761132955551 Validation loss 0.07043039798736572 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 630 Training loss 0.06868013739585876 Validation loss 0.07032742351293564 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 640 Training loss 0.0728079155087471 Validation loss 0.07053416222333908 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 650 Training loss 0.07191849499940872 Validation loss 0.0706167072057724 Accuracy 0.29330000281333923 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 660 Training loss 0.07221214473247528 Validation loss 0.0705222636461258 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 670 Training loss 0.07089883089065552 Validation loss 0.07043664902448654 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 680 Training loss 0.06761480122804642 Validation loss 0.07039634883403778 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 690 Training loss 0.07166516035795212 Validation loss 0.07036450505256653 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 700 Training loss 0.07390861958265305 Validation loss 0.07035720348358154 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 710 Training loss 0.07279814779758453 Validation loss 0.07037366926670074 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 720 Training loss 0.07178093492984772 Validation loss 0.07043187320232391 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 730 Training loss 0.07291727513074875 Validation loss 0.07039005309343338 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 740 Training loss 0.06663062423467636 Validation loss 0.07058936357498169 Accuracy 0.29350000619888306 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 750 Training loss 0.06765314936637878 Validation loss 0.07050403952598572 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 760 Training loss 0.07063902914524078 Validation loss 0.07049954682588577 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 770 Training loss 0.07057569921016693 Validation loss 0.07041998207569122 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 780 Training loss 0.07249680906534195 Validation loss 0.07061531394720078 Accuracy 0.2930999994277954 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 790 Training loss 0.0678560882806778 Validation loss 0.0705670639872551 Accuracy 0.29350000619888306 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 800 Training loss 0.07047130167484283 Validation loss 0.07046516984701157 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 810 Training loss 0.06879657506942749 Validation loss 0.07038368284702301 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 820 Training loss 0.0706394836306572 Validation loss 0.0705176368355751 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 830 Training loss 0.07078418135643005 Validation loss 0.07058356702327728 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 840 Training loss 0.07377822697162628 Validation loss 0.07049921154975891 Accuracy 0.29429998993873596 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 850 Training loss 0.07259254157543182 Validation loss 0.07063531875610352 Accuracy 0.29280000925064087 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 860 Training loss 0.07066331803798676 Validation loss 0.07045350223779678 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 870 Training loss 0.07226938009262085 Validation loss 0.07052561640739441 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 880 Training loss 0.07065953314304352 Validation loss 0.07067759335041046 Accuracy 0.2922999858856201 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 890 Training loss 0.07160940766334534 Validation loss 0.07074620574712753 Accuracy 0.29170000553131104 Accuracies by class [array(0., dtype=float32), array(0.93, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 900 Training loss 0.06864771246910095 Validation loss 0.07044333964586258 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 910 Training loss 0.07178284972906113 Validation loss 0.0703488364815712 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 920 Training loss 0.06970076262950897 Validation loss 0.07039374858140945 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 930 Training loss 0.06923971325159073 Validation loss 0.07064155489206314 Accuracy 0.2930999994277954 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 940 Training loss 0.07028613239526749 Validation loss 0.07060413807630539 Accuracy 0.29350000619888306 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 950 Training loss 0.06976878643035889 Validation loss 0.0704975426197052 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 960 Training loss 0.07060848921537399 Validation loss 0.07035430520772934 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 970 Training loss 0.06873117387294769 Validation loss 0.07040634006261826 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 980 Training loss 0.06714189052581787 Validation loss 0.070534847676754 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 990 Training loss 0.07015394419431686 Validation loss 0.07058142125606537 Accuracy 0.2937000095844269 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1000 Training loss 0.07252991944551468 Validation loss 0.07053298503160477 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1010 Training loss 0.07148963212966919 Validation loss 0.07045795768499374 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1020 Training loss 0.06995734572410583 Validation loss 0.07033942639827728 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1030 Training loss 0.07062669098377228 Validation loss 0.07047007977962494 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1040 Training loss 0.06875485181808472 Validation loss 0.07041388750076294 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1050 Training loss 0.06697072833776474 Validation loss 0.07048361003398895 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1060 Training loss 0.0717211663722992 Validation loss 0.07040673494338989 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1070 Training loss 0.06982573866844177 Validation loss 0.07040423899888992 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1080 Training loss 0.068453848361969 Validation loss 0.07037676870822906 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1090 Training loss 0.07094401121139526 Validation loss 0.07035710662603378 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1100 Training loss 0.0709676444530487 Validation loss 0.07044421881437302 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1110 Training loss 0.06969574093818665 Validation loss 0.07041089981794357 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1120 Training loss 0.07138217985630035 Validation loss 0.07033749669790268 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1130 Training loss 0.07024773210287094 Validation loss 0.07037664949893951 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1140 Training loss 0.07349254935979843 Validation loss 0.07041099667549133 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1150 Training loss 0.07041243463754654 Validation loss 0.07035523653030396 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1160 Training loss 0.07129261642694473 Validation loss 0.07048731297254562 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1170 Training loss 0.07114765793085098 Validation loss 0.07058730721473694 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1180 Training loss 0.0714634507894516 Validation loss 0.07073277980089188 Accuracy 0.2919999957084656 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1190 Training loss 0.0691470131278038 Validation loss 0.07056277990341187 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1200 Training loss 0.0692325010895729 Validation loss 0.07047256082296371 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1210 Training loss 0.06979745626449585 Validation loss 0.07055188715457916 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1220 Training loss 0.07064279913902283 Validation loss 0.07069596648216248 Accuracy 0.2922999858856201 Accuracies by class [array(0., dtype=float32), array(0.927, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1230 Training loss 0.07192570716142654 Validation loss 0.07046763598918915 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1240 Training loss 0.069785937666893 Validation loss 0.07038246840238571 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1250 Training loss 0.06923417001962662 Validation loss 0.07038022577762604 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1260 Training loss 0.07014113664627075 Validation loss 0.07038530707359314 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1270 Training loss 0.06983531266450882 Validation loss 0.07038817554712296 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1280 Training loss 0.07021172344684601 Validation loss 0.0703825056552887 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1290 Training loss 0.0734085962176323 Validation loss 0.07050551474094391 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1300 Training loss 0.06965219229459763 Validation loss 0.07049710303544998 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1310 Training loss 0.07068700343370438 Validation loss 0.07044132053852081 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1320 Training loss 0.07225743681192398 Validation loss 0.07047276943922043 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1330 Training loss 0.07140037417411804 Validation loss 0.07048171758651733 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1340 Training loss 0.07225371152162552 Validation loss 0.07046762853860855 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1350 Training loss 0.06791258603334427 Validation loss 0.07053322345018387 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1360 Training loss 0.06706390529870987 Validation loss 0.07034159451723099 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1370 Training loss 0.07168983668088913 Validation loss 0.07047929614782333 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1380 Training loss 0.07228383421897888 Validation loss 0.07039128988981247 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1390 Training loss 0.07332494854927063 Validation loss 0.07040423899888992 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1400 Training loss 0.06997353583574295 Validation loss 0.07041678577661514 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1410 Training loss 0.07249642163515091 Validation loss 0.0704224556684494 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1420 Training loss 0.06844676285982132 Validation loss 0.0704064667224884 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1430 Training loss 0.07071897387504578 Validation loss 0.07044494897127151 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1440 Training loss 0.07231003046035767 Validation loss 0.07041231542825699 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1450 Training loss 0.0691438764333725 Validation loss 0.07045898586511612 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1460 Training loss 0.07127057760953903 Validation loss 0.07061931490898132 Accuracy 0.29330000281333923 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1470 Training loss 0.07046034932136536 Validation loss 0.070542111992836 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1480 Training loss 0.070933498442173 Validation loss 0.07040324062108994 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1490 Training loss 0.07127910107374191 Validation loss 0.0704924538731575 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1500 Training loss 0.07262583076953888 Validation loss 0.07040145248174667 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1510 Training loss 0.06914076954126358 Validation loss 0.07040080428123474 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1520 Training loss 0.07323885709047318 Validation loss 0.07043781131505966 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1530 Training loss 0.06909202039241791 Validation loss 0.07038546353578568 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1540 Training loss 0.0707802101969719 Validation loss 0.07043062150478363 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1550 Training loss 0.0738719180226326 Validation loss 0.07046292722225189 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1560 Training loss 0.07061779499053955 Validation loss 0.0704655572772026 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1570 Training loss 0.06788938492536545 Validation loss 0.07041529566049576 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1580 Training loss 0.07027370482683182 Validation loss 0.07050305604934692 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1590 Training loss 0.0721164122223854 Validation loss 0.07044108211994171 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1600 Training loss 0.06942309439182281 Validation loss 0.07043389230966568 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1610 Training loss 0.07180683314800262 Validation loss 0.07044465839862823 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1620 Training loss 0.06906375288963318 Validation loss 0.07047106325626373 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1630 Training loss 0.07029999047517776 Validation loss 0.07059802114963531 Accuracy 0.29330000281333923 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1640 Training loss 0.06962279975414276 Validation loss 0.07060965150594711 Accuracy 0.29339998960494995 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1650 Training loss 0.06782841682434082 Validation loss 0.07065676152706146 Accuracy 0.29280000925064087 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1660 Training loss 0.07081294804811478 Validation loss 0.07076417654752731 Accuracy 0.29190000891685486 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1670 Training loss 0.07279551774263382 Validation loss 0.07064402103424072 Accuracy 0.29319998621940613 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1680 Training loss 0.0710805356502533 Validation loss 0.0706707239151001 Accuracy 0.2928999960422516 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1690 Training loss 0.0688830092549324 Validation loss 0.07047710567712784 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1700 Training loss 0.07060690969228745 Validation loss 0.07047861814498901 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1710 Training loss 0.0694253146648407 Validation loss 0.07040373980998993 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1720 Training loss 0.07046206295490265 Validation loss 0.07055287808179855 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1730 Training loss 0.07106571644544601 Validation loss 0.07042538374662399 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1740 Training loss 0.07207873463630676 Validation loss 0.07044023275375366 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1750 Training loss 0.07062538713216782 Validation loss 0.07042104750871658 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1760 Training loss 0.07127857208251953 Validation loss 0.0704665258526802 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1770 Training loss 0.06728224456310272 Validation loss 0.07056747376918793 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1780 Training loss 0.07227333635091782 Validation loss 0.07045754045248032 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1790 Training loss 0.07258613407611847 Validation loss 0.0704520046710968 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1800 Training loss 0.06747713685035706 Validation loss 0.07045809179544449 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1810 Training loss 0.07261724770069122 Validation loss 0.07040282338857651 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1820 Training loss 0.07298435270786285 Validation loss 0.070606529712677 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1830 Training loss 0.06796456128358841 Validation loss 0.07053516805171967 Accuracy 0.29429998993873596 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1840 Training loss 0.07228115946054459 Validation loss 0.07052311301231384 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1850 Training loss 0.07028022408485413 Validation loss 0.07060426473617554 Accuracy 0.29350000619888306 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1860 Training loss 0.0689212828874588 Validation loss 0.07058922946453094 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1870 Training loss 0.06999781727790833 Validation loss 0.07056719064712524 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1880 Training loss 0.07164241373538971 Validation loss 0.07078353315591812 Accuracy 0.2912999987602234 Accuracies by class [array(0., dtype=float32), array(0.92, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1890 Training loss 0.0712682381272316 Validation loss 0.07058017700910568 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1900 Training loss 0.06844311207532883 Validation loss 0.07050443440675735 Accuracy 0.29429998993873596 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1910 Training loss 0.06972192227840424 Validation loss 0.0703824907541275 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1920 Training loss 0.07078755646944046 Validation loss 0.0703834816813469 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1930 Training loss 0.06959917396306992 Validation loss 0.070396788418293 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1940 Training loss 0.07097077369689941 Validation loss 0.07039782404899597 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1950 Training loss 0.0692436620593071 Validation loss 0.07041388005018234 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1960 Training loss 0.07009737193584442 Validation loss 0.07037513703107834 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1970 Training loss 0.06911349296569824 Validation loss 0.07040335237979889 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1980 Training loss 0.07346222549676895 Validation loss 0.07040181010961533 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 1990 Training loss 0.06861859560012817 Validation loss 0.0703597292304039 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2000 Training loss 0.07268811762332916 Validation loss 0.07039936631917953 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2010 Training loss 0.06820846349000931 Validation loss 0.07037529349327087 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2020 Training loss 0.07131136953830719 Validation loss 0.0703851729631424 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2030 Training loss 0.07026831060647964 Validation loss 0.07039565593004227 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2040 Training loss 0.07125595211982727 Validation loss 0.07044428586959839 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2050 Training loss 0.06648458540439606 Validation loss 0.070388562977314 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2060 Training loss 0.07123389095067978 Validation loss 0.07053640484809875 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2070 Training loss 0.07103671878576279 Validation loss 0.07038234174251556 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2080 Training loss 0.07088374346494675 Validation loss 0.0703643336892128 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2090 Training loss 0.06862851977348328 Validation loss 0.07044494152069092 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2100 Training loss 0.07231414318084717 Validation loss 0.07049141824245453 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2110 Training loss 0.07088565826416016 Validation loss 0.07058198004961014 Accuracy 0.2937000095844269 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2120 Training loss 0.07266587764024734 Validation loss 0.07063645869493484 Accuracy 0.29319998621940613 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2130 Training loss 0.0704280361533165 Validation loss 0.07057918608188629 Accuracy 0.2937000095844269 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2140 Training loss 0.07312765717506409 Validation loss 0.07075761258602142 Accuracy 0.29179999232292175 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2150 Training loss 0.07112601399421692 Validation loss 0.0707797035574913 Accuracy 0.29159998893737793 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2160 Training loss 0.07427959889173508 Validation loss 0.07064579427242279 Accuracy 0.29319998621940613 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2170 Training loss 0.0692128986120224 Validation loss 0.07065893709659576 Accuracy 0.29280000925064087 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2180 Training loss 0.06931518018245697 Validation loss 0.07035927474498749 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2190 Training loss 0.07064707577228546 Validation loss 0.0703563243150711 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2200 Training loss 0.07057590782642365 Validation loss 0.07037824392318726 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2210 Training loss 0.07189327478408813 Validation loss 0.070349782705307 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2220 Training loss 0.07140304148197174 Validation loss 0.07034928351640701 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2230 Training loss 0.07169920206069946 Validation loss 0.07040520757436752 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2240 Training loss 0.06999564915895462 Validation loss 0.07054335623979568 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2250 Training loss 0.07379766553640366 Validation loss 0.07072655856609344 Accuracy 0.2921000123023987 Accuracies by class [array(0., dtype=float32), array(0.928, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2260 Training loss 0.06986492872238159 Validation loss 0.07050472497940063 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2270 Training loss 0.06912007927894592 Validation loss 0.07061512768268585 Accuracy 0.29319998621940613 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2280 Training loss 0.06923755258321762 Validation loss 0.07062619179487228 Accuracy 0.2928999960422516 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2290 Training loss 0.07111633569002151 Validation loss 0.0705932155251503 Accuracy 0.29339998960494995 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2300 Training loss 0.06893789023160934 Validation loss 0.07047943025827408 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2310 Training loss 0.07394891232252121 Validation loss 0.07091030478477478 Accuracy 0.2903999984264374 Accuracies by class [array(0., dtype=float32), array(0.909, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2320 Training loss 0.07307583838701248 Validation loss 0.07054803520441055 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2330 Training loss 0.07425379008054733 Validation loss 0.07061120122671127 Accuracy 0.29330000281333923 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2340 Training loss 0.07178932428359985 Validation loss 0.07069248706102371 Accuracy 0.29260000586509705 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2350 Training loss 0.06961632519960403 Validation loss 0.07080848515033722 Accuracy 0.29170000553131104 Accuracies by class [array(0., dtype=float32), array(0.926, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2360 Training loss 0.0692889541387558 Validation loss 0.07050271332263947 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2370 Training loss 0.07152155786752701 Validation loss 0.07052696496248245 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2380 Training loss 0.07527770847082138 Validation loss 0.07071438431739807 Accuracy 0.2922999858856201 Accuracies by class [array(0., dtype=float32), array(0.926, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2390 Training loss 0.06910092383623123 Validation loss 0.07067161053419113 Accuracy 0.29269999265670776 Accuracies by class [array(0., dtype=float32), array(0.93, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2400 Training loss 0.07012836635112762 Validation loss 0.07050739973783493 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2410 Training loss 0.069038525223732 Validation loss 0.07038825005292892 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2420 Training loss 0.06841190904378891 Validation loss 0.07043641805648804 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2430 Training loss 0.07126707583665848 Validation loss 0.07050221413373947 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2440 Training loss 0.0707201138138771 Validation loss 0.07042542099952698 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2450 Training loss 0.06993436813354492 Validation loss 0.07042457163333893 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2460 Training loss 0.07226625829935074 Validation loss 0.07041087746620178 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2470 Training loss 0.07501739263534546 Validation loss 0.0704866498708725 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2480 Training loss 0.0727972537279129 Validation loss 0.07043341547250748 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2490 Training loss 0.07076102495193481 Validation loss 0.0704764649271965 Accuracy 0.29429998993873596 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2500 Training loss 0.07305735349655151 Validation loss 0.07043558359146118 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2510 Training loss 0.07215818017721176 Validation loss 0.07041382044553757 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2520 Training loss 0.0681895837187767 Validation loss 0.0705253928899765 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2530 Training loss 0.07266656309366226 Validation loss 0.07050067186355591 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2540 Training loss 0.06989093869924545 Validation loss 0.07035651057958603 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2550 Training loss 0.07026942074298859 Validation loss 0.0704612284898758 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2560 Training loss 0.07104004174470901 Validation loss 0.07053510844707489 Accuracy 0.2939000129699707 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2570 Training loss 0.07220052182674408 Validation loss 0.07039009034633636 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2580 Training loss 0.0699184387922287 Validation loss 0.07047934830188751 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2590 Training loss 0.07210754603147507 Validation loss 0.07036794722080231 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2600 Training loss 0.06788701564073563 Validation loss 0.07036794722080231 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2610 Training loss 0.06908762454986572 Validation loss 0.07034040242433548 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2620 Training loss 0.07093042135238647 Validation loss 0.07034945487976074 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2630 Training loss 0.06827843934297562 Validation loss 0.07036115229129791 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2640 Training loss 0.07097359001636505 Validation loss 0.07046886533498764 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2650 Training loss 0.07069923728704453 Validation loss 0.07036656141281128 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2660 Training loss 0.07456174492835999 Validation loss 0.07036133855581284 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2670 Training loss 0.06895873695611954 Validation loss 0.07031635195016861 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2680 Training loss 0.07113990187644958 Validation loss 0.07034257054328918 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2690 Training loss 0.07382804155349731 Validation loss 0.07048046588897705 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2700 Training loss 0.07287131994962692 Validation loss 0.07051875442266464 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2710 Training loss 0.06881583482027054 Validation loss 0.07036467641592026 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2720 Training loss 0.07339267432689667 Validation loss 0.07035866379737854 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2730 Training loss 0.07273300737142563 Validation loss 0.07038480788469315 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2740 Training loss 0.06992866843938828 Validation loss 0.0703849270939827 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2750 Training loss 0.06962192803621292 Validation loss 0.07036574929952621 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2760 Training loss 0.06630164384841919 Validation loss 0.07035397738218307 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2770 Training loss 0.07130046933889389 Validation loss 0.07043560594320297 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2780 Training loss 0.0721219927072525 Validation loss 0.07036880403757095 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2790 Training loss 0.06946859508752823 Validation loss 0.0703815221786499 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2800 Training loss 0.07221890985965729 Validation loss 0.07033582776784897 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2810 Training loss 0.07328049093484879 Validation loss 0.07033134251832962 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2820 Training loss 0.07061539590358734 Validation loss 0.07034006714820862 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2830 Training loss 0.07097133994102478 Validation loss 0.07043316215276718 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2840 Training loss 0.07074802368879318 Validation loss 0.0704285278916359 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2850 Training loss 0.0721249058842659 Validation loss 0.07036510854959488 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2860 Training loss 0.06809242069721222 Validation loss 0.0703609436750412 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2870 Training loss 0.07079529017210007 Validation loss 0.07041622698307037 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2880 Training loss 0.06609132885932922 Validation loss 0.07033588737249374 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2890 Training loss 0.07162393629550934 Validation loss 0.0703558698296547 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2900 Training loss 0.07031553983688354 Validation loss 0.07035394757986069 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2910 Training loss 0.06702151149511337 Validation loss 0.07033146172761917 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2920 Training loss 0.0704372450709343 Validation loss 0.07034431397914886 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2930 Training loss 0.069580078125 Validation loss 0.07033082842826843 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2940 Training loss 0.0714598074555397 Validation loss 0.07045277208089828 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2950 Training loss 0.06478896737098694 Validation loss 0.07047067582607269 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2960 Training loss 0.06776799261569977 Validation loss 0.07045894861221313 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2970 Training loss 0.06773149967193604 Validation loss 0.0704750195145607 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2980 Training loss 0.07491736114025116 Validation loss 0.0704423114657402 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 2990 Training loss 0.07099823653697968 Validation loss 0.07059314101934433 Accuracy 0.2928999960422516 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3000 Training loss 0.0679377019405365 Validation loss 0.07044100761413574 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3010 Training loss 0.07265762239694595 Validation loss 0.07051678001880646 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3020 Training loss 0.07079300284385681 Validation loss 0.07046632468700409 Accuracy 0.29429998993873596 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3030 Training loss 0.07410087436437607 Validation loss 0.07032916694879532 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3040 Training loss 0.06936375796794891 Validation loss 0.07048162817955017 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3050 Training loss 0.06899602711200714 Validation loss 0.07041133940219879 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3060 Training loss 0.07203744351863861 Validation loss 0.07038778066635132 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3070 Training loss 0.06830282509326935 Validation loss 0.07039850950241089 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3080 Training loss 0.07108550518751144 Validation loss 0.07043801993131638 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3090 Training loss 0.07095331698656082 Validation loss 0.07054049521684647 Accuracy 0.29339998960494995 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3100 Training loss 0.070457823574543 Validation loss 0.0703996866941452 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3110 Training loss 0.07107958942651749 Validation loss 0.07037796825170517 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3120 Training loss 0.0709257423877716 Validation loss 0.07038570195436478 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3130 Training loss 0.06830695271492004 Validation loss 0.07039054483175278 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3140 Training loss 0.07092984765768051 Validation loss 0.07042758911848068 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3150 Training loss 0.06880450248718262 Validation loss 0.07051268219947815 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3160 Training loss 0.07210884243249893 Validation loss 0.07089287787675858 Accuracy 0.2904999852180481 Accuracies by class [array(0., dtype=float32), array(0.908, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3170 Training loss 0.07266411185264587 Validation loss 0.07081611454486847 Accuracy 0.2913999855518341 Accuracies by class [array(0., dtype=float32), array(0.917, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3180 Training loss 0.0716012641787529 Validation loss 0.07062967121601105 Accuracy 0.2928999960422516 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3190 Training loss 0.06945763528347015 Validation loss 0.0705668181180954 Accuracy 0.29350000619888306 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3200 Training loss 0.07116562873125076 Validation loss 0.07084783166646957 Accuracy 0.29100000858306885 Accuracies by class [array(0., dtype=float32), array(0.913, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3210 Training loss 0.069298155605793 Validation loss 0.07077734172344208 Accuracy 0.2915000021457672 Accuracies by class [array(0., dtype=float32), array(0.919, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3220 Training loss 0.06983862072229385 Validation loss 0.07066196203231812 Accuracy 0.29269999265670776 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3230 Training loss 0.067842997610569 Validation loss 0.07076331228017807 Accuracy 0.29100000858306885 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3240 Training loss 0.0702710673213005 Validation loss 0.07048909366130829 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3250 Training loss 0.07103133946657181 Validation loss 0.07045770436525345 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3260 Training loss 0.06977371871471405 Validation loss 0.07054035365581512 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3270 Training loss 0.06976806372404099 Validation loss 0.07040757685899734 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3280 Training loss 0.07042456418275833 Validation loss 0.07047918438911438 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3290 Training loss 0.07027148455381393 Validation loss 0.07082736492156982 Accuracy 0.29109999537467957 Accuracies by class [array(0., dtype=float32), array(0.914, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3300 Training loss 0.07046058028936386 Validation loss 0.07048393785953522 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3310 Training loss 0.06940217316150665 Validation loss 0.0705563947558403 Accuracy 0.2939000129699707 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3320 Training loss 0.06695464253425598 Validation loss 0.07053205370903015 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3330 Training loss 0.07247316092252731 Validation loss 0.07044924050569534 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3340 Training loss 0.07128898799419403 Validation loss 0.07041608542203903 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3350 Training loss 0.07249972224235535 Validation loss 0.07039660215377808 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3360 Training loss 0.06862886995077133 Validation loss 0.07040516287088394 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3370 Training loss 0.07033330947160721 Validation loss 0.07041273266077042 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3380 Training loss 0.07350409030914307 Validation loss 0.070368193089962 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3390 Training loss 0.07081713527441025 Validation loss 0.07045739889144897 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3400 Training loss 0.07411324232816696 Validation loss 0.07056592404842377 Accuracy 0.2939000129699707 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3410 Training loss 0.0673903301358223 Validation loss 0.07041160017251968 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3420 Training loss 0.06915448606014252 Validation loss 0.07039780914783478 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3430 Training loss 0.0706104263663292 Validation loss 0.07036615163087845 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3440 Training loss 0.06849868595600128 Validation loss 0.07038028538227081 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3450 Training loss 0.07056717574596405 Validation loss 0.0704009011387825 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3460 Training loss 0.06895098835229874 Validation loss 0.07040781527757645 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3470 Training loss 0.07064465433359146 Validation loss 0.07040929794311523 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3480 Training loss 0.07133264094591141 Validation loss 0.07045547664165497 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3490 Training loss 0.06740037351846695 Validation loss 0.07043321430683136 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3500 Training loss 0.06949994713068008 Validation loss 0.07037607580423355 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3510 Training loss 0.06926974654197693 Validation loss 0.0704101175069809 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3520 Training loss 0.06877709180116653 Validation loss 0.07039785385131836 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3530 Training loss 0.07099974900484085 Validation loss 0.07040301710367203 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3540 Training loss 0.06992144882678986 Validation loss 0.07039305567741394 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3550 Training loss 0.06794095039367676 Validation loss 0.07041440159082413 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3560 Training loss 0.06978919357061386 Validation loss 0.07035866379737854 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3570 Training loss 0.07361249625682831 Validation loss 0.07038864493370056 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3580 Training loss 0.0687982514500618 Validation loss 0.07037229835987091 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3590 Training loss 0.07376083731651306 Validation loss 0.07041363418102264 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3600 Training loss 0.06966166943311691 Validation loss 0.07045517861843109 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3610 Training loss 0.07125306874513626 Validation loss 0.07047892361879349 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3620 Training loss 0.06744358688592911 Validation loss 0.07047861069440842 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3630 Training loss 0.06880030035972595 Validation loss 0.07045751810073853 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3640 Training loss 0.07067864388227463 Validation loss 0.07033979147672653 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3650 Training loss 0.07093822956085205 Validation loss 0.07034022361040115 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3660 Training loss 0.06921080499887466 Validation loss 0.07034662365913391 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3670 Training loss 0.06782184541225433 Validation loss 0.07039710134267807 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3680 Training loss 0.07149630784988403 Validation loss 0.0703674703836441 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3690 Training loss 0.07066187262535095 Validation loss 0.0703432634472847 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3700 Training loss 0.07184091210365295 Validation loss 0.07034081965684891 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3710 Training loss 0.0692538321018219 Validation loss 0.07036682963371277 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3720 Training loss 0.07031011581420898 Validation loss 0.07029055804014206 Accuracy 0.2964000105857849 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3730 Training loss 0.07082726061344147 Validation loss 0.07038911432027817 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3740 Training loss 0.0708284080028534 Validation loss 0.07035385072231293 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3750 Training loss 0.06930222362279892 Validation loss 0.07037162035703659 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3760 Training loss 0.06906220316886902 Validation loss 0.07050453871488571 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3770 Training loss 0.06965760141611099 Validation loss 0.07046030461788177 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3780 Training loss 0.07099999487400055 Validation loss 0.07036992907524109 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3790 Training loss 0.07012462615966797 Validation loss 0.07035313546657562 Accuracy 0.296099990606308 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3800 Training loss 0.07012824714183807 Validation loss 0.07036300003528595 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3810 Training loss 0.06914623826742172 Validation loss 0.07035285979509354 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3820 Training loss 0.07114297896623611 Validation loss 0.07046911120414734 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3830 Training loss 0.07327715307474136 Validation loss 0.07055369019508362 Accuracy 0.2939000129699707 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3840 Training loss 0.07080283761024475 Validation loss 0.07057905197143555 Accuracy 0.2937000095844269 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3850 Training loss 0.07334613800048828 Validation loss 0.07057581096887589 Accuracy 0.2937000095844269 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3860 Training loss 0.06793361902236938 Validation loss 0.0705588087439537 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3870 Training loss 0.07228437066078186 Validation loss 0.07046054303646088 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3880 Training loss 0.06910631060600281 Validation loss 0.07036860287189484 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3890 Training loss 0.06918051093816757 Validation loss 0.0704435482621193 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3900 Training loss 0.07246112823486328 Validation loss 0.07044682651758194 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3910 Training loss 0.07079000025987625 Validation loss 0.07043501734733582 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3920 Training loss 0.07012493163347244 Validation loss 0.07045359164476395 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3930 Training loss 0.07165037840604782 Validation loss 0.07041925936937332 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3940 Training loss 0.07014910131692886 Validation loss 0.07046021521091461 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3950 Training loss 0.06898417323827744 Validation loss 0.07045561820268631 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3960 Training loss 0.06993255764245987 Validation loss 0.07045101374387741 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3970 Training loss 0.07311050593852997 Validation loss 0.07057929039001465 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3980 Training loss 0.0706380233168602 Validation loss 0.0706065222620964 Accuracy 0.29339998960494995 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 3990 Training loss 0.07008851319551468 Validation loss 0.07058258354663849 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4000 Training loss 0.06779694557189941 Validation loss 0.07058443874120712 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4010 Training loss 0.07345009595155716 Validation loss 0.07058828324079514 Accuracy 0.29350000619888306 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4020 Training loss 0.07049912214279175 Validation loss 0.07056106626987457 Accuracy 0.2939000129699707 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4030 Training loss 0.07147172093391418 Validation loss 0.07044646888971329 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4040 Training loss 0.07025261968374252 Validation loss 0.07045584917068481 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4050 Training loss 0.06780439615249634 Validation loss 0.07058931887149811 Accuracy 0.29350000619888306 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4060 Training loss 0.06929502636194229 Validation loss 0.07046473771333694 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4070 Training loss 0.06995682418346405 Validation loss 0.0704815685749054 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4080 Training loss 0.06607715785503387 Validation loss 0.07044639438390732 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4090 Training loss 0.0686846598982811 Validation loss 0.07038793712854385 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4100 Training loss 0.0651610866189003 Validation loss 0.07036245614290237 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4110 Training loss 0.06883009523153305 Validation loss 0.0703933909535408 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4120 Training loss 0.06724005192518234 Validation loss 0.07037819176912308 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4130 Training loss 0.07066552340984344 Validation loss 0.070381760597229 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4140 Training loss 0.07026486098766327 Validation loss 0.07035288959741592 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4150 Training loss 0.07029548287391663 Validation loss 0.07038748264312744 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4160 Training loss 0.06832200288772583 Validation loss 0.07033926248550415 Accuracy 0.296099990606308 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4170 Training loss 0.06671346724033356 Validation loss 0.07037069648504257 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4180 Training loss 0.06961285322904587 Validation loss 0.07040036469697952 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4190 Training loss 0.07012385874986649 Validation loss 0.07038946449756622 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4200 Training loss 0.07078664004802704 Validation loss 0.07033766061067581 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4210 Training loss 0.06939579546451569 Validation loss 0.0703800693154335 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4220 Training loss 0.06629262119531631 Validation loss 0.07040677964687347 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4230 Training loss 0.06793037056922913 Validation loss 0.0703464075922966 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4240 Training loss 0.07079189270734787 Validation loss 0.07034686207771301 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4250 Training loss 0.0686948224902153 Validation loss 0.07042431086301804 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4260 Training loss 0.06779296696186066 Validation loss 0.07042378932237625 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4270 Training loss 0.06840811669826508 Validation loss 0.07036499679088593 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4280 Training loss 0.07205511629581451 Validation loss 0.07044916599988937 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4290 Training loss 0.06893821060657501 Validation loss 0.07047897577285767 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4300 Training loss 0.07043156772851944 Validation loss 0.07050246745347977 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4310 Training loss 0.07081511616706848 Validation loss 0.0704365149140358 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4320 Training loss 0.06823252141475677 Validation loss 0.07042030990123749 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4330 Training loss 0.07130137085914612 Validation loss 0.07043487578630447 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4340 Training loss 0.06897109001874924 Validation loss 0.07043103873729706 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4350 Training loss 0.066421277821064 Validation loss 0.07043060660362244 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4360 Training loss 0.0712873712182045 Validation loss 0.07037094980478287 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4370 Training loss 0.07099663466215134 Validation loss 0.07041087001562119 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4380 Training loss 0.07194218039512634 Validation loss 0.07039012759923935 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4390 Training loss 0.06898614764213562 Validation loss 0.07040775567293167 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4400 Training loss 0.06496640294790268 Validation loss 0.07039783895015717 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4410 Training loss 0.06932222098112106 Validation loss 0.07040450721979141 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4420 Training loss 0.07027187198400497 Validation loss 0.07041767239570618 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4430 Training loss 0.06828933954238892 Validation loss 0.07037162780761719 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4440 Training loss 0.06978055089712143 Validation loss 0.07038654386997223 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4450 Training loss 0.0708853080868721 Validation loss 0.07038860023021698 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4460 Training loss 0.06812340021133423 Validation loss 0.07037472724914551 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4470 Training loss 0.06977886706590652 Validation loss 0.07045009732246399 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4480 Training loss 0.07181933522224426 Validation loss 0.07045141607522964 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4490 Training loss 0.07262284308671951 Validation loss 0.07036774605512619 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4500 Training loss 0.07246264815330505 Validation loss 0.0703980103135109 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4510 Training loss 0.07060111314058304 Validation loss 0.07049927860498428 Accuracy 0.2944999933242798 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4520 Training loss 0.07320345193147659 Validation loss 0.07058273255825043 Accuracy 0.2935999929904938 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4530 Training loss 0.06944923847913742 Validation loss 0.07066921144723892 Accuracy 0.2928999960422516 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4540 Training loss 0.07109332084655762 Validation loss 0.07051736861467361 Accuracy 0.29420000314712524 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4550 Training loss 0.07239960879087448 Validation loss 0.0704798623919487 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4560 Training loss 0.07158848643302917 Validation loss 0.07038506865501404 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4570 Training loss 0.07059185206890106 Validation loss 0.0711682066321373 Accuracy 0.28690001368522644 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.906, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4580 Training loss 0.0707072988152504 Validation loss 0.07073401659727097 Accuracy 0.29109999537467957 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4590 Training loss 0.06953376531600952 Validation loss 0.07045251876115799 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4600 Training loss 0.07020892202854156 Validation loss 0.07053972780704498 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4610 Training loss 0.0689958781003952 Validation loss 0.0705401748418808 Accuracy 0.2939000129699707 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4620 Training loss 0.07279296964406967 Validation loss 0.07053180783987045 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4630 Training loss 0.06958915293216705 Validation loss 0.07047350704669952 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4640 Training loss 0.07043509185314178 Validation loss 0.07040634751319885 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4650 Training loss 0.06974788755178452 Validation loss 0.07043565064668655 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4660 Training loss 0.0691482275724411 Validation loss 0.0704345703125 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4670 Training loss 0.07289334386587143 Validation loss 0.07054376602172852 Accuracy 0.29409998655319214 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4680 Training loss 0.07095086574554443 Validation loss 0.07046415656805038 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4690 Training loss 0.06999699026346207 Validation loss 0.07044489681720734 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4700 Training loss 0.06965500861406326 Validation loss 0.07043583691120148 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4710 Training loss 0.07332759350538254 Validation loss 0.07044006139039993 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4720 Training loss 0.07112912088632584 Validation loss 0.07052891701459885 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4730 Training loss 0.06742806732654572 Validation loss 0.07047007977962494 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4740 Training loss 0.07047845423221588 Validation loss 0.07042405754327774 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4750 Training loss 0.07124217599630356 Validation loss 0.07042934745550156 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4760 Training loss 0.07097389549016953 Validation loss 0.07037121802568436 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4770 Training loss 0.06824545562267303 Validation loss 0.07039526104927063 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4780 Training loss 0.07312162965536118 Validation loss 0.07039867341518402 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4790 Training loss 0.06816144287586212 Validation loss 0.07036439329385757 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4800 Training loss 0.07074818760156631 Validation loss 0.07037513703107834 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4810 Training loss 0.07241855561733246 Validation loss 0.07040082663297653 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4820 Training loss 0.07043382525444031 Validation loss 0.07044466584920883 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4830 Training loss 0.06958037614822388 Validation loss 0.07039700448513031 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4840 Training loss 0.07332081347703934 Validation loss 0.07046090811491013 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4850 Training loss 0.07383222132921219 Validation loss 0.07047824561595917 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4860 Training loss 0.0693330392241478 Validation loss 0.0703803300857544 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4870 Training loss 0.06916588544845581 Validation loss 0.07035025954246521 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4880 Training loss 0.07130225002765656 Validation loss 0.07040194422006607 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4890 Training loss 0.06815307587385178 Validation loss 0.07032183557748795 Accuracy 0.296099990606308 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4900 Training loss 0.0729513168334961 Validation loss 0.07031437754631042 Accuracy 0.296099990606308 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4910 Training loss 0.07102037966251373 Validation loss 0.07038775831460953 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4920 Training loss 0.06994646042585373 Validation loss 0.07035879790782928 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4930 Training loss 0.07090636342763901 Validation loss 0.07037029415369034 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4940 Training loss 0.06974251568317413 Validation loss 0.07033082097768784 Accuracy 0.2962000072002411 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4950 Training loss 0.07146286219358444 Validation loss 0.07035012543201447 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4960 Training loss 0.07345672696828842 Validation loss 0.07035872340202332 Accuracy 0.296099990606308 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4970 Training loss 0.07100329548120499 Validation loss 0.07035120576620102 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4980 Training loss 0.0695108100771904 Validation loss 0.07034821063280106 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 4990 Training loss 0.07308696210384369 Validation loss 0.07041558623313904 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5000 Training loss 0.07098926603794098 Validation loss 0.07038748264312744 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5010 Training loss 0.07298972457647324 Validation loss 0.07036902010440826 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5020 Training loss 0.0701279416680336 Validation loss 0.07039826363325119 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5030 Training loss 0.06949231028556824 Validation loss 0.07037225365638733 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5040 Training loss 0.07194291800260544 Validation loss 0.07037840038537979 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5050 Training loss 0.06760123372077942 Validation loss 0.07040820270776749 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5060 Training loss 0.07177147269248962 Validation loss 0.07040825486183167 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5070 Training loss 0.06781918555498123 Validation loss 0.07040264457464218 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5080 Training loss 0.07149405777454376 Validation loss 0.07039020210504532 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5090 Training loss 0.06911635398864746 Validation loss 0.07039497047662735 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5100 Training loss 0.06895279139280319 Validation loss 0.07046040892601013 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5110 Training loss 0.06812615692615509 Validation loss 0.07045119255781174 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5120 Training loss 0.06880273669958115 Validation loss 0.07036787271499634 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5130 Training loss 0.07362645119428635 Validation loss 0.07038591057062149 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5140 Training loss 0.07145354896783829 Validation loss 0.07040087133646011 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5150 Training loss 0.06842491775751114 Validation loss 0.07041747868061066 Accuracy 0.2953000068664551 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5160 Training loss 0.06896739453077316 Validation loss 0.07039320468902588 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5170 Training loss 0.07128030061721802 Validation loss 0.07038559764623642 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5180 Training loss 0.07291528582572937 Validation loss 0.0703706219792366 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5190 Training loss 0.06893530488014221 Validation loss 0.07041818648576736 Accuracy 0.2953999936580658 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5200 Training loss 0.06890367716550827 Validation loss 0.07054530084133148 Accuracy 0.2939999997615814 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5210 Training loss 0.07345037162303925 Validation loss 0.07067757099866867 Accuracy 0.29280000925064087 Accuracies by class [array(0., dtype=float32), array(0.935, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5220 Training loss 0.07066459953784943 Validation loss 0.07091561704874039 Accuracy 0.2906000018119812 Accuracies by class [array(0., dtype=float32), array(0.913, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5230 Training loss 0.06926897913217545 Validation loss 0.07064250856637955 Accuracy 0.2930999994277954 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5240 Training loss 0.07029518485069275 Validation loss 0.07060250639915466 Accuracy 0.2937000095844269 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5250 Training loss 0.07029584795236588 Validation loss 0.07047644257545471 Accuracy 0.2946999967098236 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5260 Training loss 0.07165328413248062 Validation loss 0.07064665853977203 Accuracy 0.29319998621940613 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5270 Training loss 0.07178045809268951 Validation loss 0.07057254761457443 Accuracy 0.2939000129699707 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5280 Training loss 0.070633664727211 Validation loss 0.07052147388458252 Accuracy 0.29440000653266907 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5290 Training loss 0.07248806208372116 Validation loss 0.0703527182340622 Accuracy 0.296099990606308 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5300 Training loss 0.06926462799310684 Validation loss 0.07031719386577606 Accuracy 0.2962000072002411 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5310 Training loss 0.0715445876121521 Validation loss 0.07033948600292206 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5320 Training loss 0.0734439268708229 Validation loss 0.0706382617354393 Accuracy 0.29280000925064087 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5330 Training loss 0.06797608733177185 Validation loss 0.07034201920032501 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5340 Training loss 0.06948602199554443 Validation loss 0.07032910734415054 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5350 Training loss 0.06941651552915573 Validation loss 0.07034225016832352 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5360 Training loss 0.07146372646093369 Validation loss 0.070327028632164 Accuracy 0.29600000381469727 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5370 Training loss 0.0716678574681282 Validation loss 0.07031382620334625 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5380 Training loss 0.06781274080276489 Validation loss 0.0703202411532402 Accuracy 0.2962000072002411 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5390 Training loss 0.07264394313097 Validation loss 0.07037211954593658 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5400 Training loss 0.0723172128200531 Validation loss 0.07034065574407578 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5410 Training loss 0.0741414874792099 Validation loss 0.07040692865848541 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5420 Training loss 0.07171715050935745 Validation loss 0.07056064158678055 Accuracy 0.29339998960494995 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5430 Training loss 0.07430870831012726 Validation loss 0.0704149678349495 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5440 Training loss 0.07116485387086868 Validation loss 0.07039030641317368 Accuracy 0.29510000348091125 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5450 Training loss 0.071119025349617 Validation loss 0.07035990804433823 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5460 Training loss 0.0694541335105896 Validation loss 0.0705915167927742 Accuracy 0.29339998960494995 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5470 Training loss 0.069442018866539 Validation loss 0.07035605609416962 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5480 Training loss 0.06688553839921951 Validation loss 0.07035505026578903 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5490 Training loss 0.07054445892572403 Validation loss 0.0703088790178299 Accuracy 0.2962999939918518 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5500 Training loss 0.06862738728523254 Validation loss 0.07032757252454758 Accuracy 0.2962000072002411 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5510 Training loss 0.06541342288255692 Validation loss 0.07040442526340485 Accuracy 0.295199990272522 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5520 Training loss 0.0717993676662445 Validation loss 0.07058093696832657 Accuracy 0.29319998621940613 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5530 Training loss 0.0686601847410202 Validation loss 0.07044155895709991 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5540 Training loss 0.07161468267440796 Validation loss 0.07043079286813736 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5550 Training loss 0.07196079194545746 Validation loss 0.07040707021951675 Accuracy 0.29499998688697815 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5560 Training loss 0.07256197184324265 Validation loss 0.07046546041965485 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5570 Training loss 0.07085355371236801 Validation loss 0.07041604816913605 Accuracy 0.2948000133037567 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5580 Training loss 0.07246112078428268 Validation loss 0.07044829428195953 Accuracy 0.2946000099182129 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5590 Training loss 0.07256756722927094 Validation loss 0.07050615549087524 Accuracy 0.2937999963760376 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5600 Training loss 0.07029154151678085 Validation loss 0.07064233720302582 Accuracy 0.2928999960422516 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5610 Training loss 0.06897351890802383 Validation loss 0.07046127319335938 Accuracy 0.29490000009536743 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5620 Training loss 0.06904376298189163 Validation loss 0.07051509618759155 Accuracy 0.29429998993873596 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5630 Training loss 0.06898687034845352 Validation loss 0.07039059698581696 Accuracy 0.2955000102519989 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5640 Training loss 0.07010513544082642 Validation loss 0.0703626349568367 Accuracy 0.2957000136375427 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5650 Training loss 0.07159887999296188 Validation loss 0.0703476145863533 Accuracy 0.29589998722076416 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5660 Training loss 0.07157409936189651 Validation loss 0.07038778066635132 Accuracy 0.2955999970436096 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5670 Training loss 0.07270663231611252 Validation loss 0.07032874971628189 Accuracy 0.29580000042915344 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5680 Training loss 0.0709613487124443 Validation loss 0.07023472338914871 Accuracy 0.2971000075340271 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.005, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5690 Training loss 0.0698341503739357 Validation loss 0.07021932303905487 Accuracy 0.296999990940094 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.006, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5700 Training loss 0.07111953943967819 Validation loss 0.0702364519238472 Accuracy 0.296999990940094 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.006, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5710 Training loss 0.07062549144029617 Validation loss 0.07023897767066956 Accuracy 0.2969000041484833 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.005, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5720 Training loss 0.07324133068323135 Validation loss 0.07025089859962463 Accuracy 0.29649999737739563 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.007, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5730 Training loss 0.06870289146900177 Validation loss 0.07025740295648575 Accuracy 0.29660001397132874 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.004, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5740 Training loss 0.0702209398150444 Validation loss 0.07022513449192047 Accuracy 0.2969000041484833 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.012, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5750 Training loss 0.07089826464653015 Validation loss 0.07030931115150452 Accuracy 0.2962999939918518 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.014, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5760 Training loss 0.06865619868040085 Validation loss 0.07029689848423004 Accuracy 0.2962000072002411 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.01, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5770 Training loss 0.07098126411437988 Validation loss 0.07020372152328491 Accuracy 0.2971000075340271 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.031, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5780 Training loss 0.06898176670074463 Validation loss 0.06985006481409073 Accuracy 0.30070000886917114 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.056, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5790 Training loss 0.06768352538347244 Validation loss 0.06958739459514618 Accuracy 0.301800012588501 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.07, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5800 Training loss 0.06654108315706253 Validation loss 0.06939306855201721 Accuracy 0.3050999939441681 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.163, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5810 Training loss 0.06829524040222168 Validation loss 0.06947483867406845 Accuracy 0.3037000000476837 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.107, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5820 Training loss 0.06674634665250778 Validation loss 0.06941324472427368 Accuracy 0.30410000681877136 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.087, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5830 Training loss 0.07211870700120926 Validation loss 0.07001804560422897 Accuracy 0.2987000048160553 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.05, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5840 Training loss 0.06644927710294724 Validation loss 0.06971484422683716 Accuracy 0.30160000920295715 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.073, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5850 Training loss 0.06683145463466644 Validation loss 0.06850340962409973 Accuracy 0.31360000371932983 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.223, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5860 Training loss 0.07157717645168304 Validation loss 0.06793998181819916 Accuracy 0.31869998574256897 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.262, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5870 Training loss 0.06445037573575974 Validation loss 0.06791722029447556 Accuracy 0.3197000026702881 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.248, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5880 Training loss 0.07143574953079224 Validation loss 0.06792116910219193 Accuracy 0.3199999928474426 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.246, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5890 Training loss 0.06994624435901642 Validation loss 0.0676095187664032 Accuracy 0.3230000138282776 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.28, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5900 Training loss 0.06528930366039276 Validation loss 0.06682402640581131 Accuracy 0.3303999900817871 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.39, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5910 Training loss 0.0669962465763092 Validation loss 0.06551522761583328 Accuracy 0.34389999508857727 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.507, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5920 Training loss 0.06751757860183716 Validation loss 0.06430258601903915 Accuracy 0.3549000024795532 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.642, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5930 Training loss 0.06450360268354416 Validation loss 0.06416027992963791 Accuracy 0.3571999967098236 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.628, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5940 Training loss 0.06383548676967621 Validation loss 0.06206072121858597 Accuracy 0.3783000111579895 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.847, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5950 Training loss 0.06257818639278412 Validation loss 0.061782728880643845 Accuracy 0.38019999861717224 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.886, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5960 Training loss 0.06672478467226028 Validation loss 0.06172354519367218 Accuracy 0.38119998574256897 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5970 Training loss 0.060840703547000885 Validation loss 0.06145154684782028 Accuracy 0.383899986743927 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5980 Training loss 0.06065049767494202 Validation loss 0.06164330989122391 Accuracy 0.3813000023365021 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.883, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 5990 Training loss 0.06211961805820465 Validation loss 0.061468418687582016 Accuracy 0.3846000134944916 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.91, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6000 Training loss 0.06130772829055786 Validation loss 0.06159299239516258 Accuracy 0.38260000944137573 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.89, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6010 Training loss 0.06149855628609657 Validation loss 0.06150095537304878 Accuracy 0.3837999999523163 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6020 Training loss 0.06053119897842407 Validation loss 0.061407070606946945 Accuracy 0.3846000134944916 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.919, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6030 Training loss 0.061371225863695145 Validation loss 0.061948247253894806 Accuracy 0.37869998812675476 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.872, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6040 Training loss 0.06054949387907982 Validation loss 0.06147352233529091 Accuracy 0.38339999318122864 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6050 Training loss 0.0620008185505867 Validation loss 0.06131134182214737 Accuracy 0.38530001044273376 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6060 Training loss 0.06144604831933975 Validation loss 0.06158532574772835 Accuracy 0.3824000060558319 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.909, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6070 Training loss 0.06431424617767334 Validation loss 0.06209658086299896 Accuracy 0.3783000111579895 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.841, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6080 Training loss 0.06120138615369797 Validation loss 0.06141898036003113 Accuracy 0.38519999384880066 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6090 Training loss 0.06183333694934845 Validation loss 0.06145657226443291 Accuracy 0.3846000134944916 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6100 Training loss 0.0604066476225853 Validation loss 0.061542171984910965 Accuracy 0.38370001316070557 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.9, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6110 Training loss 0.06427912414073944 Validation loss 0.061423398554325104 Accuracy 0.384799987077713 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.921, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6120 Training loss 0.05873452126979828 Validation loss 0.061370525509119034 Accuracy 0.38530001044273376 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6130 Training loss 0.06152043491601944 Validation loss 0.06129465252161026 Accuracy 0.3862000107765198 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6140 Training loss 0.06298813968896866 Validation loss 0.06139545887708664 Accuracy 0.3849000036716461 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.907, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6150 Training loss 0.06226818263530731 Validation loss 0.06142647936940193 Accuracy 0.38499999046325684 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.913, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6160 Training loss 0.05910623073577881 Validation loss 0.0613669827580452 Accuracy 0.3853999972343445 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.913, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6170 Training loss 0.06586629152297974 Validation loss 0.06130111217498779 Accuracy 0.3862999975681305 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6180 Training loss 0.06249995529651642 Validation loss 0.061314087361097336 Accuracy 0.3862999975681305 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6190 Training loss 0.057321324944496155 Validation loss 0.061304520815610886 Accuracy 0.3862000107765198 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6200 Training loss 0.060603175312280655 Validation loss 0.06125327944755554 Accuracy 0.38690000772476196 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6210 Training loss 0.06023327261209488 Validation loss 0.06123161315917969 Accuracy 0.3869999945163727 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6220 Training loss 0.0626911148428917 Validation loss 0.06119678542017937 Accuracy 0.38600000739097595 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6230 Training loss 0.05837738513946533 Validation loss 0.0611913725733757 Accuracy 0.38659998774528503 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6240 Training loss 0.05897609516978264 Validation loss 0.061260323971509933 Accuracy 0.38609999418258667 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6250 Training loss 0.06398945301771164 Validation loss 0.06117841973900795 Accuracy 0.3871999979019165 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6260 Training loss 0.058373015373945236 Validation loss 0.06120235472917557 Accuracy 0.3869999945163727 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6270 Training loss 0.061102479696273804 Validation loss 0.061282042413949966 Accuracy 0.3862000107765198 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.918, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6280 Training loss 0.06134384498000145 Validation loss 0.06147105619311333 Accuracy 0.38429999351501465 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.904, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6290 Training loss 0.06196742132306099 Validation loss 0.06126603111624718 Accuracy 0.38609999418258667 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.919, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6300 Training loss 0.061513498425483704 Validation loss 0.06151542067527771 Accuracy 0.3840999901294708 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.899, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6310 Training loss 0.06647377461194992 Validation loss 0.06187238171696663 Accuracy 0.3804999887943268 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.854, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6320 Training loss 0.06281435489654541 Validation loss 0.06156748905777931 Accuracy 0.382999986410141 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.911, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6330 Training loss 0.06164040043950081 Validation loss 0.06136658042669296 Accuracy 0.3849000036716461 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6340 Training loss 0.06204833835363388 Validation loss 0.06124131754040718 Accuracy 0.38580000400543213 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6350 Training loss 0.06085259094834328 Validation loss 0.061027754098176956 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6360 Training loss 0.062460824847221375 Validation loss 0.06113869696855545 Accuracy 0.38670000433921814 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6370 Training loss 0.054266251623630524 Validation loss 0.06111622229218483 Accuracy 0.38760000467300415 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6380 Training loss 0.06320086121559143 Validation loss 0.061204422265291214 Accuracy 0.38670000433921814 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6390 Training loss 0.06472909450531006 Validation loss 0.06212489306926727 Accuracy 0.37779998779296875 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.817, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6400 Training loss 0.06208533048629761 Validation loss 0.06126944720745087 Accuracy 0.38679999113082886 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6410 Training loss 0.0601632334291935 Validation loss 0.06131453439593315 Accuracy 0.38609999418258667 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.909, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6420 Training loss 0.05933072417974472 Validation loss 0.061154596507549286 Accuracy 0.3871999979019165 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6430 Training loss 0.06175381317734718 Validation loss 0.0615190789103508 Accuracy 0.38370001316070557 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.882, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6440 Training loss 0.062115129083395004 Validation loss 0.06162776052951813 Accuracy 0.3831000030040741 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.902, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6450 Training loss 0.060570403933525085 Validation loss 0.06192265450954437 Accuracy 0.38019999861717224 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.897, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6460 Training loss 0.06283310800790787 Validation loss 0.06133412569761276 Accuracy 0.38600000739097595 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6470 Training loss 0.05778365954756737 Validation loss 0.06132446229457855 Accuracy 0.38609999418258667 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6480 Training loss 0.06282646954059601 Validation loss 0.06121956929564476 Accuracy 0.38609999418258667 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6490 Training loss 0.05933140963315964 Validation loss 0.06137145683169365 Accuracy 0.38580000400543213 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6500 Training loss 0.06118270382285118 Validation loss 0.061176102608442307 Accuracy 0.3869999945163727 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6510 Training loss 0.060354817658662796 Validation loss 0.06151309609413147 Accuracy 0.3822000026702881 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6520 Training loss 0.06010538712143898 Validation loss 0.06114790216088295 Accuracy 0.38769999146461487 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6530 Training loss 0.06206941232085228 Validation loss 0.06116025522351265 Accuracy 0.38760000467300415 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6540 Training loss 0.05731413885951042 Validation loss 0.06113910675048828 Accuracy 0.3880000114440918 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6550 Training loss 0.05706007033586502 Validation loss 0.06114902347326279 Accuracy 0.387800008058548 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6560 Training loss 0.06277186423540115 Validation loss 0.0610126256942749 Accuracy 0.3889000117778778 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6570 Training loss 0.06289822608232498 Validation loss 0.061065614223480225 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6580 Training loss 0.06278906762599945 Validation loss 0.061134256422519684 Accuracy 0.3873000144958496 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6590 Training loss 0.058663107454776764 Validation loss 0.06106016784906387 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6600 Training loss 0.06515990942716599 Validation loss 0.06120658293366432 Accuracy 0.38690000772476196 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6610 Training loss 0.06154029443860054 Validation loss 0.061243388801813126 Accuracy 0.3865000009536743 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6620 Training loss 0.06257309764623642 Validation loss 0.06107158586382866 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6630 Training loss 0.0597233772277832 Validation loss 0.06116299703717232 Accuracy 0.38760000467300415 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6640 Training loss 0.0599081814289093 Validation loss 0.06107832118868828 Accuracy 0.3880000114440918 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6650 Training loss 0.058800291270017624 Validation loss 0.061069998890161514 Accuracy 0.387800008058548 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6660 Training loss 0.06183239817619324 Validation loss 0.06103600561618805 Accuracy 0.38769999146461487 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6670 Training loss 0.06352760642766953 Validation loss 0.06107154116034508 Accuracy 0.3873000144958496 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6680 Training loss 0.06385062634944916 Validation loss 0.06107241287827492 Accuracy 0.3880999982357025 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6690 Training loss 0.05866137519478798 Validation loss 0.061053402721881866 Accuracy 0.3880000114440918 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6700 Training loss 0.062208205461502075 Validation loss 0.061050672084093094 Accuracy 0.3878999948501587 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6710 Training loss 0.05851612240076065 Validation loss 0.061213139444589615 Accuracy 0.38679999113082886 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6720 Training loss 0.061891693621873856 Validation loss 0.061037577688694 Accuracy 0.3882000148296356 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6730 Training loss 0.06201699748635292 Validation loss 0.061002183705568314 Accuracy 0.3885999917984009 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6740 Training loss 0.059097595512866974 Validation loss 0.061039038002491 Accuracy 0.38830000162124634 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6750 Training loss 0.05956907197833061 Validation loss 0.06115780398249626 Accuracy 0.38760000467300415 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6760 Training loss 0.059945423156023026 Validation loss 0.0610048733651638 Accuracy 0.387800008058548 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6770 Training loss 0.06408900022506714 Validation loss 0.06183912605047226 Accuracy 0.37940001487731934 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.863, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6780 Training loss 0.06304489076137543 Validation loss 0.061127688735723495 Accuracy 0.3873000144958496 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6790 Training loss 0.0625840574502945 Validation loss 0.061008915305137634 Accuracy 0.38850000500679016 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6800 Training loss 0.06426400691270828 Validation loss 0.060933154076337814 Accuracy 0.38909998536109924 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6810 Training loss 0.06185392290353775 Validation loss 0.061087191104888916 Accuracy 0.3882000148296356 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6820 Training loss 0.059818558394908905 Validation loss 0.061212874948978424 Accuracy 0.3856000006198883 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6830 Training loss 0.06152616813778877 Validation loss 0.061258770525455475 Accuracy 0.385699987411499 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6840 Training loss 0.06074262037873268 Validation loss 0.06118649244308472 Accuracy 0.38659998774528503 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6850 Training loss 0.06083295866847038 Validation loss 0.061692975461483 Accuracy 0.3808000087738037 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6860 Training loss 0.06326185911893845 Validation loss 0.06142561137676239 Accuracy 0.38420000672340393 Accuracies by class [array(0., dtype=float32), array(0.927, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6870 Training loss 0.061594538390636444 Validation loss 0.06157698854804039 Accuracy 0.3831999897956848 Accuracies by class [array(0., dtype=float32), array(0.931, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6880 Training loss 0.05995379015803337 Validation loss 0.061582911759614944 Accuracy 0.38280001282691956 Accuracies by class [array(0., dtype=float32), array(0.92, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6890 Training loss 0.06104544922709465 Validation loss 0.061214398592710495 Accuracy 0.38679999113082886 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6900 Training loss 0.058222439140081406 Validation loss 0.0611451081931591 Accuracy 0.3873000144958496 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6910 Training loss 0.060690056532621384 Validation loss 0.06109630689024925 Accuracy 0.38769999146461487 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6920 Training loss 0.05715962126851082 Validation loss 0.0613103024661541 Accuracy 0.38530001044273376 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6930 Training loss 0.05938178673386574 Validation loss 0.06114184111356735 Accuracy 0.3871000111103058 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6940 Training loss 0.06013066694140434 Validation loss 0.06089556962251663 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6950 Training loss 0.060569945722818375 Validation loss 0.060987845063209534 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6960 Training loss 0.06206084042787552 Validation loss 0.06106487289071083 Accuracy 0.38760000467300415 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.918, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6970 Training loss 0.057786110788583755 Validation loss 0.06135893985629082 Accuracy 0.3840999901294708 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.897, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6980 Training loss 0.06174369156360626 Validation loss 0.06109738349914551 Accuracy 0.387800008058548 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 6990 Training loss 0.061482321470975876 Validation loss 0.061303719878196716 Accuracy 0.38609999418258667 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7000 Training loss 0.06095889210700989 Validation loss 0.06122172251343727 Accuracy 0.3871000111103058 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7010 Training loss 0.06059708818793297 Validation loss 0.06124653294682503 Accuracy 0.38690000772476196 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7020 Training loss 0.060463763773441315 Validation loss 0.061191149055957794 Accuracy 0.3873000144958496 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7030 Training loss 0.06246291100978851 Validation loss 0.06114746630191803 Accuracy 0.38749998807907104 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.918, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7040 Training loss 0.060011714696884155 Validation loss 0.06097177416086197 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7050 Training loss 0.058292683213949203 Validation loss 0.06091492250561714 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7060 Training loss 0.057628411799669266 Validation loss 0.06096982955932617 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7070 Training loss 0.06656423211097717 Validation loss 0.06094614416360855 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7080 Training loss 0.06486566364765167 Validation loss 0.06098581850528717 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7090 Training loss 0.06398002803325653 Validation loss 0.06113344803452492 Accuracy 0.3874000012874603 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7100 Training loss 0.05729652941226959 Validation loss 0.06100725010037422 Accuracy 0.3880000114440918 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7110 Training loss 0.05800790339708328 Validation loss 0.06095287203788757 Accuracy 0.38920000195503235 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7120 Training loss 0.06294160336256027 Validation loss 0.06091304123401642 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7130 Training loss 0.06285244971513748 Validation loss 0.06101240590214729 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7140 Training loss 0.06183583661913872 Validation loss 0.06246005743741989 Accuracy 0.37470000982284546 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.782, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7150 Training loss 0.06258665770292282 Validation loss 0.06180461868643761 Accuracy 0.3813999891281128 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.859, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7160 Training loss 0.06116928160190582 Validation loss 0.06091940402984619 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7170 Training loss 0.060420408844947815 Validation loss 0.060963068157434464 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7180 Training loss 0.061332788318395615 Validation loss 0.06094737350940704 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7190 Training loss 0.05577673390507698 Validation loss 0.060899168252944946 Accuracy 0.39010000228881836 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7200 Training loss 0.058174263685941696 Validation loss 0.06105268374085426 Accuracy 0.3882000148296356 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7210 Training loss 0.05974883586168289 Validation loss 0.06106999143958092 Accuracy 0.3878999948501587 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7220 Training loss 0.060327477753162384 Validation loss 0.0611298643052578 Accuracy 0.38690000772476196 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7230 Training loss 0.057834915816783905 Validation loss 0.06111055612564087 Accuracy 0.38760000467300415 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7240 Training loss 0.061599619686603546 Validation loss 0.06222120299935341 Accuracy 0.375900000333786 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.866, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7250 Training loss 0.05884307995438576 Validation loss 0.061084095388650894 Accuracy 0.3880999982357025 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7260 Training loss 0.05720798298716545 Validation loss 0.06104316562414169 Accuracy 0.387800008058548 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7270 Training loss 0.06047585979104042 Validation loss 0.060986194759607315 Accuracy 0.38830000162124634 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7280 Training loss 0.062322042882442474 Validation loss 0.06099903956055641 Accuracy 0.3889000117778778 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7290 Training loss 0.06103463098406792 Validation loss 0.06087415665388107 Accuracy 0.38989999890327454 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7300 Training loss 0.05873428285121918 Validation loss 0.06082643195986748 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7310 Training loss 0.05905180796980858 Validation loss 0.06087080016732216 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7320 Training loss 0.06473615765571594 Validation loss 0.060936033725738525 Accuracy 0.3894999921321869 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7330 Training loss 0.05809443071484566 Validation loss 0.060971297323703766 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7340 Training loss 0.0603644922375679 Validation loss 0.06103067472577095 Accuracy 0.3889000117778778 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7350 Training loss 0.060051850974559784 Validation loss 0.06092669814825058 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7360 Training loss 0.06229909136891365 Validation loss 0.06090435013175011 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7370 Training loss 0.059379782527685165 Validation loss 0.06107315421104431 Accuracy 0.38760000467300415 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7380 Training loss 0.06074327975511551 Validation loss 0.06087411940097809 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7390 Training loss 0.0644294023513794 Validation loss 0.062723308801651 Accuracy 0.3702000081539154 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.893, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.845, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7400 Training loss 0.06072219833731651 Validation loss 0.060963936150074005 Accuracy 0.3887999951839447 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7410 Training loss 0.05804239958524704 Validation loss 0.060897234827280045 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7420 Training loss 0.06121502071619034 Validation loss 0.06104759871959686 Accuracy 0.38769999146461487 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7430 Training loss 0.05935868248343468 Validation loss 0.0617511086165905 Accuracy 0.38019999861717224 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.894, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7440 Training loss 0.05951981619000435 Validation loss 0.06101388856768608 Accuracy 0.3880999982357025 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7450 Training loss 0.059855274856090546 Validation loss 0.06101854890584946 Accuracy 0.38850000500679016 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7460 Training loss 0.06482438743114471 Validation loss 0.0609901063144207 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7470 Training loss 0.06316874176263809 Validation loss 0.06136879697442055 Accuracy 0.3846000134944916 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7480 Training loss 0.06089705228805542 Validation loss 0.06143871694803238 Accuracy 0.38350000977516174 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7490 Training loss 0.0619346909224987 Validation loss 0.060921914875507355 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7500 Training loss 0.06179686635732651 Validation loss 0.06085612252354622 Accuracy 0.38989999890327454 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7510 Training loss 0.06391169130802155 Validation loss 0.06097566708922386 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7520 Training loss 0.06324036419391632 Validation loss 0.06099094823002815 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7530 Training loss 0.05750640481710434 Validation loss 0.06103011220693588 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7540 Training loss 0.056312572211027145 Validation loss 0.06112656742334366 Accuracy 0.38679999113082886 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7550 Training loss 0.06336913257837296 Validation loss 0.06101264804601669 Accuracy 0.3882000148296356 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7560 Training loss 0.06182710826396942 Validation loss 0.06095445156097412 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7570 Training loss 0.061272673308849335 Validation loss 0.06103726476430893 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7580 Training loss 0.06032074615359306 Validation loss 0.061001669615507126 Accuracy 0.3882000148296356 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7590 Training loss 0.0591459646821022 Validation loss 0.060867469757795334 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7600 Training loss 0.06130286678671837 Validation loss 0.06086542829871178 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7610 Training loss 0.06345324963331223 Validation loss 0.06091560050845146 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7620 Training loss 0.059578750282526016 Validation loss 0.06097822263836861 Accuracy 0.388700008392334 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7630 Training loss 0.05845244228839874 Validation loss 0.060991089791059494 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7640 Training loss 0.06266236305236816 Validation loss 0.06096843630075455 Accuracy 0.38909998536109924 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7650 Training loss 0.06053030490875244 Validation loss 0.06090296059846878 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7660 Training loss 0.06574059277772903 Validation loss 0.060958605259656906 Accuracy 0.38920000195503235 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7670 Training loss 0.0599081926047802 Validation loss 0.060951873660087585 Accuracy 0.3894999921321869 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7680 Training loss 0.06242765113711357 Validation loss 0.06109023466706276 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7690 Training loss 0.05810496583580971 Validation loss 0.060910049825906754 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7700 Training loss 0.058161355555057526 Validation loss 0.06089676916599274 Accuracy 0.3898000121116638 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7710 Training loss 0.06028011068701744 Validation loss 0.06090357527136803 Accuracy 0.38989999890327454 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7720 Training loss 0.06157912686467171 Validation loss 0.06097143143415451 Accuracy 0.38909998536109924 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7730 Training loss 0.06419491022825241 Validation loss 0.06104461848735809 Accuracy 0.387800008058548 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7740 Training loss 0.06162981316447258 Validation loss 0.06131861358880997 Accuracy 0.3847000002861023 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7750 Training loss 0.05896754935383797 Validation loss 0.06095687672495842 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7760 Training loss 0.06067459285259247 Validation loss 0.06088901311159134 Accuracy 0.39010000228881836 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7770 Training loss 0.06108810380101204 Validation loss 0.06113554164767265 Accuracy 0.38760000467300415 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7780 Training loss 0.060111187398433685 Validation loss 0.061062347143888474 Accuracy 0.3880000114440918 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7790 Training loss 0.0633733868598938 Validation loss 0.06104455888271332 Accuracy 0.38830000162124634 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7800 Training loss 0.059940796345472336 Validation loss 0.061099741607904434 Accuracy 0.3882000148296356 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7810 Training loss 0.062489449977874756 Validation loss 0.061035703867673874 Accuracy 0.3885999917984009 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7820 Training loss 0.05970963090658188 Validation loss 0.06096624955534935 Accuracy 0.38920000195503235 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7830 Training loss 0.06234701722860336 Validation loss 0.06095563620328903 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7840 Training loss 0.06245226040482521 Validation loss 0.06091303378343582 Accuracy 0.3901999890804291 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7850 Training loss 0.06112412363290787 Validation loss 0.06098303571343422 Accuracy 0.3894999921321869 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7860 Training loss 0.06273671239614487 Validation loss 0.060963232070207596 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7870 Training loss 0.06156512349843979 Validation loss 0.06092841923236847 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7880 Training loss 0.05893728509545326 Validation loss 0.060998544096946716 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7890 Training loss 0.06230820342898369 Validation loss 0.061037514358758926 Accuracy 0.3878999948501587 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7900 Training loss 0.057204682379961014 Validation loss 0.060886915773153305 Accuracy 0.3898000121116638 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7910 Training loss 0.06164485588669777 Validation loss 0.061312511563301086 Accuracy 0.3855000138282776 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7920 Training loss 0.05947739630937576 Validation loss 0.060867466032505035 Accuracy 0.39010000228881836 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7930 Training loss 0.05920102447271347 Validation loss 0.060890082269907 Accuracy 0.38989999890327454 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7940 Training loss 0.05819504335522652 Validation loss 0.060882069170475006 Accuracy 0.38989999890327454 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7950 Training loss 0.06185248866677284 Validation loss 0.06085982918739319 Accuracy 0.3898000121116638 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7960 Training loss 0.06027847155928612 Validation loss 0.06095351651310921 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7970 Training loss 0.06196129694581032 Validation loss 0.061241514980793 Accuracy 0.38690000772476196 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7980 Training loss 0.060470789670944214 Validation loss 0.061331212520599365 Accuracy 0.38499999046325684 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 7990 Training loss 0.06160788610577583 Validation loss 0.060950104147195816 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8000 Training loss 0.06487451493740082 Validation loss 0.06094787269830704 Accuracy 0.38920000195503235 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8010 Training loss 0.059580933302640915 Validation loss 0.06103351712226868 Accuracy 0.3880000114440918 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8020 Training loss 0.06389481574296951 Validation loss 0.0616774819791317 Accuracy 0.3824999928474426 Accuracies by class [array(0., dtype=float32), array(0.995, dtype=float32), array(0.883, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8030 Training loss 0.060577891767024994 Validation loss 0.06118958443403244 Accuracy 0.3873000144958496 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8040 Training loss 0.06174593046307564 Validation loss 0.06131149083375931 Accuracy 0.3853999972343445 Accuracies by class [array(0., dtype=float32), array(0.992, dtype=float32), array(0.896, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8050 Training loss 0.062034763395786285 Validation loss 0.06099037081003189 Accuracy 0.3889000117778778 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8060 Training loss 0.05887999385595322 Validation loss 0.0609738752245903 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8070 Training loss 0.0598372258245945 Validation loss 0.06092827022075653 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8080 Training loss 0.059146132320165634 Validation loss 0.061074502766132355 Accuracy 0.38850000500679016 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8090 Training loss 0.060956235975027084 Validation loss 0.06108491122722626 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8100 Training loss 0.06115630269050598 Validation loss 0.06102918088436127 Accuracy 0.3887999951839447 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8110 Training loss 0.0593569241464138 Validation loss 0.060907550156116486 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8120 Training loss 0.06202422082424164 Validation loss 0.06086860969662666 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8130 Training loss 0.0606059767305851 Validation loss 0.06099040061235428 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8140 Training loss 0.056862615048885345 Validation loss 0.06147238239645958 Accuracy 0.382999986410141 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.919, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8150 Training loss 0.0610215850174427 Validation loss 0.06121252104640007 Accuracy 0.38609999418258667 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8160 Training loss 0.061934687197208405 Validation loss 0.061013150960206985 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8170 Training loss 0.060458626598119736 Validation loss 0.061313528567552567 Accuracy 0.38589999079704285 Accuracies by class [array(0., dtype=float32), array(0.992, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8180 Training loss 0.059993352741003036 Validation loss 0.061041995882987976 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8190 Training loss 0.06477300077676773 Validation loss 0.06104929372668266 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.991, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8200 Training loss 0.0654197409749031 Validation loss 0.06106076017022133 Accuracy 0.3880999982357025 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8210 Training loss 0.06123097613453865 Validation loss 0.061049044132232666 Accuracy 0.3882000148296356 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8220 Training loss 0.06151590868830681 Validation loss 0.06103485822677612 Accuracy 0.3880000114440918 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8230 Training loss 0.057073067873716354 Validation loss 0.061052147299051285 Accuracy 0.38830000162124634 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8240 Training loss 0.05765537545084953 Validation loss 0.06105165183544159 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.993, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8250 Training loss 0.06002957001328468 Validation loss 0.06088331714272499 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8260 Training loss 0.05977719649672508 Validation loss 0.06086931750178337 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8270 Training loss 0.06007479503750801 Validation loss 0.06078394129872322 Accuracy 0.3910999894142151 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8280 Training loss 0.0605507418513298 Validation loss 0.06101573258638382 Accuracy 0.3878999948501587 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8290 Training loss 0.06404229253530502 Validation loss 0.06110003590583801 Accuracy 0.38749998807907104 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8300 Training loss 0.06294073164463043 Validation loss 0.06080503761768341 Accuracy 0.39070001244544983 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8310 Training loss 0.060227733105421066 Validation loss 0.06102130189538002 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8320 Training loss 0.061046671122312546 Validation loss 0.06084365397691727 Accuracy 0.3898000121116638 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8330 Training loss 0.06297501921653748 Validation loss 0.06239054724574089 Accuracy 0.3734000027179718 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.817, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8340 Training loss 0.060399629175662994 Validation loss 0.061124321073293686 Accuracy 0.3874000012874603 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8350 Training loss 0.061065323650836945 Validation loss 0.061292972415685654 Accuracy 0.38530001044273376 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8360 Training loss 0.0599113330245018 Validation loss 0.06106152385473251 Accuracy 0.3878999948501587 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8370 Training loss 0.06257478892803192 Validation loss 0.06107017397880554 Accuracy 0.3880999982357025 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8380 Training loss 0.0634024515748024 Validation loss 0.061221715062856674 Accuracy 0.38600000739097595 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8390 Training loss 0.06336904317140579 Validation loss 0.06094614416360855 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8400 Training loss 0.058675073087215424 Validation loss 0.06100241839885712 Accuracy 0.38850000500679016 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8410 Training loss 0.06308544427156448 Validation loss 0.06147276237607002 Accuracy 0.3840000033378601 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.883, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8420 Training loss 0.06310820579528809 Validation loss 0.06136660277843475 Accuracy 0.3849000036716461 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.896, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8430 Training loss 0.0573355108499527 Validation loss 0.061065781861543655 Accuracy 0.3882000148296356 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8440 Training loss 0.06158747524023056 Validation loss 0.061350252479314804 Accuracy 0.3853999972343445 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.9, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8450 Training loss 0.06485500931739807 Validation loss 0.06135185435414314 Accuracy 0.3853999972343445 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.905, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8460 Training loss 0.06243735924363136 Validation loss 0.060999296605587006 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8470 Training loss 0.061843495815992355 Validation loss 0.06103026494383812 Accuracy 0.38830000162124634 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8480 Training loss 0.06538113206624985 Validation loss 0.06091103330254555 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8490 Training loss 0.06372379511594772 Validation loss 0.060903944075107574 Accuracy 0.38989999890327454 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8500 Training loss 0.06723532825708389 Validation loss 0.061286456882953644 Accuracy 0.3862000107765198 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8510 Training loss 0.060623373836278915 Validation loss 0.06129886209964752 Accuracy 0.38600000739097595 Accuracies by class [array(0., dtype=float32), array(0.93, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8520 Training loss 0.05696519464254379 Validation loss 0.06118461862206459 Accuracy 0.3871999979019165 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8530 Training loss 0.058583714067935944 Validation loss 0.06086832657456398 Accuracy 0.38989999890327454 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8540 Training loss 0.06017293781042099 Validation loss 0.060982123017311096 Accuracy 0.3887999951839447 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8550 Training loss 0.06277509778738022 Validation loss 0.06082817539572716 Accuracy 0.3901999890804291 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8560 Training loss 0.06072402372956276 Validation loss 0.06081707030534744 Accuracy 0.3903999924659729 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8570 Training loss 0.05924663320183754 Validation loss 0.060940880328416824 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8580 Training loss 0.06109215319156647 Validation loss 0.0612470842897892 Accuracy 0.38589999079704285 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8590 Training loss 0.05919929966330528 Validation loss 0.061204515397548676 Accuracy 0.3862000107765198 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8600 Training loss 0.06040409952402115 Validation loss 0.0609285905957222 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8610 Training loss 0.0621887631714344 Validation loss 0.06087741255760193 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8620 Training loss 0.060946572571992874 Validation loss 0.06094641238451004 Accuracy 0.3887999951839447 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8630 Training loss 0.0659579411149025 Validation loss 0.060815609991550446 Accuracy 0.39070001244544983 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8640 Training loss 0.059721242636442184 Validation loss 0.060798194259405136 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8650 Training loss 0.059434469789266586 Validation loss 0.060830894857645035 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8660 Training loss 0.06257709115743637 Validation loss 0.06095447391271591 Accuracy 0.3887999951839447 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8670 Training loss 0.05849476158618927 Validation loss 0.060721367597579956 Accuracy 0.39149999618530273 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8680 Training loss 0.060119688510894775 Validation loss 0.060964249074459076 Accuracy 0.3898000121116638 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8690 Training loss 0.05935249850153923 Validation loss 0.06081512197852135 Accuracy 0.3910999894142151 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8700 Training loss 0.060775380581617355 Validation loss 0.060842789709568024 Accuracy 0.39079999923706055 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8710 Training loss 0.0627652257680893 Validation loss 0.06099822744727135 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8720 Training loss 0.06326846778392792 Validation loss 0.060904745012521744 Accuracy 0.39010000228881836 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8730 Training loss 0.06075384467840195 Validation loss 0.06089815869927406 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8740 Training loss 0.06429493427276611 Validation loss 0.06080467626452446 Accuracy 0.3910999894142151 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8750 Training loss 0.05930126830935478 Validation loss 0.06084390729665756 Accuracy 0.39079999923706055 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8760 Training loss 0.06386585533618927 Validation loss 0.06081409752368927 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8770 Training loss 0.06311801820993423 Validation loss 0.060877446085214615 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8780 Training loss 0.061074573546648026 Validation loss 0.06175239756703377 Accuracy 0.3813999891281128 Accuracies by class [array(0., dtype=float32), array(0.995, dtype=float32), array(0.848, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8790 Training loss 0.058361347764730453 Validation loss 0.0608927346765995 Accuracy 0.3898000121116638 Accuracies by class [array(0., dtype=float32), array(0.994, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8800 Training loss 0.05950804799795151 Validation loss 0.06097913905978203 Accuracy 0.3887999951839447 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8810 Training loss 0.05969421938061714 Validation loss 0.06115160137414932 Accuracy 0.3871000111103058 Accuracies by class [array(0., dtype=float32), array(0.994, dtype=float32), array(0.909, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8820 Training loss 0.05709415674209595 Validation loss 0.06094343587756157 Accuracy 0.3887999951839447 Accuracies by class [array(0., dtype=float32), array(0.995, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8830 Training loss 0.06329376250505447 Validation loss 0.061001088470220566 Accuracy 0.3889000117778778 Accuracies by class [array(0., dtype=float32), array(0.994, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8840 Training loss 0.058827485889196396 Validation loss 0.0609220452606678 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.993, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8850 Training loss 0.061826691031455994 Validation loss 0.06110821291804314 Accuracy 0.38749998807907104 Accuracies by class [array(0., dtype=float32), array(0.994, dtype=float32), array(0.909, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8860 Training loss 0.05998232588171959 Validation loss 0.060710418969392776 Accuracy 0.3917999863624573 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8870 Training loss 0.06027078256011009 Validation loss 0.06074874848127365 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8880 Training loss 0.06184764578938484 Validation loss 0.06083221733570099 Accuracy 0.3903999924659729 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8890 Training loss 0.06315577775239944 Validation loss 0.060965802520513535 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8900 Training loss 0.06223370507359505 Validation loss 0.06131990626454353 Accuracy 0.3856000006198883 Accuracies by class [array(0., dtype=float32), array(0.995, dtype=float32), array(0.911, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8910 Training loss 0.05924445018172264 Validation loss 0.06089315935969353 Accuracy 0.3896999955177307 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8920 Training loss 0.06238546594977379 Validation loss 0.06085328757762909 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8930 Training loss 0.06076665595173836 Validation loss 0.06085410714149475 Accuracy 0.3901999890804291 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8940 Training loss 0.058625586330890656 Validation loss 0.060743216425180435 Accuracy 0.3912000060081482 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8950 Training loss 0.06391502171754837 Validation loss 0.06071661040186882 Accuracy 0.391400009393692 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8960 Training loss 0.0589633584022522 Validation loss 0.06091992184519768 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.991, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8970 Training loss 0.061629582196474075 Validation loss 0.06079338490962982 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8980 Training loss 0.059119533747434616 Validation loss 0.0610857717692852 Accuracy 0.38749998807907104 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 8990 Training loss 0.060799963772296906 Validation loss 0.060812149196863174 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9000 Training loss 0.057878900319337845 Validation loss 0.06094157323241234 Accuracy 0.38920000195503235 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9010 Training loss 0.061024390161037445 Validation loss 0.06083053722977638 Accuracy 0.3903999924659729 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9020 Training loss 0.05884909629821777 Validation loss 0.060709092766046524 Accuracy 0.3912000060081482 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9030 Training loss 0.05969197675585747 Validation loss 0.06074054539203644 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9040 Training loss 0.05851532891392708 Validation loss 0.06069990247488022 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9050 Training loss 0.06232531741261482 Validation loss 0.060753144323825836 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9060 Training loss 0.060151759535074234 Validation loss 0.06085604056715965 Accuracy 0.3903000056743622 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9070 Training loss 0.05928422883152962 Validation loss 0.06079226732254028 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9080 Training loss 0.05730942636728287 Validation loss 0.06083418056368828 Accuracy 0.3903999924659729 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9090 Training loss 0.06234993040561676 Validation loss 0.06072273477911949 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9100 Training loss 0.05982528626918793 Validation loss 0.06070638820528984 Accuracy 0.39149999618530273 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9110 Training loss 0.06010942533612251 Validation loss 0.060692451894283295 Accuracy 0.3917999863624573 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9120 Training loss 0.06114881485700607 Validation loss 0.06070953980088234 Accuracy 0.39160001277923584 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.963, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9130 Training loss 0.06302748620510101 Validation loss 0.061461564153432846 Accuracy 0.38420000672340393 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.971, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(1., dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.889, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9140 Training loss 0.05942627415060997 Validation loss 0.060789018869400024 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.963, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9150 Training loss 0.05926147848367691 Validation loss 0.060736410319805145 Accuracy 0.391400009393692 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.962, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9160 Training loss 0.06210450083017349 Validation loss 0.060713134706020355 Accuracy 0.391400009393692 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.961, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9170 Training loss 0.058731306344270706 Validation loss 0.060964033007621765 Accuracy 0.3887999951839447 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9180 Training loss 0.06012868881225586 Validation loss 0.06140061467885971 Accuracy 0.3849000036716461 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.992, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.892, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9190 Training loss 0.06094000115990639 Validation loss 0.06086663901805878 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.974, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9200 Training loss 0.06067783758044243 Validation loss 0.06067664176225662 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9210 Training loss 0.06284702569246292 Validation loss 0.060783546417951584 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9220 Training loss 0.06325898319482803 Validation loss 0.06097704544663429 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9230 Training loss 0.06006152182817459 Validation loss 0.060732971876859665 Accuracy 0.3912000060081482 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9240 Training loss 0.06155038997530937 Validation loss 0.06103125587105751 Accuracy 0.3880000114440918 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9250 Training loss 0.057916197925806046 Validation loss 0.06088477000594139 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.994, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9260 Training loss 0.06136494502425194 Validation loss 0.061704326421022415 Accuracy 0.38109999895095825 Accuracies by class [array(0., dtype=float32), array(0.991, dtype=float32), array(0.839, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9270 Training loss 0.0599992461502552 Validation loss 0.060836631804704666 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.992, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9280 Training loss 0.05972619727253914 Validation loss 0.06077079474925995 Accuracy 0.39079999923706055 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9290 Training loss 0.05897332727909088 Validation loss 0.061171259731054306 Accuracy 0.38690000772476196 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9300 Training loss 0.06081128492951393 Validation loss 0.06085118278861046 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.991, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9310 Training loss 0.05662650614976883 Validation loss 0.06095733493566513 Accuracy 0.38909998536109924 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9320 Training loss 0.05921977385878563 Validation loss 0.06088091433048248 Accuracy 0.38999998569488525 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9330 Training loss 0.05702972784638405 Validation loss 0.061073075979948044 Accuracy 0.38839998841285706 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9340 Training loss 0.060802798718214035 Validation loss 0.060842715203762054 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9350 Training loss 0.05878407880663872 Validation loss 0.06091717630624771 Accuracy 0.39010000228881836 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9360 Training loss 0.06456851214170456 Validation loss 0.06104329228401184 Accuracy 0.388700008392334 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9370 Training loss 0.06208807975053787 Validation loss 0.06071239337325096 Accuracy 0.391400009393692 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9380 Training loss 0.06298357248306274 Validation loss 0.06084086745977402 Accuracy 0.39079999923706055 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9390 Training loss 0.06008986011147499 Validation loss 0.06094743683934212 Accuracy 0.38909998536109924 Accuracies by class [array(0., dtype=float32), array(0.993, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9400 Training loss 0.05406706780195236 Validation loss 0.0608205646276474 Accuracy 0.39070001244544983 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9410 Training loss 0.0632408857345581 Validation loss 0.06086704134941101 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9420 Training loss 0.06257709115743637 Validation loss 0.060820821672677994 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9430 Training loss 0.061728376895189285 Validation loss 0.06116657704114914 Accuracy 0.387800008058548 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9440 Training loss 0.060062225908041 Validation loss 0.06135774403810501 Accuracy 0.3849000036716461 Accuracies by class [array(0., dtype=float32), array(0.993, dtype=float32), array(0.879, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9450 Training loss 0.061393849551677704 Validation loss 0.06084146350622177 Accuracy 0.3903000056743622 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9460 Training loss 0.05853061005473137 Validation loss 0.06079695001244545 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9470 Training loss 0.05889010801911354 Validation loss 0.0607917495071888 Accuracy 0.3901999890804291 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9480 Training loss 0.058857399970293045 Validation loss 0.060727305710315704 Accuracy 0.3912000060081482 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9490 Training loss 0.0650545060634613 Validation loss 0.06086970865726471 Accuracy 0.38909998536109924 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9500 Training loss 0.06139975041151047 Validation loss 0.061227671802043915 Accuracy 0.38659998774528503 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.983, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.902, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9510 Training loss 0.06227949634194374 Validation loss 0.060798924416303635 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.971, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9520 Training loss 0.05934794247150421 Validation loss 0.06084264814853668 Accuracy 0.3903000056743622 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9530 Training loss 0.05834943801164627 Validation loss 0.060844507068395615 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.987, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9540 Training loss 0.05662962421774864 Validation loss 0.06097640097141266 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.988, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9550 Training loss 0.05834653601050377 Validation loss 0.06094459816813469 Accuracy 0.3889999985694885 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.988, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9560 Training loss 0.06086772680282593 Validation loss 0.06117357313632965 Accuracy 0.3871999979019165 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.991, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9570 Training loss 0.05944710224866867 Validation loss 0.06079883128404617 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.984, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9580 Training loss 0.0597112774848938 Validation loss 0.06090773269534111 Accuracy 0.3894999921321869 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9590 Training loss 0.06161661818623543 Validation loss 0.060730546712875366 Accuracy 0.3912000060081482 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.983, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9600 Training loss 0.06075359135866165 Validation loss 0.060621876269578934 Accuracy 0.392300009727478 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9610 Training loss 0.05996323749423027 Validation loss 0.060742802917957306 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9620 Training loss 0.05755285546183586 Validation loss 0.06077899411320686 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.986, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9630 Training loss 0.05973600223660469 Validation loss 0.06074237823486328 Accuracy 0.39169999957084656 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9640 Training loss 0.06070639565587044 Validation loss 0.06062593683600426 Accuracy 0.39239999651908875 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9650 Training loss 0.0602908730506897 Validation loss 0.06075523793697357 Accuracy 0.39160001277923584 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9660 Training loss 0.059515319764614105 Validation loss 0.060667578130960464 Accuracy 0.391400009393692 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9670 Training loss 0.06341539323329926 Validation loss 0.060933467000722885 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9680 Training loss 0.05662114545702934 Validation loss 0.06098557636141777 Accuracy 0.38940000534057617 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9690 Training loss 0.06007783114910126 Validation loss 0.06093958020210266 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9700 Training loss 0.05777078866958618 Validation loss 0.06079813092947006 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9710 Training loss 0.05882561206817627 Validation loss 0.06078155338764191 Accuracy 0.3910999894142151 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9720 Training loss 0.06297843903303146 Validation loss 0.060656532645225525 Accuracy 0.39250001311302185 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9730 Training loss 0.060922008007764816 Validation loss 0.060704998672008514 Accuracy 0.3912000060081482 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9740 Training loss 0.06355657428503036 Validation loss 0.06065687537193298 Accuracy 0.3919000029563904 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9750 Training loss 0.06025712937116623 Validation loss 0.060614656656980515 Accuracy 0.3919000029563904 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9760 Training loss 0.062097035348415375 Validation loss 0.06073855981230736 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.981, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9770 Training loss 0.06049373000860214 Validation loss 0.060590047389268875 Accuracy 0.3921999931335449 Accuracies by class [array(0., dtype=float32), array(0.98, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9780 Training loss 0.059529468417167664 Validation loss 0.06058904156088829 Accuracy 0.39259999990463257 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9790 Training loss 0.06086571887135506 Validation loss 0.060598526149988174 Accuracy 0.392300009727478 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9800 Training loss 0.05896462872624397 Validation loss 0.060761913657188416 Accuracy 0.3912999927997589 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9810 Training loss 0.06147707626223564 Validation loss 0.06084205210208893 Accuracy 0.39089998602867126 Accuracies by class [array(0., dtype=float32), array(0.981, dtype=float32), array(0.982, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9820 Training loss 0.06490103155374527 Validation loss 0.06079774349927902 Accuracy 0.3910999894142151 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.976, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9830 Training loss 0.06516771763563156 Validation loss 0.06067796051502228 Accuracy 0.3921999931335449 Accuracies by class [array(0., dtype=float32), array(0.982, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9840 Training loss 0.05771479383111 Validation loss 0.0606253519654274 Accuracy 0.3926999866962433 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9850 Training loss 0.06249472126364708 Validation loss 0.06078723073005676 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9860 Training loss 0.0614875853061676 Validation loss 0.061120349913835526 Accuracy 0.3878999948501587 Accuracies by class [array(0., dtype=float32), array(0.994, dtype=float32), array(0.909, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9870 Training loss 0.06207926571369171 Validation loss 0.06081870570778847 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.991, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9880 Training loss 0.061854247003793716 Validation loss 0.06085052713751793 Accuracy 0.3898000121116638 Accuracies by class [array(0., dtype=float32), array(0.992, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9890 Training loss 0.061397094279527664 Validation loss 0.06077626347541809 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9900 Training loss 0.060401611030101776 Validation loss 0.060811638832092285 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9910 Training loss 0.06182563677430153 Validation loss 0.06078911945223808 Accuracy 0.3901999890804291 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9920 Training loss 0.06074095144867897 Validation loss 0.06081037223339081 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9930 Training loss 0.06028489023447037 Validation loss 0.060831218957901 Accuracy 0.390500009059906 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9940 Training loss 0.05848833546042442 Validation loss 0.06090931594371796 Accuracy 0.3894999921321869 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9950 Training loss 0.06003810465335846 Validation loss 0.06102704256772995 Accuracy 0.38850000500679016 Accuracies by class [array(0., dtype=float32), array(0.983, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9960 Training loss 0.06164490431547165 Validation loss 0.06071417033672333 Accuracy 0.391400009393692 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9970 Training loss 0.06124386563897133 Validation loss 0.060734689235687256 Accuracy 0.3912000060081482 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9980 Training loss 0.05950360745191574 Validation loss 0.06079239770770073 Accuracy 0.39100000262260437 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 9990 Training loss 0.05927591025829315 Validation loss 0.060853611677885056 Accuracy 0.3903999924659729 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10000 Training loss 0.05858519673347473 Validation loss 0.060830257833004 Accuracy 0.39070001244544983 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10010 Training loss 0.0610976405441761 Validation loss 0.06071784347295761 Accuracy 0.3917999863624573 Accuracies by class [array(0., dtype=float32), array(0.984, dtype=float32), array(0.977, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10020 Training loss 0.06258974224328995 Validation loss 0.0608585849404335 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10030 Training loss 0.05981224402785301 Validation loss 0.06076604500412941 Accuracy 0.39160001277923584 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10040 Training loss 0.06077742576599121 Validation loss 0.06082041189074516 Accuracy 0.3910999894142151 Accuracies by class [array(0., dtype=float32), array(0.988, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10050 Training loss 0.06075768545269966 Validation loss 0.06066601723432541 Accuracy 0.39250001311302185 Accuracies by class [array(0., dtype=float32), array(0.985, dtype=float32), array(0.966, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10060 Training loss 0.060107726603746414 Validation loss 0.06072345748543739 Accuracy 0.3910999894142151 Accuracies by class [array(0., dtype=float32), array(0.99, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10070 Training loss 0.06291854381561279 Validation loss 0.060762230306863785 Accuracy 0.3905999958515167 Accuracies by class [array(0., dtype=float32), array(0.989, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10080 Training loss 0.05664782598614693 Validation loss 0.06092749163508415 Accuracy 0.38929998874664307 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.991, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10090 Training loss 0.0606355220079422 Validation loss 0.0611223503947258 Accuracy 0.38769999146461487 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.994, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10100 Training loss 0.06034199893474579 Validation loss 0.061003003269433975 Accuracy 0.388700008392334 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.992, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10110 Training loss 0.05816219747066498 Validation loss 0.06078741326928139 Accuracy 0.39089998602867126 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.987, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10120 Training loss 0.06308353692293167 Validation loss 0.06076375022530556 Accuracy 0.39070001244544983 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.987, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10130 Training loss 0.05878005921840668 Validation loss 0.060864370316267014 Accuracy 0.38960000872612 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.987, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10140 Training loss 0.06028115749359131 Validation loss 0.06072736158967018 Accuracy 0.3912000060081482 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.983, dtype=float32), array(0.002, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10150 Training loss 0.057525359094142914 Validation loss 0.060671284794807434 Accuracy 0.3917999863624573 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.977, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10160 Training loss 0.06076318398118019 Validation loss 0.06064382940530777 Accuracy 0.39239999651908875 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.978, dtype=float32), array(0.002, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10170 Training loss 0.05956771969795227 Validation loss 0.0590999610722065 Accuracy 0.40689998865127563 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.966, dtype=float32), array(0.163, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10180 Training loss 0.055332787334918976 Validation loss 0.05380323901772499 Accuracy 0.45890000462532043 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.869, dtype=float32), array(0.848, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10190 Training loss 0.051090922206640244 Validation loss 0.05284195393323898 Accuracy 0.4697999954223633 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.929, dtype=float32), array(0.86, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10200 Training loss 0.05258823186159134 Validation loss 0.052870482206344604 Accuracy 0.4699000120162964 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.956, dtype=float32), array(0.854, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10210 Training loss 0.05149317532777786 Validation loss 0.05265457183122635 Accuracy 0.47209998965263367 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.897, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10220 Training loss 0.05394779518246651 Validation loss 0.05303182080388069 Accuracy 0.4683000147342682 Accuracies by class [array(0., dtype=float32), array(0.928, dtype=float32), array(0.96, dtype=float32), array(0.825, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10230 Training loss 0.054451171308755875 Validation loss 0.05464920401573181 Accuracy 0.4521999955177307 Accuracies by class [array(0., dtype=float32), array(0.692, dtype=float32), array(0.929, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10240 Training loss 0.05454835668206215 Validation loss 0.05220828950405121 Accuracy 0.4763000011444092 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.955, dtype=float32), array(0.898, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10250 Training loss 0.05413946136832237 Validation loss 0.05234960839152336 Accuracy 0.47450000047683716 Accuracies by class [array(0., dtype=float32), array(0.925, dtype=float32), array(0.956, dtype=float32), array(0.895, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10260 Training loss 0.05134884640574455 Validation loss 0.05247396603226662 Accuracy 0.47369998693466187 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.965, dtype=float32), array(0.859, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10270 Training loss 0.052578847855329514 Validation loss 0.052150994539260864 Accuracy 0.4772000014781952 Accuracies by class [array(0., dtype=float32), array(0.935, dtype=float32), array(0.935, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10280 Training loss 0.052546657621860504 Validation loss 0.05239710584282875 Accuracy 0.47450000047683716 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.945, dtype=float32), array(0.873, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10290 Training loss 0.05217811092734337 Validation loss 0.05254139006137848 Accuracy 0.47279998660087585 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.949, dtype=float32), array(0.854, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10300 Training loss 0.05348588898777962 Validation loss 0.05221385881304741 Accuracy 0.4765999913215637 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.954, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10310 Training loss 0.051875270903110504 Validation loss 0.05244539678096771 Accuracy 0.47429999709129333 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.97, dtype=float32), array(0.911, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10320 Training loss 0.052660923451185226 Validation loss 0.05217441916465759 Accuracy 0.4771000146865845 Accuracies by class [array(0., dtype=float32), array(0.912, dtype=float32), array(0.955, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10330 Training loss 0.050608694553375244 Validation loss 0.05238485708832741 Accuracy 0.4749000072479248 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.97, dtype=float32), array(0.881, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10340 Training loss 0.0508103221654892 Validation loss 0.052262842655181885 Accuracy 0.4763999879360199 Accuracies by class [array(0., dtype=float32), array(0.915, dtype=float32), array(0.961, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10350 Training loss 0.049904610961675644 Validation loss 0.05317579209804535 Accuracy 0.4666999876499176 Accuracies by class [array(0., dtype=float32), array(0.892, dtype=float32), array(0.839, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10360 Training loss 0.05321722477674484 Validation loss 0.05198364332318306 Accuracy 0.47850000858306885 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.953, dtype=float32), array(0.902, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10370 Training loss 0.04974210262298584 Validation loss 0.05207793787121773 Accuracy 0.47769999504089355 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.957, dtype=float32), array(0.892, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10380 Training loss 0.05282294750213623 Validation loss 0.052991483360528946 Accuracy 0.4681999981403351 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.945, dtype=float32), array(0.803, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10390 Training loss 0.053101543337106705 Validation loss 0.052109457552433014 Accuracy 0.4781000018119812 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.962, dtype=float32), array(0.924, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10400 Training loss 0.05211962014436722 Validation loss 0.0525205172598362 Accuracy 0.4740999937057495 Accuracies by class [array(0., dtype=float32), array(0.913, dtype=float32), array(0.956, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10410 Training loss 0.05309312045574188 Validation loss 0.05256064608693123 Accuracy 0.4733999967575073 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.976, dtype=float32), array(0.878, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10420 Training loss 0.05303366109728813 Validation loss 0.052742283791303635 Accuracy 0.47130000591278076 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.977, dtype=float32), array(0.841, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10430 Training loss 0.05258507281541824 Validation loss 0.0520259365439415 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.955, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10440 Training loss 0.04773874953389168 Validation loss 0.05205799266695976 Accuracy 0.4787999987602234 Accuracies by class [array(0., dtype=float32), array(0.926, dtype=float32), array(0.936, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10450 Training loss 0.052768524736166 Validation loss 0.05207668989896774 Accuracy 0.4779999852180481 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.958, dtype=float32), array(0.91, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10460 Training loss 0.05252739414572716 Validation loss 0.05231669917702675 Accuracy 0.4756999909877777 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.969, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10470 Training loss 0.05508483946323395 Validation loss 0.05228833854198456 Accuracy 0.47609999775886536 Accuracies by class [array(0., dtype=float32), array(0.927, dtype=float32), array(0.968, dtype=float32), array(0.916, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10480 Training loss 0.05091531202197075 Validation loss 0.05230511352419853 Accuracy 0.47600001096725464 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.972, dtype=float32), array(0.895, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10490 Training loss 0.050961121916770935 Validation loss 0.052097246050834656 Accuracy 0.4781000018119812 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.946, dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10500 Training loss 0.051608067005872726 Validation loss 0.05201216787099838 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.949, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10510 Training loss 0.05211884528398514 Validation loss 0.05201129987835884 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.948, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10520 Training loss 0.05161530524492264 Validation loss 0.05263586342334747 Accuracy 0.4722999930381775 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.96, dtype=float32), array(0.834, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10530 Training loss 0.05508299916982651 Validation loss 0.05202700197696686 Accuracy 0.4787999987602234 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.944, dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10540 Training loss 0.05016888305544853 Validation loss 0.05193885788321495 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.938, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10550 Training loss 0.05239436775445938 Validation loss 0.052113570272922516 Accuracy 0.477400004863739 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.954, dtype=float32), array(0.884, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10560 Training loss 0.05246348679065704 Validation loss 0.05201257765293121 Accuracy 0.4787999987602234 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.953, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10570 Training loss 0.05045703053474426 Validation loss 0.051944490522146225 Accuracy 0.47940000891685486 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.93, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10580 Training loss 0.050275787711143494 Validation loss 0.05229237303137779 Accuracy 0.47510001063346863 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.94, dtype=float32), array(0.874, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10590 Training loss 0.04949311912059784 Validation loss 0.052369024604558945 Accuracy 0.4742000102996826 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.912, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10600 Training loss 0.052194565534591675 Validation loss 0.05225224047899246 Accuracy 0.47620001435279846 Accuracies by class [array(0., dtype=float32), array(0.901, dtype=float32), array(0.94, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10610 Training loss 0.04771150276064873 Validation loss 0.051982052624225616 Accuracy 0.47870001196861267 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.945, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10620 Training loss 0.05344928428530693 Validation loss 0.05204809084534645 Accuracy 0.4779999852180481 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.94, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10630 Training loss 0.05181412771344185 Validation loss 0.05179845541715622 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.935, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10640 Training loss 0.05397048592567444 Validation loss 0.051792044192552567 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.941, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10650 Training loss 0.050044432282447815 Validation loss 0.052241768687963486 Accuracy 0.4763999879360199 Accuracies by class [array(0., dtype=float32), array(0.926, dtype=float32), array(0.903, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10660 Training loss 0.0511297844350338 Validation loss 0.05196205899119377 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.951, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10670 Training loss 0.04968803748488426 Validation loss 0.052171822637319565 Accuracy 0.4767000079154968 Accuracies by class [array(0., dtype=float32), array(0.931, dtype=float32), array(0.912, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10680 Training loss 0.05165925249457359 Validation loss 0.051971372216939926 Accuracy 0.47850000858306885 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.938, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10690 Training loss 0.05157731473445892 Validation loss 0.051850058138370514 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.941, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10700 Training loss 0.04884883761405945 Validation loss 0.05217437446117401 Accuracy 0.4771000146865845 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.958, dtype=float32), array(0.897, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10710 Training loss 0.051564980298280716 Validation loss 0.052450794726610184 Accuracy 0.47360000014305115 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.912, dtype=float32), array(0.886, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10720 Training loss 0.05445141717791557 Validation loss 0.052362535148859024 Accuracy 0.475600004196167 Accuracies by class [array(0., dtype=float32), array(0.926, dtype=float32), array(0.882, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10730 Training loss 0.053944721817970276 Validation loss 0.05196887627243996 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.957, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10740 Training loss 0.0547676682472229 Validation loss 0.05211357772350311 Accuracy 0.47780001163482666 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.965, dtype=float32), array(0.898, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10750 Training loss 0.05328076705336571 Validation loss 0.052229952067136765 Accuracy 0.4771000146865845 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.944, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10760 Training loss 0.05328568443655968 Validation loss 0.05188959836959839 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.954, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10770 Training loss 0.051358480006456375 Validation loss 0.05175647884607315 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.951, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10780 Training loss 0.049009378999471664 Validation loss 0.052181825041770935 Accuracy 0.47760000824928284 Accuracies by class [array(0., dtype=float32), array(0.917, dtype=float32), array(0.933, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10790 Training loss 0.051457375288009644 Validation loss 0.05180484801530838 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.929, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10800 Training loss 0.0492728129029274 Validation loss 0.052040278911590576 Accuracy 0.47850000858306885 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.911, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10810 Training loss 0.051429592072963715 Validation loss 0.051982346922159195 Accuracy 0.47859999537467957 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(0.928, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10820 Training loss 0.04965236783027649 Validation loss 0.051783349364995956 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.949, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10830 Training loss 0.05035711079835892 Validation loss 0.051947858184576035 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.959, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10840 Training loss 0.05277901142835617 Validation loss 0.052091334015131 Accuracy 0.47780001163482666 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.951, dtype=float32), array(0.886, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10850 Training loss 0.050590433180332184 Validation loss 0.05182114616036415 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.939, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10860 Training loss 0.05002327635884285 Validation loss 0.05183054506778717 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.941, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10870 Training loss 0.051472850143909454 Validation loss 0.05189063400030136 Accuracy 0.4803999960422516 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.939, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10880 Training loss 0.054399337619543076 Validation loss 0.05176344886422157 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.943, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10890 Training loss 0.05068530887365341 Validation loss 0.051820337772369385 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.94, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10900 Training loss 0.052536312490701675 Validation loss 0.05189196392893791 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.929, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10910 Training loss 0.04862116649746895 Validation loss 0.05212926119565964 Accuracy 0.4781000018119812 Accuracies by class [array(0., dtype=float32), array(0.924, dtype=float32), array(0.922, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10920 Training loss 0.04770996421575546 Validation loss 0.051836736500263214 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.927, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10930 Training loss 0.04755009338259697 Validation loss 0.05188002437353134 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.958, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10940 Training loss 0.04906065762042999 Validation loss 0.05200378969311714 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.928, dtype=float32), array(0.948, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10950 Training loss 0.05341902747750282 Validation loss 0.05202252417802811 Accuracy 0.47839999198913574 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.96, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10960 Training loss 0.05071453005075455 Validation loss 0.0516999214887619 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.949, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10970 Training loss 0.04888278618454933 Validation loss 0.05187032371759415 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.949, dtype=float32), array(0.913, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10980 Training loss 0.04626154154539108 Validation loss 0.05213307589292526 Accuracy 0.47769999504089355 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.961, dtype=float32), array(0.889, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 10990 Training loss 0.04943910613656044 Validation loss 0.051897063851356506 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.935, dtype=float32), array(0.949, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11000 Training loss 0.051278695464134216 Validation loss 0.0518697053194046 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.958, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11010 Training loss 0.04982101917266846 Validation loss 0.05183818191289902 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.96, dtype=float32), array(0.921, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11020 Training loss 0.05095332860946655 Validation loss 0.05174938961863518 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.943, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11030 Training loss 0.054629191756248474 Validation loss 0.05181955173611641 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.926, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11040 Training loss 0.052769072353839874 Validation loss 0.052063219249248505 Accuracy 0.47839999198913574 Accuracies by class [array(0., dtype=float32), array(0.935, dtype=float32), array(0.912, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11050 Training loss 0.05671047046780586 Validation loss 0.05194425955414772 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.927, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11060 Training loss 0.04817948490381241 Validation loss 0.05207028239965439 Accuracy 0.477400004863739 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.946, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11070 Training loss 0.049477141350507736 Validation loss 0.05185583978891373 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.96, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11080 Training loss 0.05125286802649498 Validation loss 0.052218686789274216 Accuracy 0.47620001435279846 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.971, dtype=float32), array(0.89, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11090 Training loss 0.052040670067071915 Validation loss 0.05263137072324753 Accuracy 0.4717000126838684 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.971, dtype=float32), array(0.836, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11100 Training loss 0.05424906313419342 Validation loss 0.052070122212171555 Accuracy 0.4779999852180481 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.959, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11110 Training loss 0.05107932537794113 Validation loss 0.05193188786506653 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.943, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11120 Training loss 0.050788067281246185 Validation loss 0.05191098153591156 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.951, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11130 Training loss 0.0504787303507328 Validation loss 0.05183766037225723 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.957, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11140 Training loss 0.0516163632273674 Validation loss 0.051795594394207 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.94, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11150 Training loss 0.0516425184905529 Validation loss 0.051748935133218765 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.95, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11160 Training loss 0.04892582818865776 Validation loss 0.051786839962005615 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.958, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11170 Training loss 0.05323600769042969 Validation loss 0.05183243006467819 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.95, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11180 Training loss 0.048798106610774994 Validation loss 0.051773086190223694 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.944, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11190 Training loss 0.050125233829021454 Validation loss 0.05175108462572098 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.928, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11200 Training loss 0.04956172779202461 Validation loss 0.05188644677400589 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.926, dtype=float32), array(0.947, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11210 Training loss 0.04864714667201042 Validation loss 0.0521078035235405 Accuracy 0.47760000824928284 Accuracies by class [array(0., dtype=float32), array(0.917, dtype=float32), array(0.914, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11220 Training loss 0.05121730640530586 Validation loss 0.05178488418459892 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.963, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11230 Training loss 0.05013953521847725 Validation loss 0.052164580672979355 Accuracy 0.47690001130104065 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.97, dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11240 Training loss 0.0497264489531517 Validation loss 0.05331023409962654 Accuracy 0.4652000069618225 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.981, dtype=float32), array(0.787, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11250 Training loss 0.04912016913294792 Validation loss 0.051981210708618164 Accuracy 0.4788999855518341 Accuracies by class [array(0., dtype=float32), array(0.93, dtype=float32), array(0.959, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11260 Training loss 0.05041090026497841 Validation loss 0.05210847780108452 Accuracy 0.47769999504089355 Accuracies by class [array(0., dtype=float32), array(0.919, dtype=float32), array(0.944, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11270 Training loss 0.05069529265165329 Validation loss 0.05205259844660759 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.918, dtype=float32), array(0.944, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11280 Training loss 0.05191895365715027 Validation loss 0.051791876554489136 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.96, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11290 Training loss 0.04899672418832779 Validation loss 0.051967553794384 Accuracy 0.47850000858306885 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.941, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11300 Training loss 0.05393839627504349 Validation loss 0.05217170715332031 Accuracy 0.4763000011444092 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.954, dtype=float32), array(0.893, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11310 Training loss 0.05374908819794655 Validation loss 0.051849402487277985 Accuracy 0.47940000891685486 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.952, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11320 Training loss 0.04937678948044777 Validation loss 0.052103377878665924 Accuracy 0.47760000824928284 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.937, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11330 Training loss 0.04992160201072693 Validation loss 0.052215952426195145 Accuracy 0.47690001130104065 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.913, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11340 Training loss 0.05033709853887558 Validation loss 0.05220108479261398 Accuracy 0.47620001435279846 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.969, dtype=float32), array(0.911, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11350 Training loss 0.04677748680114746 Validation loss 0.0518842376768589 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.961, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11360 Training loss 0.05269483104348183 Validation loss 0.052022743970155716 Accuracy 0.4778999984264374 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.964, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11370 Training loss 0.05701225623488426 Validation loss 0.05171448737382889 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.96, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11380 Training loss 0.05314064398407936 Validation loss 0.05181517079472542 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.949, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11390 Training loss 0.049464743584394455 Validation loss 0.05203160643577576 Accuracy 0.4778999984264374 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.957, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11400 Training loss 0.05153488740324974 Validation loss 0.05183830484747887 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.955, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11410 Training loss 0.04798056185245514 Validation loss 0.05169634893536568 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.939, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11420 Training loss 0.05150856450200081 Validation loss 0.0517384372651577 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.953, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11430 Training loss 0.05024028569459915 Validation loss 0.051710668951272964 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.961, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11440 Training loss 0.05413427576422691 Validation loss 0.05165679752826691 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.961, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11450 Training loss 0.050511740148067474 Validation loss 0.05170603469014168 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.959, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11460 Training loss 0.05076874792575836 Validation loss 0.05183602496981621 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.963, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11470 Training loss 0.054912861436605453 Validation loss 0.05175972729921341 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.936, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11480 Training loss 0.049716517329216 Validation loss 0.051867228001356125 Accuracy 0.4799000024795532 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.963, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11490 Training loss 0.055662721395492554 Validation loss 0.051795825362205505 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.937, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11500 Training loss 0.0550696924328804 Validation loss 0.05202731490135193 Accuracy 0.4781000018119812 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.924, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11510 Training loss 0.05527148395776749 Validation loss 0.05179364234209061 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.95, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11520 Training loss 0.0497308149933815 Validation loss 0.051912032067775726 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.935, dtype=float32), array(0.948, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11530 Training loss 0.05165814235806465 Validation loss 0.05193808674812317 Accuracy 0.4799000024795532 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.942, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11540 Training loss 0.05311722308397293 Validation loss 0.051764197647571564 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.953, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11550 Training loss 0.05262117460370064 Validation loss 0.051763374358415604 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11560 Training loss 0.05153222009539604 Validation loss 0.051745180040597916 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.955, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11570 Training loss 0.04827047884464264 Validation loss 0.051694031804800034 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.96, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11580 Training loss 0.047495514154434204 Validation loss 0.051860176026821136 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.953, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11590 Training loss 0.05038461461663246 Validation loss 0.05187132954597473 Accuracy 0.47940000891685486 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.957, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11600 Training loss 0.050744879990816116 Validation loss 0.05228501558303833 Accuracy 0.47589999437332153 Accuracies by class [array(0., dtype=float32), array(0.919, dtype=float32), array(0.918, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11610 Training loss 0.05036497116088867 Validation loss 0.051718808710575104 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.942, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11620 Training loss 0.04930578172206879 Validation loss 0.05187732353806496 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.928, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11630 Training loss 0.05181991681456566 Validation loss 0.0518963485956192 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.942, dtype=float32), array(0.91, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11640 Training loss 0.05047976225614548 Validation loss 0.051590751856565475 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.96, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11650 Training loss 0.0482608862221241 Validation loss 0.05157812684774399 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.962, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11660 Training loss 0.05193350836634636 Validation loss 0.05216030403971672 Accuracy 0.4771000146865845 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.962, dtype=float32), array(0.877, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11670 Training loss 0.05417915806174278 Validation loss 0.051598720252513885 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.95, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11680 Training loss 0.0543806292116642 Validation loss 0.05176636576652527 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.925, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11690 Training loss 0.053275711834430695 Validation loss 0.051614854484796524 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.948, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11700 Training loss 0.05086614564061165 Validation loss 0.05190575122833252 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.954, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11710 Training loss 0.049564260989427567 Validation loss 0.05202366039156914 Accuracy 0.4781999886035919 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.974, dtype=float32), array(0.884, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11720 Training loss 0.051309142261743546 Validation loss 0.0527249276638031 Accuracy 0.4713999927043915 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.98, dtype=float32), array(0.844, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11730 Training loss 0.048690225929021835 Validation loss 0.051785293966531754 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.965, dtype=float32), array(0.907, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11740 Training loss 0.049639295786619186 Validation loss 0.05216653272509575 Accuracy 0.4765999913215637 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.964, dtype=float32), array(0.868, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11750 Training loss 0.05174894258379936 Validation loss 0.051991287618875504 Accuracy 0.4781999886035919 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.969, dtype=float32), array(0.9, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11760 Training loss 0.05230940878391266 Validation loss 0.0523981973528862 Accuracy 0.4740000069141388 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.936, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.924, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11770 Training loss 0.05296781286597252 Validation loss 0.051578033715486526 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.959, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11780 Training loss 0.04932982847094536 Validation loss 0.051660049706697464 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.945, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11790 Training loss 0.05290674790740013 Validation loss 0.05188833922147751 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.93, dtype=float32), array(0.931, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11800 Training loss 0.053312744945287704 Validation loss 0.051726341247558594 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.955, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11810 Training loss 0.04902704805135727 Validation loss 0.051662612706422806 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.952, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11820 Training loss 0.0544513463973999 Validation loss 0.05166841670870781 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.956, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11830 Training loss 0.05293857678771019 Validation loss 0.05193090811371803 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.962, dtype=float32), array(0.884, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11840 Training loss 0.05321439728140831 Validation loss 0.05161996930837631 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.955, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11850 Training loss 0.05471780523657799 Validation loss 0.05195854604244232 Accuracy 0.47859999537467957 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.96, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11860 Training loss 0.05479789152741432 Validation loss 0.05191444233059883 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.942, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11870 Training loss 0.05299634858965874 Validation loss 0.051773540675640106 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.955, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11880 Training loss 0.047904692590236664 Validation loss 0.051680807024240494 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.959, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11890 Training loss 0.05018088221549988 Validation loss 0.05230069160461426 Accuracy 0.4745999872684479 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.959, dtype=float32), array(0.903, dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11900 Training loss 0.0490446463227272 Validation loss 0.051750730723142624 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.93, dtype=float32), array(0.942, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11910 Training loss 0.051761820912361145 Validation loss 0.052208878099918365 Accuracy 0.4763000011444092 Accuracies by class [array(0., dtype=float32), array(0.908, dtype=float32), array(0.909, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11920 Training loss 0.05027107894420624 Validation loss 0.05258800461888313 Accuracy 0.4733999967575073 Accuracies by class [array(0., dtype=float32), array(0.912, dtype=float32), array(0.938, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.909, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11930 Training loss 0.0508708655834198 Validation loss 0.05184667930006981 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.958, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11940 Training loss 0.05266646295785904 Validation loss 0.05252384766936302 Accuracy 0.47369998693466187 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.85, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11950 Training loss 0.053452495485544205 Validation loss 0.052434541285037994 Accuracy 0.47290000319480896 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.896, dtype=float32), array(0.877, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11960 Training loss 0.04887162148952484 Validation loss 0.05183684453368187 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.925, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11970 Training loss 0.051125649362802505 Validation loss 0.051729366183280945 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.93, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11980 Training loss 0.05326026305556297 Validation loss 0.05179218575358391 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.933, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 11990 Training loss 0.05675249919295311 Validation loss 0.05169470235705376 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.922, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12000 Training loss 0.050653573125600815 Validation loss 0.05171690881252289 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.92, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12010 Training loss 0.052796345204114914 Validation loss 0.05204271525144577 Accuracy 0.4772000014781952 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.969, dtype=float32), array(0.903, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12020 Training loss 0.04857495799660683 Validation loss 0.05206741392612457 Accuracy 0.4778999984264374 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.976, dtype=float32), array(0.9, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12030 Training loss 0.05345013365149498 Validation loss 0.051791392266750336 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.966, dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12040 Training loss 0.04977233707904816 Validation loss 0.05185123160481453 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.928, dtype=float32), array(0.937, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12050 Training loss 0.05521561577916145 Validation loss 0.05185582861304283 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.924, dtype=float32), array(0.939, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12060 Training loss 0.05168632045388222 Validation loss 0.05285419896245003 Accuracy 0.46939998865127563 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.981, dtype=float32), array(0.839, dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12070 Training loss 0.0554126538336277 Validation loss 0.052237167954444885 Accuracy 0.4756999909877777 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.973, dtype=float32), array(0.924, dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12080 Training loss 0.053874995559453964 Validation loss 0.05312043055891991 Accuracy 0.4668999910354614 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.973, dtype=float32), array(0.898, dtype=float32), array(0., dtype=float32), array(0.863, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12090 Training loss 0.05185526981949806 Validation loss 0.0527367927134037 Accuracy 0.47049999237060547 Accuracies by class [array(0., dtype=float32), array(0.936, dtype=float32), array(0.953, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.879, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12100 Training loss 0.0533628985285759 Validation loss 0.051660019904375076 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.952, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12110 Training loss 0.046246424317359924 Validation loss 0.05193018540740013 Accuracy 0.47940000891685486 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.925, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12120 Training loss 0.052454207092523575 Validation loss 0.05245194211602211 Accuracy 0.4745999872684479 Accuracies by class [array(0., dtype=float32), array(0.911, dtype=float32), array(0.896, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12130 Training loss 0.05097757652401924 Validation loss 0.05228085815906525 Accuracy 0.476500004529953 Accuracies by class [array(0., dtype=float32), array(0.919, dtype=float32), array(0.915, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12140 Training loss 0.047074947506189346 Validation loss 0.05191255733370781 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.911, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12150 Training loss 0.0538172721862793 Validation loss 0.051675014197826385 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.945, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12160 Training loss 0.046619292348623276 Validation loss 0.05174250155687332 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.944, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12170 Training loss 0.05100952088832855 Validation loss 0.05170142650604248 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.939, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12180 Training loss 0.050946157425642014 Validation loss 0.05186576768755913 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.925, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12190 Training loss 0.052112314850091934 Validation loss 0.052014656364917755 Accuracy 0.4787999987602234 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.968, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12200 Training loss 0.056578513234853745 Validation loss 0.052025970071554184 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.949, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12210 Training loss 0.05154193192720413 Validation loss 0.051849886775016785 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.93, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12220 Training loss 0.04922639578580856 Validation loss 0.05210426449775696 Accuracy 0.47780001163482666 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.952, dtype=float32), array(0.916, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12230 Training loss 0.053443074226379395 Validation loss 0.05209005996584892 Accuracy 0.47679999470710754 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.959, dtype=float32), array(0.872, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12240 Training loss 0.05172998458147049 Validation loss 0.0523437075316906 Accuracy 0.47440001368522644 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.974, dtype=float32), array(0.861, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12250 Training loss 0.04920804500579834 Validation loss 0.051784686744213104 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.973, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12260 Training loss 0.05058849975466728 Validation loss 0.051701050251722336 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.97, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12270 Training loss 0.045809291303157806 Validation loss 0.05164683237671852 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.962, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12280 Training loss 0.05062941834330559 Validation loss 0.051574766635894775 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.953, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12290 Training loss 0.050079457461833954 Validation loss 0.05158192291855812 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.947, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12300 Training loss 0.05198952555656433 Validation loss 0.05175391212105751 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.942, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12310 Training loss 0.05288102850317955 Validation loss 0.05165603384375572 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.962, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12320 Training loss 0.053540728986263275 Validation loss 0.05228045582771301 Accuracy 0.47519999742507935 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.953, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12330 Training loss 0.04921561852097511 Validation loss 0.05151144415140152 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.952, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12340 Training loss 0.05140618979930878 Validation loss 0.05185939371585846 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.964, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12350 Training loss 0.0516875721514225 Validation loss 0.05223420262336731 Accuracy 0.47589999437332153 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.975, dtype=float32), array(0.89, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12360 Training loss 0.04911027476191521 Validation loss 0.05166596546769142 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.936, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12370 Training loss 0.04859914258122444 Validation loss 0.05161822587251663 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.939, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12380 Training loss 0.048212118446826935 Validation loss 0.051991719752550125 Accuracy 0.4787999987602234 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.891, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12390 Training loss 0.04795052111148834 Validation loss 0.051712047308683395 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.926, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12400 Training loss 0.05036317929625511 Validation loss 0.05218803882598877 Accuracy 0.47620001435279846 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.937, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12410 Training loss 0.04902007058262825 Validation loss 0.05171850696206093 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.959, dtype=float32), array(0.918, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12420 Training loss 0.05318516120314598 Validation loss 0.05163990333676338 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.966, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12430 Training loss 0.054328110069036484 Validation loss 0.052711524069309235 Accuracy 0.4715999960899353 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.982, dtype=float32), array(0.849, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12440 Training loss 0.049248743802309036 Validation loss 0.052008211612701416 Accuracy 0.4781000018119812 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.975, dtype=float32), array(0.896, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12450 Training loss 0.05018766224384308 Validation loss 0.05187273025512695 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.922, dtype=float32), array(0.953, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12460 Training loss 0.04872562736272812 Validation loss 0.0524095818400383 Accuracy 0.47450000047683716 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.98, dtype=float32), array(0.869, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12470 Training loss 0.05022973567247391 Validation loss 0.05196766182780266 Accuracy 0.47839999198913574 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.968, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12480 Training loss 0.05405541509389877 Validation loss 0.05213893577456474 Accuracy 0.47699999809265137 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.979, dtype=float32), array(0.896, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12490 Training loss 0.05415818840265274 Validation loss 0.052217207849025726 Accuracy 0.47600001096725464 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.978, dtype=float32), array(0.898, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12500 Training loss 0.048936694860458374 Validation loss 0.051944684237241745 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.976, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12510 Training loss 0.05564682185649872 Validation loss 0.052244242280721664 Accuracy 0.47620001435279846 Accuracies by class [array(0., dtype=float32), array(0.909, dtype=float32), array(0.93, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12520 Training loss 0.05159664899110794 Validation loss 0.05271894857287407 Accuracy 0.47110000252723694 Accuracies by class [array(0., dtype=float32), array(0.906, dtype=float32), array(0.933, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12530 Training loss 0.049891047179698944 Validation loss 0.052233945578336716 Accuracy 0.47600001096725464 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.977, dtype=float32), array(0.9, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12540 Training loss 0.05397413671016693 Validation loss 0.05322449654340744 Accuracy 0.46619999408721924 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.981, dtype=float32), array(0.883, dtype=float32), array(0., dtype=float32), array(0.901, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12550 Training loss 0.053923409432172775 Validation loss 0.052208803594112396 Accuracy 0.47609999775886536 Accuracies by class [array(0., dtype=float32), array(0.931, dtype=float32), array(0.97, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12560 Training loss 0.049591146409511566 Validation loss 0.052458036690950394 Accuracy 0.4740000069141388 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.983, dtype=float32), array(0.894, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12570 Training loss 0.051350951194763184 Validation loss 0.0521080456674099 Accuracy 0.47760000824928284 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.976, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12580 Training loss 0.05209076777100563 Validation loss 0.05202329158782959 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.93, dtype=float32), array(0.97, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12590 Training loss 0.05228447914123535 Validation loss 0.0524478405714035 Accuracy 0.4745999872684479 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.98, dtype=float32), array(0.888, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12600 Training loss 0.048945456743240356 Validation loss 0.052318423986434937 Accuracy 0.4758000075817108 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.979, dtype=float32), array(0.878, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12610 Training loss 0.056711096316576004 Validation loss 0.051700614392757416 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.954, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12620 Training loss 0.05115709826350212 Validation loss 0.05169914662837982 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.953, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12630 Training loss 0.05279890075325966 Validation loss 0.05192987248301506 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(0.938, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12640 Training loss 0.05194634944200516 Validation loss 0.05176181718707085 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.95, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12650 Training loss 0.051095325499773026 Validation loss 0.05169014632701874 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.973, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12660 Training loss 0.05007626861333847 Validation loss 0.052162762731313705 Accuracy 0.4772000014781952 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.977, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12670 Training loss 0.049399636685848236 Validation loss 0.05201801657676697 Accuracy 0.47850000858306885 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(0.974, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12680 Training loss 0.05196458101272583 Validation loss 0.052119940519332886 Accuracy 0.4771000146865845 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.976, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12690 Training loss 0.05327901244163513 Validation loss 0.05272515118122101 Accuracy 0.47130000591278076 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.986, dtype=float32), array(0.875, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12700 Training loss 0.04860064759850502 Validation loss 0.05247623845934868 Accuracy 0.4738999903202057 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.98, dtype=float32), array(0.902, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12710 Training loss 0.051481932401657104 Validation loss 0.051962465047836304 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.921, dtype=float32), array(0.971, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12720 Training loss 0.050162967294454575 Validation loss 0.051818348467350006 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.962, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12730 Training loss 0.05013531818985939 Validation loss 0.05244698375463486 Accuracy 0.4745999872684479 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.981, dtype=float32), array(0.88, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12740 Training loss 0.05141126364469528 Validation loss 0.05170648545026779 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.966, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12750 Training loss 0.050915129482746124 Validation loss 0.05159829184412956 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.96, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12760 Training loss 0.04931005463004112 Validation loss 0.05174446105957031 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.936, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12770 Training loss 0.0514947809278965 Validation loss 0.05165741220116615 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.938, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12780 Training loss 0.05269859358668327 Validation loss 0.05161528289318085 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.945, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12790 Training loss 0.046966735273599625 Validation loss 0.05164089426398277 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.939, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12800 Training loss 0.052141666412353516 Validation loss 0.0519772507250309 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.905, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12810 Training loss 0.051264528185129166 Validation loss 0.051650408655405045 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.947, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12820 Training loss 0.05123541131615639 Validation loss 0.05162179842591286 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.96, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12830 Training loss 0.04979591444134712 Validation loss 0.05230731889605522 Accuracy 0.4754999876022339 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.966, dtype=float32), array(0.857, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12840 Training loss 0.05345265567302704 Validation loss 0.05180298909544945 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.973, dtype=float32), array(0.916, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12850 Training loss 0.050412293523550034 Validation loss 0.05239212512969971 Accuracy 0.474700003862381 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.974, dtype=float32), array(0.845, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12860 Training loss 0.05215960741043091 Validation loss 0.05222010612487793 Accuracy 0.4767000079154968 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.969, dtype=float32), array(0.866, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12870 Training loss 0.05141757056117058 Validation loss 0.051816873252391815 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.965, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12880 Training loss 0.05066320672631264 Validation loss 0.0519363135099411 Accuracy 0.4799000024795532 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.971, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12890 Training loss 0.05180169269442558 Validation loss 0.05230884626507759 Accuracy 0.47620001435279846 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.98, dtype=float32), array(0.901, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12900 Training loss 0.051806483417749405 Validation loss 0.051607560366392136 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.957, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12910 Training loss 0.05112181976437569 Validation loss 0.05212421715259552 Accuracy 0.4781000018119812 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.921, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12920 Training loss 0.050784651190042496 Validation loss 0.0516243502497673 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.953, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12930 Training loss 0.04951701685786247 Validation loss 0.05181866139173508 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.913, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12940 Training loss 0.05471980199217796 Validation loss 0.051569677889347076 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.962, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12950 Training loss 0.05175034701824188 Validation loss 0.052055906504392624 Accuracy 0.47760000824928284 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.966, dtype=float32), array(0.89, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12960 Training loss 0.05398052930831909 Validation loss 0.0519566535949707 Accuracy 0.4787999987602234 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.969, dtype=float32), array(0.902, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12970 Training loss 0.04964042082428932 Validation loss 0.05163320153951645 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.961, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12980 Training loss 0.0519174262881279 Validation loss 0.05177679657936096 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.96, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 12990 Training loss 0.054775696247816086 Validation loss 0.051538802683353424 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.935, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13000 Training loss 0.05016583576798439 Validation loss 0.051600728183984756 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.934, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13010 Training loss 0.047553692013025284 Validation loss 0.05164659023284912 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.931, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13020 Training loss 0.051559362560510635 Validation loss 0.05205300822854042 Accuracy 0.4781999886035919 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.92, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13030 Training loss 0.055045388638973236 Validation loss 0.05191551148891449 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.915, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13040 Training loss 0.05123204365372658 Validation loss 0.05216415971517563 Accuracy 0.47690001130104065 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.899, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13050 Training loss 0.051424358040094376 Validation loss 0.0519050769507885 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.929, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13060 Training loss 0.050158534198999405 Validation loss 0.051932401955127716 Accuracy 0.47920000553131104 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.958, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13070 Training loss 0.05020787939429283 Validation loss 0.05149801820516586 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.957, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13080 Training loss 0.05148596316576004 Validation loss 0.05179538577795029 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.931, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13090 Training loss 0.05169883742928505 Validation loss 0.05159458890557289 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.942, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13100 Training loss 0.05404123291373253 Validation loss 0.05200183764100075 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.975, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13110 Training loss 0.05083024501800537 Validation loss 0.05184512585401535 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.972, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13120 Training loss 0.05050814896821976 Validation loss 0.05157076567411423 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.961, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13130 Training loss 0.05303970351815224 Validation loss 0.051672451198101044 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.967, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13140 Training loss 0.051326826214790344 Validation loss 0.05197456106543541 Accuracy 0.4788999855518341 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.968, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13150 Training loss 0.05216193571686745 Validation loss 0.05162260681390762 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.963, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13160 Training loss 0.051525626331567764 Validation loss 0.051795292645692825 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.974, dtype=float32), array(0.916, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13170 Training loss 0.05254383757710457 Validation loss 0.05187198147177696 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.978, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13180 Training loss 0.05210908502340317 Validation loss 0.05163426324725151 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.96, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13190 Training loss 0.05082089081406593 Validation loss 0.05154820904135704 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.943, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13200 Training loss 0.05028475821018219 Validation loss 0.05179755762219429 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.929, dtype=float32), array(0.941, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13210 Training loss 0.052640676498413086 Validation loss 0.05161463841795921 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.954, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13220 Training loss 0.04960838332772255 Validation loss 0.05171937122941017 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.95, dtype=float32), array(0.919, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13230 Training loss 0.05084395408630371 Validation loss 0.0519358292222023 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.909, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13240 Training loss 0.050690118223428726 Validation loss 0.051796942949295044 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.922, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13250 Training loss 0.049439214169979095 Validation loss 0.05179893225431442 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.912, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13260 Training loss 0.05013048276305199 Validation loss 0.05157522112131119 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.96, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13270 Training loss 0.05435493588447571 Validation loss 0.05161885917186737 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.957, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13280 Training loss 0.04860218986868858 Validation loss 0.05172047019004822 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.972, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13290 Training loss 0.053130339831113815 Validation loss 0.051471609622240067 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.964, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13300 Training loss 0.046029698103666306 Validation loss 0.05160106346011162 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.947, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13310 Training loss 0.04714973270893097 Validation loss 0.05157063901424408 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.959, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13320 Training loss 0.051687899976968765 Validation loss 0.05153139680624008 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.961, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13330 Training loss 0.04738582298159599 Validation loss 0.051971834152936935 Accuracy 0.47859999537467957 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.972, dtype=float32), array(0.894, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13340 Training loss 0.05052439495921135 Validation loss 0.05162061005830765 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.968, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13350 Training loss 0.05170835182070732 Validation loss 0.05165177583694458 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.965, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13360 Training loss 0.05291655659675598 Validation loss 0.05154397338628769 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.948, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13370 Training loss 0.051971565932035446 Validation loss 0.05163782089948654 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.933, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13380 Training loss 0.050794366747140884 Validation loss 0.05197932571172714 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.936, dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13390 Training loss 0.05304614081978798 Validation loss 0.0522182434797287 Accuracy 0.4763999879360199 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.962, dtype=float32), array(0.871, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13400 Training loss 0.05075700953602791 Validation loss 0.05167463794350624 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.966, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13410 Training loss 0.05300880968570709 Validation loss 0.05164722725749016 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.963, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13420 Training loss 0.05076132342219353 Validation loss 0.05169028416275978 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.953, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13430 Training loss 0.050977423787117004 Validation loss 0.051618896424770355 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.964, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13440 Training loss 0.05184364691376686 Validation loss 0.051910851150751114 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.945, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13450 Training loss 0.05110478028655052 Validation loss 0.052005067467689514 Accuracy 0.4778999984264374 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.967, dtype=float32), array(0.903, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13460 Training loss 0.05180077999830246 Validation loss 0.052000872790813446 Accuracy 0.47839999198913574 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.977, dtype=float32), array(0.897, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13470 Training loss 0.05318402498960495 Validation loss 0.0515616312623024 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.965, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13480 Training loss 0.05197871848940849 Validation loss 0.051840413361787796 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.975, dtype=float32), array(0.907, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13490 Training loss 0.050113387405872345 Validation loss 0.05165543034672737 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.955, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13500 Training loss 0.05238135904073715 Validation loss 0.05172156170010567 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.956, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13510 Training loss 0.05213847756385803 Validation loss 0.051773592829704285 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.953, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13520 Training loss 0.05164492130279541 Validation loss 0.05184316262602806 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.954, dtype=float32), array(0.907, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13530 Training loss 0.050387680530548096 Validation loss 0.05160227045416832 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.946, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13540 Training loss 0.051432717591524124 Validation loss 0.05198436602950096 Accuracy 0.4781000018119812 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.908, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13550 Training loss 0.05289090797305107 Validation loss 0.05248722806572914 Accuracy 0.4731999933719635 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.896, dtype=float32), array(0.89, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13560 Training loss 0.0496646948158741 Validation loss 0.052193935960531235 Accuracy 0.47620001435279846 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.927, dtype=float32), array(0.905, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13570 Training loss 0.053216878324747086 Validation loss 0.05193757265806198 Accuracy 0.47920000553131104 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.931, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13580 Training loss 0.05331960693001747 Validation loss 0.0519094318151474 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.955, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13590 Training loss 0.05288613960146904 Validation loss 0.052105579525232315 Accuracy 0.4772000014781952 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.951, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13600 Training loss 0.05184576287865639 Validation loss 0.0521232970058918 Accuracy 0.477400004863739 Accuracies by class [array(0., dtype=float32), array(0.92, dtype=float32), array(0.919, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13610 Training loss 0.049969349056482315 Validation loss 0.052273768931627274 Accuracy 0.47600001096725464 Accuracies by class [array(0., dtype=float32), array(0.925, dtype=float32), array(0.961, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.91, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13620 Training loss 0.05262455716729164 Validation loss 0.051771651953458786 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.952, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13630 Training loss 0.04972328618168831 Validation loss 0.05253368616104126 Accuracy 0.47269999980926514 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(0.951, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.902, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13640 Training loss 0.050011228770017624 Validation loss 0.052376843988895416 Accuracy 0.47450000047683716 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.974, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.919, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13650 Training loss 0.05229547992348671 Validation loss 0.052068427205085754 Accuracy 0.4771000146865845 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.971, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13660 Training loss 0.05189015716314316 Validation loss 0.051895853132009506 Accuracy 0.47940000891685486 Accuracies by class [array(0., dtype=float32), array(0.936, dtype=float32), array(0.939, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13670 Training loss 0.05080047622323036 Validation loss 0.05169236660003662 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.967, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13680 Training loss 0.05122528225183487 Validation loss 0.05185292661190033 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.917, dtype=float32), array(0.951, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13690 Training loss 0.051279038190841675 Validation loss 0.05183784291148186 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.925, dtype=float32), array(0.952, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13700 Training loss 0.05055700242519379 Validation loss 0.052073244005441666 Accuracy 0.47769999504089355 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.965, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13710 Training loss 0.04900854453444481 Validation loss 0.0516921766102314 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.95, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13720 Training loss 0.05366156995296478 Validation loss 0.05186711996793747 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.976, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13730 Training loss 0.050427984446287155 Validation loss 0.0521525964140892 Accuracy 0.47699999809265137 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.982, dtype=float32), array(0.896, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13740 Training loss 0.051751744002103806 Validation loss 0.05221208557486534 Accuracy 0.47679999470710754 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.982, dtype=float32), array(0.896, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13750 Training loss 0.049468450248241425 Validation loss 0.051913484930992126 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.977, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13760 Training loss 0.051036398857831955 Validation loss 0.05183366686105728 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.971, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13770 Training loss 0.05144154280424118 Validation loss 0.051810503005981445 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.972, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13780 Training loss 0.05417611822485924 Validation loss 0.0518157035112381 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.971, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13790 Training loss 0.049427978694438934 Validation loss 0.05152066797018051 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.953, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13800 Training loss 0.05375324562191963 Validation loss 0.05151475593447685 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.958, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13810 Training loss 0.05382118746638298 Validation loss 0.05159510672092438 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.941, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13820 Training loss 0.05165642872452736 Validation loss 0.05156807228922844 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.93, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13830 Training loss 0.05130500718951225 Validation loss 0.05155743286013603 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.93, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13840 Training loss 0.05018549785017967 Validation loss 0.05180094763636589 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.929, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13850 Training loss 0.05090128630399704 Validation loss 0.05177517607808113 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.933, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13860 Training loss 0.05172155797481537 Validation loss 0.05192595720291138 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.912, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13870 Training loss 0.04790304973721504 Validation loss 0.051669809967279434 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.948, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13880 Training loss 0.05077924579381943 Validation loss 0.051739584654569626 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.934, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13890 Training loss 0.049067575484514236 Validation loss 0.052194420248270035 Accuracy 0.4765999913215637 Accuracies by class [array(0., dtype=float32), array(0.936, dtype=float32), array(0.885, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13900 Training loss 0.054616596549749374 Validation loss 0.05168310925364494 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.954, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13910 Training loss 0.05111158266663551 Validation loss 0.051800619810819626 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.973, dtype=float32), array(0.909, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13920 Training loss 0.05246961489319801 Validation loss 0.05158410221338272 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.955, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13930 Training loss 0.05342930182814598 Validation loss 0.05166766420006752 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.949, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13940 Training loss 0.05055682733654976 Validation loss 0.051620643585920334 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.963, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13950 Training loss 0.053879495710134506 Validation loss 0.05153212323784828 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.952, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13960 Training loss 0.04636574909090996 Validation loss 0.05158606916666031 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.94, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13970 Training loss 0.05284128710627556 Validation loss 0.051684703677892685 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.926, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13980 Training loss 0.05200512707233429 Validation loss 0.05163707211613655 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.96, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 13990 Training loss 0.05085980147123337 Validation loss 0.05195411294698715 Accuracy 0.47870001196861267 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.907, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14000 Training loss 0.052323274314403534 Validation loss 0.051848046481609344 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.913, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14010 Training loss 0.051744505763053894 Validation loss 0.05152660235762596 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.949, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14020 Training loss 0.05309600755572319 Validation loss 0.05147871747612953 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.952, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14030 Training loss 0.050108153373003006 Validation loss 0.05193357914686203 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.913, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14040 Training loss 0.05340392142534256 Validation loss 0.05198601633310318 Accuracy 0.4781999886035919 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.888, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14050 Training loss 0.05102396756410599 Validation loss 0.051615748554468155 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.936, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14060 Training loss 0.051249805837869644 Validation loss 0.05182085558772087 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.96, dtype=float32), array(0.905, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14070 Training loss 0.05305438116192818 Validation loss 0.05148383975028992 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.964, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14080 Training loss 0.051041536033153534 Validation loss 0.0515395849943161 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.971, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14090 Training loss 0.04499625042080879 Validation loss 0.05155309662222862 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.969, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14100 Training loss 0.0522880032658577 Validation loss 0.05165690928697586 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.957, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14110 Training loss 0.05021212622523308 Validation loss 0.051748570054769516 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.968, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14120 Training loss 0.050665851682424545 Validation loss 0.05170237272977829 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.966, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14130 Training loss 0.04667242616415024 Validation loss 0.05165410041809082 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.968, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14140 Training loss 0.04925592616200447 Validation loss 0.05146219581365585 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.961, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14150 Training loss 0.05016331002116203 Validation loss 0.05154924467206001 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.958, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14160 Training loss 0.049626924097537994 Validation loss 0.051522187888622284 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.946, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14170 Training loss 0.04976082965731621 Validation loss 0.05181027203798294 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.938, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14180 Training loss 0.05111950635910034 Validation loss 0.05195463448762894 Accuracy 0.4779999852180481 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.965, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14190 Training loss 0.05033157020807266 Validation loss 0.05179045721888542 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.978, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14200 Training loss 0.04943748936057091 Validation loss 0.05169028416275978 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.968, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14210 Training loss 0.05309999734163284 Validation loss 0.051604047417640686 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.967, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14220 Training loss 0.05298149213194847 Validation loss 0.051388006657361984 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.963, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14230 Training loss 0.05224030464887619 Validation loss 0.051441553980112076 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.961, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14240 Training loss 0.049682557582855225 Validation loss 0.05170731619000435 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.968, dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14250 Training loss 0.05339660495519638 Validation loss 0.05164323374629021 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.958, dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14260 Training loss 0.05162443593144417 Validation loss 0.0515313483774662 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.959, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14270 Training loss 0.05182347446680069 Validation loss 0.05149536579847336 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.954, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14280 Training loss 0.04872581362724304 Validation loss 0.051634468138217926 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.921, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14290 Training loss 0.05110043287277222 Validation loss 0.052896078675985336 Accuracy 0.46889999508857727 Accuracies by class [array(0., dtype=float32), array(0.918, dtype=float32), array(0.85, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14300 Training loss 0.04808962345123291 Validation loss 0.052129462361335754 Accuracy 0.4765999913215637 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.931, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14310 Training loss 0.05046618729829788 Validation loss 0.05162772908806801 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.959, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14320 Training loss 0.05225765332579613 Validation loss 0.052945926785469055 Accuracy 0.46779999136924744 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.839, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14330 Training loss 0.04898643121123314 Validation loss 0.05203261598944664 Accuracy 0.477400004863739 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.879, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14340 Training loss 0.04887646809220314 Validation loss 0.05167253315448761 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.935, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14350 Training loss 0.055544957518577576 Validation loss 0.05170203745365143 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.929, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14360 Training loss 0.05031042918562889 Validation loss 0.051727890968322754 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.947, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14370 Training loss 0.051754098385572433 Validation loss 0.05170053616166115 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.932, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14380 Training loss 0.05197025462985039 Validation loss 0.05237971618771553 Accuracy 0.4742000102996826 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.92, dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14390 Training loss 0.055882155895233154 Validation loss 0.0518258772790432 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.923, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14400 Training loss 0.05362546443939209 Validation loss 0.05185289680957794 Accuracy 0.4799000024795532 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.967, dtype=float32), array(0.898, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14410 Training loss 0.052265752106904984 Validation loss 0.05185762792825699 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.96, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14420 Training loss 0.05263417586684227 Validation loss 0.05164676904678345 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.954, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14430 Training loss 0.05357768386602402 Validation loss 0.051852449774742126 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.937, dtype=float32), array(0.924, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14440 Training loss 0.04709189012646675 Validation loss 0.051996033638715744 Accuracy 0.47859999537467957 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.899, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14450 Training loss 0.05277051404118538 Validation loss 0.0518522784113884 Accuracy 0.47940000891685486 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.91, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14460 Training loss 0.052860189229249954 Validation loss 0.05203036963939667 Accuracy 0.4779999852180481 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.882, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14470 Training loss 0.05061173066496849 Validation loss 0.05158209428191185 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.946, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14480 Training loss 0.049558792263269424 Validation loss 0.05219888687133789 Accuracy 0.4758000075817108 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.906, dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14490 Training loss 0.04956112802028656 Validation loss 0.05167103186249733 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.955, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14500 Training loss 0.056138355284929276 Validation loss 0.05165770649909973 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.96, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14510 Training loss 0.05287918448448181 Validation loss 0.051675353199243546 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.968, dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14520 Training loss 0.05126176029443741 Validation loss 0.051886461675167084 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.954, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14530 Training loss 0.05265311524271965 Validation loss 0.05192138999700546 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.948, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14540 Training loss 0.05508168414235115 Validation loss 0.05209903419017792 Accuracy 0.47780001163482666 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.956, dtype=float32), array(0.888, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14550 Training loss 0.051513753831386566 Validation loss 0.05179484561085701 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.959, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14560 Training loss 0.050859104841947556 Validation loss 0.05165039002895355 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.957, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14570 Training loss 0.0544343926012516 Validation loss 0.05181245133280754 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.965, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14580 Training loss 0.05026192590594292 Validation loss 0.05226677283644676 Accuracy 0.47519999742507935 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.881, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14590 Training loss 0.056041743606328964 Validation loss 0.05173344910144806 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.962, dtype=float32), array(0.918, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14600 Training loss 0.052684515714645386 Validation loss 0.051880430430173874 Accuracy 0.4787999987602234 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.939, dtype=float32), array(0.91, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14610 Training loss 0.05174398794770241 Validation loss 0.05167820677161217 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.954, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14620 Training loss 0.0531279481947422 Validation loss 0.05167870968580246 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.961, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14630 Training loss 0.05360931530594826 Validation loss 0.05168880894780159 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.965, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14640 Training loss 0.04965410754084587 Validation loss 0.05169588699936867 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.972, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14650 Training loss 0.049890320748090744 Validation loss 0.05166066437959671 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.958, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14660 Training loss 0.053691234439611435 Validation loss 0.051557499915361404 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.956, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14670 Training loss 0.04706417769193649 Validation loss 0.05167548730969429 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.943, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14680 Training loss 0.05289788171648979 Validation loss 0.05160652846097946 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.961, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14690 Training loss 0.05183756351470947 Validation loss 0.051574498414993286 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.959, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14700 Training loss 0.0506528876721859 Validation loss 0.05173543840646744 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.954, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14710 Training loss 0.052293725311756134 Validation loss 0.05163409188389778 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.944, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14720 Training loss 0.05370131507515907 Validation loss 0.052420176565647125 Accuracy 0.47360000014305115 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.867, dtype=float32), array(0.916, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14730 Training loss 0.05211573466658592 Validation loss 0.05193279683589935 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.905, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14740 Training loss 0.05399145558476448 Validation loss 0.05174034833908081 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.931, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14750 Training loss 0.05071955546736717 Validation loss 0.05189209803938866 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.936, dtype=float32), array(0.945, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14760 Training loss 0.04758751764893532 Validation loss 0.051911551505327225 Accuracy 0.47929999232292175 Accuracies by class [array(0., dtype=float32), array(0.914, dtype=float32), array(0.958, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14770 Training loss 0.05286378040909767 Validation loss 0.05192060023546219 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.92, dtype=float32), array(0.934, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14780 Training loss 0.050501033663749695 Validation loss 0.051629893481731415 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.937, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14790 Training loss 0.053379125893116 Validation loss 0.05157523974776268 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.957, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14800 Training loss 0.05007246509194374 Validation loss 0.05238711088895798 Accuracy 0.4745999872684479 Accuracies by class [array(0., dtype=float32), array(0.879, dtype=float32), array(0.92, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14810 Training loss 0.04972825199365616 Validation loss 0.051614124327898026 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.95, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14820 Training loss 0.04770863428711891 Validation loss 0.05165936052799225 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.947, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14830 Training loss 0.04975927248597145 Validation loss 0.05183045566082001 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.953, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14840 Training loss 0.049482885748147964 Validation loss 0.0518595315515995 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.946, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14850 Training loss 0.049813978374004364 Validation loss 0.05171073600649834 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.931, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14860 Training loss 0.053077708929777145 Validation loss 0.05151762813329697 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.96, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14870 Training loss 0.05117421969771385 Validation loss 0.051483552902936935 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.938, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14880 Training loss 0.05062925070524216 Validation loss 0.05155177786946297 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.97, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14890 Training loss 0.049011051654815674 Validation loss 0.051539815962314606 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.963, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14900 Training loss 0.05314259976148605 Validation loss 0.05168541520833969 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.936, dtype=float32), array(0.946, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14910 Training loss 0.05236079543828964 Validation loss 0.0515570305287838 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.943, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14920 Training loss 0.05234324187040329 Validation loss 0.051592402160167694 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.954, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14930 Training loss 0.04863797500729561 Validation loss 0.05149121209979057 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.951, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14940 Training loss 0.053120605647563934 Validation loss 0.05148337781429291 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.959, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14950 Training loss 0.04974393546581268 Validation loss 0.05147731304168701 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.937, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14960 Training loss 0.04937901720404625 Validation loss 0.05150582641363144 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.944, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14970 Training loss 0.052677541971206665 Validation loss 0.05169418454170227 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.929, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14980 Training loss 0.05309759080410004 Validation loss 0.051557060331106186 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.934, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 14990 Training loss 0.05241944268345833 Validation loss 0.051665786653757095 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.923, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15000 Training loss 0.05058230459690094 Validation loss 0.05193760618567467 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.936, dtype=float32), array(0.929, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15010 Training loss 0.05293497070670128 Validation loss 0.05171474441885948 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.938, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15020 Training loss 0.053479257971048355 Validation loss 0.0515599399805069 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.945, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15030 Training loss 0.05018629878759384 Validation loss 0.05151303857564926 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.969, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15040 Training loss 0.04955284297466278 Validation loss 0.051746513694524765 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.947, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15050 Training loss 0.051585450768470764 Validation loss 0.05164379999041557 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.95, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15060 Training loss 0.05173283442854881 Validation loss 0.051612235605716705 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.94, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15070 Training loss 0.05017848685383797 Validation loss 0.05171887204051018 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.914, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15080 Training loss 0.05388375371694565 Validation loss 0.05172343924641609 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.916, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15090 Training loss 0.050376832485198975 Validation loss 0.05152963846921921 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.958, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15100 Training loss 0.05262730270624161 Validation loss 0.05156518891453743 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.953, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15110 Training loss 0.053699951618909836 Validation loss 0.05164568871259689 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.921, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15120 Training loss 0.05003982409834862 Validation loss 0.05171270668506622 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.944, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15130 Training loss 0.04995550587773323 Validation loss 0.05169152468442917 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.946, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15140 Training loss 0.04970972239971161 Validation loss 0.05177241191267967 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.947, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15150 Training loss 0.04946523159742355 Validation loss 0.051575224846601486 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.96, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15160 Training loss 0.05146954953670502 Validation loss 0.05157209932804108 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.951, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15170 Training loss 0.050502292811870575 Validation loss 0.05154411122202873 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.958, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15180 Training loss 0.04942556098103523 Validation loss 0.05161886662244797 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.973, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15190 Training loss 0.05154913291335106 Validation loss 0.051705501973629 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.97, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15200 Training loss 0.05292225256562233 Validation loss 0.05172142758965492 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.969, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15210 Training loss 0.047778207808732986 Validation loss 0.05157002434134483 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.947, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15220 Training loss 0.04866180568933487 Validation loss 0.05172444507479668 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.95, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15230 Training loss 0.051367707550525665 Validation loss 0.05172156170010567 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.961, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15240 Training loss 0.05153605714440346 Validation loss 0.05159556493163109 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.959, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15250 Training loss 0.050140202045440674 Validation loss 0.05158667638897896 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.935, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15260 Training loss 0.05279868468642235 Validation loss 0.05207885801792145 Accuracy 0.47699999809265137 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.922, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15270 Training loss 0.05267222970724106 Validation loss 0.05169054865837097 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.932, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15280 Training loss 0.054348770529031754 Validation loss 0.05158902332186699 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.955, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15290 Training loss 0.05316666513681412 Validation loss 0.0517103411257267 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.947, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15300 Training loss 0.051478151232004166 Validation loss 0.051724325865507126 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.968, dtype=float32), array(0.916, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15310 Training loss 0.05330267176032066 Validation loss 0.051705341786146164 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.942, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15320 Training loss 0.05320111662149429 Validation loss 0.051642198115587234 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.919, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15330 Training loss 0.05012403801083565 Validation loss 0.05190068483352661 Accuracy 0.4799000024795532 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.907, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15340 Training loss 0.05205070972442627 Validation loss 0.05140445753931999 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.946, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15350 Training loss 0.05158712714910507 Validation loss 0.05161099508404732 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.934, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15360 Training loss 0.051526330411434174 Validation loss 0.05184488743543625 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.937, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15370 Training loss 0.049321249127388 Validation loss 0.05196728929877281 Accuracy 0.47859999537467957 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(0.906, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15380 Training loss 0.04934234544634819 Validation loss 0.05155574157834053 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.947, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15390 Training loss 0.05255982279777527 Validation loss 0.051685087382793427 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.967, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15400 Training loss 0.051145900040864944 Validation loss 0.05193445459008217 Accuracy 0.47870001196861267 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.968, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15410 Training loss 0.049482524394989014 Validation loss 0.051797039806842804 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.958, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15420 Training loss 0.05188964679837227 Validation loss 0.051844701170921326 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15430 Training loss 0.05181353911757469 Validation loss 0.05182075500488281 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.935, dtype=float32), array(0.962, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15440 Training loss 0.04989287257194519 Validation loss 0.05202705040574074 Accuracy 0.47780001163482666 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.97, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15450 Training loss 0.052383504807949066 Validation loss 0.051642708480358124 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.959, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15460 Training loss 0.051792774349451065 Validation loss 0.05169161781668663 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.952, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15470 Training loss 0.05234459042549133 Validation loss 0.05188736692070961 Accuracy 0.4796000123023987 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.923, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15480 Training loss 0.047896113246679306 Validation loss 0.051503267139196396 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.934, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15490 Training loss 0.052174732089042664 Validation loss 0.05181145668029785 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.921, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15500 Training loss 0.05008002743124962 Validation loss 0.05191804841160774 Accuracy 0.47920000553131104 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.936, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15510 Training loss 0.05179878696799278 Validation loss 0.05186733230948448 Accuracy 0.47929999232292175 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.92, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15520 Training loss 0.0492495521903038 Validation loss 0.051821041852235794 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.93, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15530 Training loss 0.05205371975898743 Validation loss 0.05184255912899971 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.92, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15540 Training loss 0.05050570145249367 Validation loss 0.051805682480335236 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.928, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15550 Training loss 0.05485527217388153 Validation loss 0.0517030768096447 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.918, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15560 Training loss 0.05306889861822128 Validation loss 0.051589541137218475 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.936, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15570 Training loss 0.049606431275606155 Validation loss 0.05159388855099678 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.962, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15580 Training loss 0.04986939579248428 Validation loss 0.05180294066667557 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.95, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15590 Training loss 0.05240681394934654 Validation loss 0.05178813636302948 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.974, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15600 Training loss 0.04677640274167061 Validation loss 0.05157743766903877 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.967, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15610 Training loss 0.05178341269493103 Validation loss 0.051512472331523895 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.953, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15620 Training loss 0.05284932628273964 Validation loss 0.05169815570116043 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.925, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15630 Training loss 0.04928882420063019 Validation loss 0.05160100758075714 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.934, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15640 Training loss 0.0498647578060627 Validation loss 0.05161174386739731 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.957, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15650 Training loss 0.052002083510160446 Validation loss 0.0517299510538578 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.955, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15660 Training loss 0.05255250632762909 Validation loss 0.05200978368520737 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.961, dtype=float32), array(0.913, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15670 Training loss 0.04700005054473877 Validation loss 0.05137703940272331 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.951, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15680 Training loss 0.05064716562628746 Validation loss 0.05161471292376518 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.932, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15690 Training loss 0.05410301312804222 Validation loss 0.051885850727558136 Accuracy 0.47920000553131104 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.885, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15700 Training loss 0.048752233386039734 Validation loss 0.051795050501823425 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.933, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15710 Training loss 0.0501389279961586 Validation loss 0.05149547755718231 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.957, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15720 Training loss 0.05234504118561745 Validation loss 0.0519670769572258 Accuracy 0.47850000858306885 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.914, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15730 Training loss 0.049822356551885605 Validation loss 0.05136369541287422 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.945, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15740 Training loss 0.04922547936439514 Validation loss 0.051778268069028854 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.948, dtype=float32), array(0.921, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15750 Training loss 0.0500851608812809 Validation loss 0.05196283385157585 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.954, dtype=float32), array(0.898, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15760 Training loss 0.05016840994358063 Validation loss 0.05135186389088631 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.958, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15770 Training loss 0.053694915026426315 Validation loss 0.051777735352516174 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.927, dtype=float32), array(0.929, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15780 Training loss 0.05072086676955223 Validation loss 0.05189105495810509 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.932, dtype=float32), array(0.936, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15790 Training loss 0.049134090542793274 Validation loss 0.05257746949791908 Accuracy 0.4726000130176544 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.924, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.903, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15800 Training loss 0.05143255740404129 Validation loss 0.05281136557459831 Accuracy 0.47029998898506165 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.943, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.863, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15810 Training loss 0.04969843477010727 Validation loss 0.0515022948384285 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.936, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15820 Training loss 0.0483165979385376 Validation loss 0.051580674946308136 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.925, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15830 Training loss 0.04990900307893753 Validation loss 0.0517081692814827 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.916, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15840 Training loss 0.052009761333465576 Validation loss 0.05153295025229454 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.944, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15850 Training loss 0.05005925893783569 Validation loss 0.051688022911548615 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.968, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15860 Training loss 0.05016366392374039 Validation loss 0.051620449870824814 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.963, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15870 Training loss 0.04894723370671272 Validation loss 0.05177837237715721 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.978, dtype=float32), array(0.913, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15880 Training loss 0.05221237242221832 Validation loss 0.05193446949124336 Accuracy 0.47870001196861267 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.981, dtype=float32), array(0.884, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15890 Training loss 0.0556122250854969 Validation loss 0.05209602043032646 Accuracy 0.47699999809265137 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.968, dtype=float32), array(0.895, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15900 Training loss 0.045942194759845734 Validation loss 0.05171190947294235 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.964, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15910 Training loss 0.056285783648490906 Validation loss 0.052353229373693466 Accuracy 0.4749000072479248 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.976, dtype=float32), array(0.841, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15920 Training loss 0.0524718277156353 Validation loss 0.05208234488964081 Accuracy 0.4767000079154968 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.974, dtype=float32), array(0.854, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15930 Training loss 0.050046730786561966 Validation loss 0.051936544477939606 Accuracy 0.47929999232292175 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.981, dtype=float32), array(0.899, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15940 Training loss 0.051187027245759964 Validation loss 0.05160956457257271 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.971, dtype=float32), array(0.916, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15950 Training loss 0.050285980105400085 Validation loss 0.051558494567871094 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.956, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15960 Training loss 0.048918526619672775 Validation loss 0.051774926483631134 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.935, dtype=float32), array(0.933, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15970 Training loss 0.04997388646006584 Validation loss 0.05164741352200508 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.939, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15980 Training loss 0.05429450422525406 Validation loss 0.05164863169193268 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.949, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 15990 Training loss 0.051511794328689575 Validation loss 0.05156716704368591 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.96, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16000 Training loss 0.051468007266521454 Validation loss 0.051823873072862625 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.931, dtype=float32), array(0.965, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16010 Training loss 0.051372773945331573 Validation loss 0.05173637717962265 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.931, dtype=float32), array(0.951, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16020 Training loss 0.050122953951358795 Validation loss 0.05160226672887802 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.957, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16030 Training loss 0.05529788136482239 Validation loss 0.05148348584771156 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.968, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16040 Training loss 0.050359614193439484 Validation loss 0.051720429211854935 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.964, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16050 Training loss 0.048813436180353165 Validation loss 0.05158250406384468 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.944, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16060 Training loss 0.051562510430812836 Validation loss 0.05153298005461693 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.937, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16070 Training loss 0.053559210151433945 Validation loss 0.051629092544317245 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.925, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16080 Training loss 0.049651265144348145 Validation loss 0.0514899417757988 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.94, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16090 Training loss 0.050821345299482346 Validation loss 0.05143845081329346 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.945, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16100 Training loss 0.05095665156841278 Validation loss 0.051486000418663025 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.962, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16110 Training loss 0.05125126615166664 Validation loss 0.05149177089333534 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.943, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16120 Training loss 0.0482763908803463 Validation loss 0.05174717679619789 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.934, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16130 Training loss 0.049183156341314316 Validation loss 0.05138084292411804 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.955, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16140 Training loss 0.04843693599104881 Validation loss 0.051588501781225204 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.948, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16150 Training loss 0.0485573410987854 Validation loss 0.052078232169151306 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.888, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16160 Training loss 0.04781921207904816 Validation loss 0.051992226392030716 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.892, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16170 Training loss 0.05065659433603287 Validation loss 0.05152193829417229 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.946, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16180 Training loss 0.05511980131268501 Validation loss 0.051665060222148895 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.957, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16190 Training loss 0.04919404163956642 Validation loss 0.051673270761966705 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.909, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16200 Training loss 0.05336219444870949 Validation loss 0.0518006831407547 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.908, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16210 Training loss 0.05192336067557335 Validation loss 0.05166385695338249 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.941, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16220 Training loss 0.04989943280816078 Validation loss 0.051501039415597916 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.947, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16230 Training loss 0.0543932244181633 Validation loss 0.051519278436899185 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.958, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16240 Training loss 0.0537942573428154 Validation loss 0.05144369974732399 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.953, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16250 Training loss 0.05321807414293289 Validation loss 0.05153731629252434 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.963, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16260 Training loss 0.049420762807130814 Validation loss 0.05186768248677254 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.92, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16270 Training loss 0.05158187821507454 Validation loss 0.05170837417244911 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.924, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16280 Training loss 0.05006628483533859 Validation loss 0.05182710289955139 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.913, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16290 Training loss 0.0509013757109642 Validation loss 0.051551368087530136 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.951, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16300 Training loss 0.05064363777637482 Validation loss 0.051592886447906494 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.926, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16310 Training loss 0.054032038897275925 Validation loss 0.05150776356458664 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.943, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16320 Training loss 0.05155707150697708 Validation loss 0.05154332146048546 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.951, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16330 Training loss 0.04986075684428215 Validation loss 0.051581475883722305 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.946, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16340 Training loss 0.049738261848688126 Validation loss 0.0516834482550621 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.92, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16350 Training loss 0.04937780648469925 Validation loss 0.051703110337257385 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.911, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16360 Training loss 0.05077090486884117 Validation loss 0.05177271366119385 Accuracy 0.4803999960422516 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.907, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16370 Training loss 0.05295100435614586 Validation loss 0.05192272365093231 Accuracy 0.47929999232292175 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.899, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16380 Training loss 0.05173221230506897 Validation loss 0.051559820771217346 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.964, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16390 Training loss 0.05068952962756157 Validation loss 0.05169883742928505 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.97, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16400 Training loss 0.050532858818769455 Validation loss 0.051682304590940475 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.973, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16410 Training loss 0.05198725312948227 Validation loss 0.05151896923780441 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.973, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16420 Training loss 0.0538409948348999 Validation loss 0.052038755267858505 Accuracy 0.4781999886035919 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.984, dtype=float32), array(0.895, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16430 Training loss 0.050188131630420685 Validation loss 0.051519472151994705 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.961, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16440 Training loss 0.052303165197372437 Validation loss 0.051602743566036224 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16450 Training loss 0.05123686045408249 Validation loss 0.05176900327205658 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.973, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16460 Training loss 0.05283259600400925 Validation loss 0.051675375550985336 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.972, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16470 Training loss 0.04489607363939285 Validation loss 0.051621995866298676 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.964, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16480 Training loss 0.052622631192207336 Validation loss 0.05169841647148132 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.96, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16490 Training loss 0.05269293859601021 Validation loss 0.051631368696689606 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.965, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16500 Training loss 0.05031295120716095 Validation loss 0.05171296000480652 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.973, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16510 Training loss 0.052532002329826355 Validation loss 0.05159246176481247 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.951, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16520 Training loss 0.0511368066072464 Validation loss 0.051472313702106476 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.949, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16530 Training loss 0.05472349002957344 Validation loss 0.05147518962621689 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.968, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16540 Training loss 0.05140572041273117 Validation loss 0.05147969722747803 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.953, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16550 Training loss 0.05419575050473213 Validation loss 0.05157218873500824 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.963, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16560 Training loss 0.04980229586362839 Validation loss 0.051590416580438614 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.934, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16570 Training loss 0.0496651828289032 Validation loss 0.0514795295894146 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.965, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16580 Training loss 0.05058388411998749 Validation loss 0.051456160843372345 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.947, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16590 Training loss 0.053648050874471664 Validation loss 0.052141476422548294 Accuracy 0.47699999809265137 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.939, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16600 Training loss 0.05126520246267319 Validation loss 0.051474429666996 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.956, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16610 Training loss 0.05087314173579216 Validation loss 0.051472462713718414 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.971, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16620 Training loss 0.048291921615600586 Validation loss 0.05166272446513176 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.95, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16630 Training loss 0.04823915660381317 Validation loss 0.05179915949702263 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.933, dtype=float32), array(0.943, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16640 Training loss 0.050545383244752884 Validation loss 0.05165266990661621 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.945, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16650 Training loss 0.050791554152965546 Validation loss 0.051633261144161224 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.946, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16660 Training loss 0.04883115738630295 Validation loss 0.05142837390303612 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.958, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16670 Training loss 0.050346482545137405 Validation loss 0.05142936483025551 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.956, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16680 Training loss 0.052859675139188766 Validation loss 0.05142318457365036 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.962, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16690 Training loss 0.049859821796417236 Validation loss 0.051469169557094574 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.954, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16700 Training loss 0.0521937720477581 Validation loss 0.05166759714484215 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.957, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16710 Training loss 0.04795180261135101 Validation loss 0.051418736577034 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.956, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16720 Training loss 0.04968329519033432 Validation loss 0.05154282599687576 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.964, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16730 Training loss 0.05258876457810402 Validation loss 0.05153140798211098 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.956, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16740 Training loss 0.052442874759435654 Validation loss 0.05143033713102341 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.956, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16750 Training loss 0.0492306612432003 Validation loss 0.0516616515815258 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.931, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16760 Training loss 0.04899788275361061 Validation loss 0.051552049815654755 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.946, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16770 Training loss 0.05031789094209671 Validation loss 0.05138351395726204 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.956, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16780 Training loss 0.05031319335103035 Validation loss 0.0514790378510952 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.967, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16790 Training loss 0.055449146777391434 Validation loss 0.051439058035612106 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.972, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16800 Training loss 0.04954446479678154 Validation loss 0.05155133828520775 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.97, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16810 Training loss 0.04919234290719032 Validation loss 0.05166959390044212 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.973, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16820 Training loss 0.050395477563142776 Validation loss 0.05168018862605095 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.97, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16830 Training loss 0.05301632732152939 Validation loss 0.05157579109072685 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.962, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16840 Training loss 0.0493575744330883 Validation loss 0.05156071111559868 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.968, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16850 Training loss 0.05132214352488518 Validation loss 0.05162285268306732 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.968, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16860 Training loss 0.049317799508571625 Validation loss 0.05169403925538063 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.952, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16870 Training loss 0.05136021599173546 Validation loss 0.053077567368745804 Accuracy 0.46810001134872437 Accuracies by class [array(0., dtype=float32), array(0.912, dtype=float32), array(0.81, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16880 Training loss 0.05224835127592087 Validation loss 0.05179208144545555 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.917, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16890 Training loss 0.046949535608291626 Validation loss 0.05144583806395531 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.941, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16900 Training loss 0.052215393632650375 Validation loss 0.05148009583353996 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16910 Training loss 0.04830535873770714 Validation loss 0.051546961069107056 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.968, dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16920 Training loss 0.049642711877822876 Validation loss 0.05148948356509209 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.944, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16930 Training loss 0.048027221113443375 Validation loss 0.051456790417432785 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.943, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16940 Training loss 0.05441251024603844 Validation loss 0.05146082118153572 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.938, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16950 Training loss 0.051965758204460144 Validation loss 0.051504939794540405 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.942, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16960 Training loss 0.05360402911901474 Validation loss 0.051613274961709976 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.947, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16970 Training loss 0.05010135844349861 Validation loss 0.05160284787416458 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.921, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16980 Training loss 0.0551091805100441 Validation loss 0.05190064385533333 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.92, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 16990 Training loss 0.0499868169426918 Validation loss 0.0517207533121109 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.921, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17000 Training loss 0.04972042888402939 Validation loss 0.0517498254776001 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.935, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17010 Training loss 0.049183472990989685 Validation loss 0.051844190806150436 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.914, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17020 Training loss 0.0511370450258255 Validation loss 0.051779042929410934 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.911, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17030 Training loss 0.052516642957925797 Validation loss 0.051660556346178055 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.933, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17040 Training loss 0.051541659981012344 Validation loss 0.051541466265916824 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.953, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17050 Training loss 0.04614686220884323 Validation loss 0.051703013479709625 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.973, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17060 Training loss 0.050357919186353683 Validation loss 0.05175793915987015 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.968, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17070 Training loss 0.05274591222405434 Validation loss 0.05185912176966667 Accuracy 0.47929999232292175 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.934, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17080 Training loss 0.04939211532473564 Validation loss 0.05157161131501198 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.95, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17090 Training loss 0.05107472091913223 Validation loss 0.051513414829969406 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.946, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17100 Training loss 0.049650661647319794 Validation loss 0.051971763372421265 Accuracy 0.4779999852180481 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.953, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17110 Training loss 0.052172496914863586 Validation loss 0.051680561155080795 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.966, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17120 Training loss 0.05216153338551521 Validation loss 0.05160871148109436 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17130 Training loss 0.052469972521066666 Validation loss 0.051733896136283875 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.966, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17140 Training loss 0.05004407837986946 Validation loss 0.05159136280417442 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.957, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17150 Training loss 0.052496641874313354 Validation loss 0.05164958909153938 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.921, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17160 Training loss 0.05005389079451561 Validation loss 0.051618658006191254 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.933, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17170 Training loss 0.0513363853096962 Validation loss 0.05154211074113846 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.916, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17180 Training loss 0.05228408798575401 Validation loss 0.051411453634500504 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.935, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17190 Training loss 0.05375150218605995 Validation loss 0.05171719565987587 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.959, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17200 Training loss 0.05045410245656967 Validation loss 0.05147877335548401 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.954, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17210 Training loss 0.05226964130997658 Validation loss 0.05138189345598221 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.959, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17220 Training loss 0.05237501859664917 Validation loss 0.051636893302202225 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.924, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17230 Training loss 0.05293596535921097 Validation loss 0.05144406855106354 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.953, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17240 Training loss 0.05270618572831154 Validation loss 0.05146735534071922 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.965, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17250 Training loss 0.05009886249899864 Validation loss 0.0514933206140995 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.957, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17260 Training loss 0.04964308813214302 Validation loss 0.051563046872615814 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.961, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17270 Training loss 0.05233443155884743 Validation loss 0.05173111334443092 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.956, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17280 Training loss 0.05180046707391739 Validation loss 0.05154518038034439 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.948, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17290 Training loss 0.05117255821824074 Validation loss 0.05147487670183182 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.947, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17300 Training loss 0.053637757897377014 Validation loss 0.05139265954494476 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.944, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17310 Training loss 0.05143862962722778 Validation loss 0.05141139030456543 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.952, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17320 Training loss 0.05136702582240105 Validation loss 0.05138031393289566 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.953, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17330 Training loss 0.050120849162340164 Validation loss 0.05160638689994812 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.947, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17340 Training loss 0.051693886518478394 Validation loss 0.05149860680103302 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.947, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17350 Training loss 0.048840031027793884 Validation loss 0.05165247991681099 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.937, dtype=float32), array(0.937, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17360 Training loss 0.05272633954882622 Validation loss 0.05146946758031845 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.964, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17370 Training loss 0.0512821339070797 Validation loss 0.05151946842670441 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.975, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17380 Training loss 0.051545314490795135 Validation loss 0.05151224508881569 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.943, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17390 Training loss 0.05066536366939545 Validation loss 0.051475174725055695 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.946, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17400 Training loss 0.05207083746790886 Validation loss 0.05148250609636307 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.949, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17410 Training loss 0.052533309906721115 Validation loss 0.05146804079413414 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.939, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17420 Training loss 0.05136919021606445 Validation loss 0.05143733322620392 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.941, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17430 Training loss 0.05384805426001549 Validation loss 0.05158644914627075 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.926, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17440 Training loss 0.05330916494131088 Validation loss 0.05156228691339493 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.949, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17450 Training loss 0.051607608795166016 Validation loss 0.05166923999786377 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.955, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17460 Training loss 0.05245831236243248 Validation loss 0.05149505287408829 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.948, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17470 Training loss 0.051877349615097046 Validation loss 0.05137849599123001 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.944, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17480 Training loss 0.049337320029735565 Validation loss 0.05139218643307686 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.955, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17490 Training loss 0.05200067162513733 Validation loss 0.05135152488946915 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.961, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17500 Training loss 0.04956395551562309 Validation loss 0.051372088491916656 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.954, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17510 Training loss 0.05279512703418732 Validation loss 0.051386456936597824 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.961, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17520 Training loss 0.05025796592235565 Validation loss 0.05169864371418953 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.971, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17530 Training loss 0.04867052286863327 Validation loss 0.051465146243572235 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.962, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17540 Training loss 0.05179821699857712 Validation loss 0.0515851229429245 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.968, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17550 Training loss 0.05087440460920334 Validation loss 0.05156642198562622 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.958, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17560 Training loss 0.05053068697452545 Validation loss 0.05143730342388153 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.967, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17570 Training loss 0.04743865132331848 Validation loss 0.05138193443417549 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.968, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17580 Training loss 0.052258703857660294 Validation loss 0.05134015530347824 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.968, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17590 Training loss 0.05350872874259949 Validation loss 0.05174027383327484 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.957, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17600 Training loss 0.04844353348016739 Validation loss 0.05146133899688721 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.959, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17610 Training loss 0.04843767359852791 Validation loss 0.05153367295861244 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.974, dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17620 Training loss 0.050927698612213135 Validation loss 0.051365405321121216 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.969, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17630 Training loss 0.05262463539838791 Validation loss 0.05154716223478317 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.954, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17640 Training loss 0.05528318136930466 Validation loss 0.05160031095147133 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.912, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17650 Training loss 0.05045860633254051 Validation loss 0.0515434667468071 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.977, dtype=float32), array(0.921, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17660 Training loss 0.04895845055580139 Validation loss 0.051401328295469284 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.961, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17670 Training loss 0.05064481496810913 Validation loss 0.05142904818058014 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.958, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17680 Training loss 0.05286750942468643 Validation loss 0.051519136875867844 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.951, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17690 Training loss 0.05223865807056427 Validation loss 0.05137857422232628 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.953, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17700 Training loss 0.053578149527311325 Validation loss 0.051366087049245834 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.96, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17710 Training loss 0.047321707010269165 Validation loss 0.05167161673307419 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.929, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17720 Training loss 0.05320746824145317 Validation loss 0.05184841901063919 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.896, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17730 Training loss 0.051380086690187454 Validation loss 0.05140361934900284 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.943, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17740 Training loss 0.04862602800130844 Validation loss 0.0513860248029232 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17750 Training loss 0.04671921953558922 Validation loss 0.05143989622592926 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.955, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17760 Training loss 0.04783681407570839 Validation loss 0.0513787679374218 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.959, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17770 Training loss 0.051028259098529816 Validation loss 0.05142604932188988 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.951, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17780 Training loss 0.052674803882837296 Validation loss 0.05138363316655159 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.959, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17790 Training loss 0.04943050071597099 Validation loss 0.0514078326523304 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.971, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17800 Training loss 0.051089465618133545 Validation loss 0.05144122987985611 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.954, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17810 Training loss 0.050206974148750305 Validation loss 0.05153216794133186 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.966, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17820 Training loss 0.05195382237434387 Validation loss 0.0515139065682888 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.963, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17830 Training loss 0.05356655269861221 Validation loss 0.051521558314561844 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.976, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17840 Training loss 0.050297144800424576 Validation loss 0.05152544006705284 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.97, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17850 Training loss 0.04857528582215309 Validation loss 0.051603853702545166 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.936, dtype=float32), array(0.961, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17860 Training loss 0.05323968455195427 Validation loss 0.051728855818510056 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.981, dtype=float32), array(0.921, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17870 Training loss 0.051970224827528 Validation loss 0.05187336727976799 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.98, dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17880 Training loss 0.05108383670449257 Validation loss 0.05155796930193901 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.969, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17890 Training loss 0.04991115257143974 Validation loss 0.05148190259933472 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.973, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17900 Training loss 0.049563780426979065 Validation loss 0.05144372209906578 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.96, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17910 Training loss 0.05046205222606659 Validation loss 0.05147743597626686 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.962, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17920 Training loss 0.05022164061665535 Validation loss 0.05167785659432411 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.929, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17930 Training loss 0.05236371234059334 Validation loss 0.05167364701628685 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.926, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17940 Training loss 0.05155764892697334 Validation loss 0.05148868262767792 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.944, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17950 Training loss 0.04796572029590607 Validation loss 0.05147087946534157 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.952, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17960 Training loss 0.05383387207984924 Validation loss 0.05161858722567558 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.924, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17970 Training loss 0.04926252365112305 Validation loss 0.05171717703342438 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.944, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17980 Training loss 0.05439753830432892 Validation loss 0.051642198115587234 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.93, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 17990 Training loss 0.049622807651758194 Validation loss 0.05161700025200844 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.928, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18000 Training loss 0.049572769552469254 Validation loss 0.05192902311682701 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.927, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18010 Training loss 0.050372350960969925 Validation loss 0.05149330198764801 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.97, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18020 Training loss 0.051790058612823486 Validation loss 0.05139581859111786 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.952, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18030 Training loss 0.051969654858112335 Validation loss 0.051434796303510666 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.955, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18040 Training loss 0.04998082295060158 Validation loss 0.05149206519126892 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.966, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18050 Training loss 0.04902735352516174 Validation loss 0.05148421972990036 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.968, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18060 Training loss 0.055097270756959915 Validation loss 0.05158510431647301 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.965, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18070 Training loss 0.053274210542440414 Validation loss 0.051334332674741745 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.962, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18080 Training loss 0.051939040422439575 Validation loss 0.05137014016509056 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.936, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18090 Training loss 0.05059009790420532 Validation loss 0.05147842690348625 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.943, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18100 Training loss 0.05596964433789253 Validation loss 0.05139686539769173 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.955, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18110 Training loss 0.049906305968761444 Validation loss 0.05140623450279236 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.958, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18120 Training loss 0.05348778888583183 Validation loss 0.05148930847644806 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.974, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18130 Training loss 0.05297121778130531 Validation loss 0.05187325179576874 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.979, dtype=float32), array(0.912, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18140 Training loss 0.05103754252195358 Validation loss 0.05155239626765251 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.979, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18150 Training loss 0.05224597826600075 Validation loss 0.051519788801670074 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.972, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18160 Training loss 0.05041414499282837 Validation loss 0.051571112126111984 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.969, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18170 Training loss 0.051434509456157684 Validation loss 0.05151665210723877 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.974, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18180 Training loss 0.050168365240097046 Validation loss 0.05146270617842674 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.959, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18190 Training loss 0.050885263830423355 Validation loss 0.051495715975761414 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.94, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18200 Training loss 0.05272183194756508 Validation loss 0.05140417441725731 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.965, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18210 Training loss 0.051971811801195145 Validation loss 0.051461029797792435 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.965, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18220 Training loss 0.04758917912840843 Validation loss 0.05140307545661926 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.959, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18230 Training loss 0.05378061905503273 Validation loss 0.051511459052562714 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.972, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18240 Training loss 0.05149425193667412 Validation loss 0.051556073129177094 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.957, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18250 Training loss 0.05058213695883751 Validation loss 0.05143144354224205 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.97, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18260 Training loss 0.05058540403842926 Validation loss 0.051802948117256165 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.978, dtype=float32), array(0.904, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18270 Training loss 0.05387406051158905 Validation loss 0.0514126755297184 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.965, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18280 Training loss 0.050735488533973694 Validation loss 0.05141656845808029 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.957, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18290 Training loss 0.05235077068209648 Validation loss 0.05144844576716423 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.965, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18300 Training loss 0.049371473491191864 Validation loss 0.05142069235444069 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.956, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18310 Training loss 0.04932894557714462 Validation loss 0.051540203392505646 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.969, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18320 Training loss 0.04838956519961357 Validation loss 0.05173115059733391 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.947, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18330 Training loss 0.05075953155755997 Validation loss 0.051893461495637894 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.944, dtype=float32), array(0.896, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18340 Training loss 0.0485571026802063 Validation loss 0.05159784108400345 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18350 Training loss 0.052647922188043594 Validation loss 0.051703691482543945 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.938, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18360 Training loss 0.05140683054924011 Validation loss 0.051485396921634674 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.936, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18370 Training loss 0.054349444806575775 Validation loss 0.05140969902276993 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.977, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18380 Training loss 0.05093647167086601 Validation loss 0.05147501826286316 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.957, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18390 Training loss 0.050863116979599 Validation loss 0.051342226564884186 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18400 Training loss 0.050224028527736664 Validation loss 0.05135195329785347 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.961, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18410 Training loss 0.05144363269209862 Validation loss 0.05140324681997299 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.971, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18420 Training loss 0.04598906636238098 Validation loss 0.051565468311309814 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.936, dtype=float32), array(0.942, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18430 Training loss 0.050867024809122086 Validation loss 0.05162103474140167 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.913, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18440 Training loss 0.050481222569942474 Validation loss 0.05141352489590645 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.949, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18450 Training loss 0.05077008903026581 Validation loss 0.051559384912252426 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.977, dtype=float32), array(0.919, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18460 Training loss 0.05041739344596863 Validation loss 0.051480889320373535 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.97, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18470 Training loss 0.05146991088986397 Validation loss 0.05142277851700783 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.962, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18480 Training loss 0.049275655299425125 Validation loss 0.051404327154159546 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.966, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18490 Training loss 0.0482490174472332 Validation loss 0.05139726400375366 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.94, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18500 Training loss 0.05127551406621933 Validation loss 0.051504962146282196 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.952, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18510 Training loss 0.05188203603029251 Validation loss 0.051532577723264694 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.973, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18520 Training loss 0.051635194569826126 Validation loss 0.05138911306858063 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.951, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18530 Training loss 0.049601808190345764 Validation loss 0.05149379000067711 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.944, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18540 Training loss 0.04814236983656883 Validation loss 0.0514393076300621 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.942, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18550 Training loss 0.0512094721198082 Validation loss 0.05141222104430199 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.944, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18560 Training loss 0.049712080508470535 Validation loss 0.05153346806764603 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.929, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18570 Training loss 0.049854401499032974 Validation loss 0.05142250657081604 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.95, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18580 Training loss 0.05131953954696655 Validation loss 0.05161404237151146 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.977, dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18590 Training loss 0.05237400904297829 Validation loss 0.052110396325588226 Accuracy 0.4772999882698059 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.978, dtype=float32), array(0.859, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18600 Training loss 0.05143318325281143 Validation loss 0.05144540220499039 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.949, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18610 Training loss 0.04906395077705383 Validation loss 0.05162522941827774 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.941, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18620 Training loss 0.051313430070877075 Validation loss 0.05201917141675949 Accuracy 0.4781999886035919 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.979, dtype=float32), array(0.907, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18630 Training loss 0.050011005252599716 Validation loss 0.05162372067570686 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.968, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18640 Training loss 0.05208239331841469 Validation loss 0.051688216626644135 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.975, dtype=float32), array(0.921, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18650 Training loss 0.05226413533091545 Validation loss 0.05165901780128479 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.976, dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18660 Training loss 0.04708057641983032 Validation loss 0.0513576865196228 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.962, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18670 Training loss 0.049315694719552994 Validation loss 0.051442574709653854 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.961, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18680 Training loss 0.04884546250104904 Validation loss 0.05141158774495125 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.955, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18690 Training loss 0.05419611185789108 Validation loss 0.05174501985311508 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.959, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18700 Training loss 0.04935088008642197 Validation loss 0.051362983882427216 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.963, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18710 Training loss 0.051160238683223724 Validation loss 0.05138072371482849 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.961, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18720 Training loss 0.04972519353032112 Validation loss 0.051471490412950516 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.947, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18730 Training loss 0.055864930152893066 Validation loss 0.05139202997088432 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.957, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18740 Training loss 0.05157726630568504 Validation loss 0.051304105669260025 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.97, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18750 Training loss 0.050223927944898605 Validation loss 0.051345352083444595 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.957, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18760 Training loss 0.05336909368634224 Validation loss 0.05147257819771767 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.965, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18770 Training loss 0.05369199439883232 Validation loss 0.05144377052783966 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.965, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18780 Training loss 0.05182141438126564 Validation loss 0.05160731449723244 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.919, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18790 Training loss 0.05255693942308426 Validation loss 0.05192742124199867 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.941, dtype=float32), array(0.899, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18800 Training loss 0.05103731155395508 Validation loss 0.05163728818297386 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.932, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18810 Training loss 0.05170726031064987 Validation loss 0.05146173760294914 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.948, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18820 Training loss 0.05066009983420372 Validation loss 0.051393602043390274 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.973, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18830 Training loss 0.04915322735905647 Validation loss 0.05164515972137451 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.951, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18840 Training loss 0.052500754594802856 Validation loss 0.05150805786252022 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.962, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18850 Training loss 0.05309467017650604 Validation loss 0.05158049985766411 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.978, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18860 Training loss 0.05091419070959091 Validation loss 0.05140836536884308 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.971, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18870 Training loss 0.054381877183914185 Validation loss 0.05132986232638359 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.968, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18880 Training loss 0.05050275847315788 Validation loss 0.05160719156265259 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.977, dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18890 Training loss 0.05285094678401947 Validation loss 0.05158902332186699 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.97, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18900 Training loss 0.05637031048536301 Validation loss 0.05159229412674904 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.948, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18910 Training loss 0.051105573773384094 Validation loss 0.051397547125816345 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.949, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18920 Training loss 0.050599172711372375 Validation loss 0.05189158022403717 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.942, dtype=float32), array(0.918, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18930 Training loss 0.051985520869493484 Validation loss 0.051402054727077484 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.963, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18940 Training loss 0.051478415727615356 Validation loss 0.0514230914413929 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.973, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18950 Training loss 0.05146685615181923 Validation loss 0.05144219845533371 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.931, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18960 Training loss 0.0510433129966259 Validation loss 0.05172520875930786 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.899, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18970 Training loss 0.050791382789611816 Validation loss 0.05131791904568672 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.958, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18980 Training loss 0.052397388964891434 Validation loss 0.05136009678244591 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.959, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 18990 Training loss 0.04986768588423729 Validation loss 0.051462508738040924 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.953, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19000 Training loss 0.050233691930770874 Validation loss 0.0514674112200737 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.963, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19010 Training loss 0.05086105316877365 Validation loss 0.05142716318368912 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.946, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19020 Training loss 0.04612649604678154 Validation loss 0.05148344859480858 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.974, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19030 Training loss 0.052556391805410385 Validation loss 0.0514298714697361 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.943, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19040 Training loss 0.053158923983573914 Validation loss 0.05140950530767441 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.963, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19050 Training loss 0.05305970087647438 Validation loss 0.0518246591091156 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.98, dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19060 Training loss 0.050889577716588974 Validation loss 0.051442284137010574 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.965, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19070 Training loss 0.04677000269293785 Validation loss 0.05147741362452507 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.963, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19080 Training loss 0.0470704659819603 Validation loss 0.05159791186451912 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.977, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19090 Training loss 0.04838944599032402 Validation loss 0.05154929310083389 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.97, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19100 Training loss 0.04927891492843628 Validation loss 0.051635321229696274 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.978, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19110 Training loss 0.05310416594147682 Validation loss 0.05150870978832245 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.952, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19120 Training loss 0.049587149173021317 Validation loss 0.051520079374313354 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.953, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19130 Training loss 0.04776279628276825 Validation loss 0.051464952528476715 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.961, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19140 Training loss 0.05269957333803177 Validation loss 0.05154566839337349 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.953, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19150 Training loss 0.04919713735580444 Validation loss 0.05148826166987419 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.964, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19160 Training loss 0.04737764596939087 Validation loss 0.05152362957596779 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.948, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19170 Training loss 0.05363510549068451 Validation loss 0.05140214040875435 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.97, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19180 Training loss 0.05161235108971596 Validation loss 0.051487792283296585 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.964, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19190 Training loss 0.04968656226992607 Validation loss 0.05156301334500313 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.959, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19200 Training loss 0.052599020302295685 Validation loss 0.05158935487270355 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.956, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19210 Training loss 0.05071873590350151 Validation loss 0.051615722477436066 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.951, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19220 Training loss 0.05177455022931099 Validation loss 0.05148538574576378 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.946, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19230 Training loss 0.05144112557172775 Validation loss 0.051685430109500885 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.968, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19240 Training loss 0.05127457529306412 Validation loss 0.05180440843105316 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.98, dtype=float32), array(0.918, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19250 Training loss 0.04838428646326065 Validation loss 0.05147935450077057 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.972, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19260 Training loss 0.05226169899106026 Validation loss 0.05147319287061691 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.973, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19270 Training loss 0.05154186859726906 Validation loss 0.05151607096195221 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.977, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19280 Training loss 0.05084923282265663 Validation loss 0.05132255703210831 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.947, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19290 Training loss 0.05276802182197571 Validation loss 0.05228356271982193 Accuracy 0.4747999906539917 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.891, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19300 Training loss 0.05194028839468956 Validation loss 0.05181005224585533 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.913, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19310 Training loss 0.050700489431619644 Validation loss 0.051521219313144684 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.949, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19320 Training loss 0.05299050733447075 Validation loss 0.05187166482210159 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.904, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19330 Training loss 0.05502570420503616 Validation loss 0.05185188353061676 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.921, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19340 Training loss 0.05378728359937668 Validation loss 0.05133345350623131 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.96, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19350 Training loss 0.04676977917551994 Validation loss 0.05143778398633003 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.947, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19360 Training loss 0.04857220873236656 Validation loss 0.05142046511173248 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.94, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19370 Training loss 0.05251773074269295 Validation loss 0.05142778158187866 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.953, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19380 Training loss 0.0533805713057518 Validation loss 0.0514674112200737 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.938, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19390 Training loss 0.05304533988237381 Validation loss 0.05177038908004761 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.932, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19400 Training loss 0.051070429384708405 Validation loss 0.0516439788043499 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.909, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19410 Training loss 0.05027497187256813 Validation loss 0.051410965621471405 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.963, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19420 Training loss 0.051283713430166245 Validation loss 0.05199703574180603 Accuracy 0.47769999504089355 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.963, dtype=float32), array(0.871, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19430 Training loss 0.04885629191994667 Validation loss 0.05143815279006958 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.931, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19440 Training loss 0.05025188624858856 Validation loss 0.05164186656475067 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.921, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19450 Training loss 0.0523705892264843 Validation loss 0.051689911633729935 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.909, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19460 Training loss 0.04949093610048294 Validation loss 0.05149238184094429 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.945, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19470 Training loss 0.052403248846530914 Validation loss 0.05137034133076668 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.964, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19480 Training loss 0.05283479392528534 Validation loss 0.051399338990449905 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.949, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19490 Training loss 0.05215369537472725 Validation loss 0.05162573233246803 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.964, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19500 Training loss 0.0520765520632267 Validation loss 0.05155638977885246 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.962, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19510 Training loss 0.05037546157836914 Validation loss 0.05143686383962631 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.967, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19520 Training loss 0.049759939312934875 Validation loss 0.05139938369393349 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.948, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19530 Training loss 0.05139754340052605 Validation loss 0.0514521598815918 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.964, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19540 Training loss 0.053363826125860214 Validation loss 0.05139832943677902 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.963, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19550 Training loss 0.051709819585084915 Validation loss 0.051429785788059235 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.967, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19560 Training loss 0.047333747148513794 Validation loss 0.051442962139844894 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.967, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19570 Training loss 0.05254252254962921 Validation loss 0.05145048722624779 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.959, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19580 Training loss 0.05543947219848633 Validation loss 0.05150677636265755 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.971, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19590 Training loss 0.0502212829887867 Validation loss 0.05146915465593338 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.971, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19600 Training loss 0.055501602590084076 Validation loss 0.051562193781137466 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.948, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19610 Training loss 0.057820819318294525 Validation loss 0.05159411579370499 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.964, dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19620 Training loss 0.05200673267245293 Validation loss 0.05143170431256294 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.943, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19630 Training loss 0.05105387419462204 Validation loss 0.05138968676328659 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.962, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19640 Training loss 0.04913986101746559 Validation loss 0.05134953185915947 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.949, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19650 Training loss 0.05038710683584213 Validation loss 0.051344823092222214 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.964, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19660 Training loss 0.0540204755961895 Validation loss 0.05158759281039238 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.932, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19670 Training loss 0.05092243477702141 Validation loss 0.051355086266994476 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.964, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19680 Training loss 0.05511843413114548 Validation loss 0.051333505660295486 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.969, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19690 Training loss 0.04976225644350052 Validation loss 0.05194060504436493 Accuracy 0.4788999855518341 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.982, dtype=float32), array(0.88, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19700 Training loss 0.05120174214243889 Validation loss 0.05132640153169632 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.961, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19710 Training loss 0.04944533854722977 Validation loss 0.051390260457992554 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.953, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19720 Training loss 0.05421951413154602 Validation loss 0.051409441977739334 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.968, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19730 Training loss 0.05092566832900047 Validation loss 0.051352374255657196 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.966, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19740 Training loss 0.05472879484295845 Validation loss 0.05145018547773361 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.974, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19750 Training loss 0.05182339623570442 Validation loss 0.05131052806973457 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.97, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19760 Training loss 0.050274770706892014 Validation loss 0.05136130005121231 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.971, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19770 Training loss 0.05006749927997589 Validation loss 0.05133865401148796 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.967, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19780 Training loss 0.05236401781439781 Validation loss 0.05163627862930298 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.977, dtype=float32), array(0.922, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19790 Training loss 0.051450032740831375 Validation loss 0.051441505551338196 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.962, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19800 Training loss 0.05007316544651985 Validation loss 0.051545750349760056 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.939, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19810 Training loss 0.05091731250286102 Validation loss 0.05157932639122009 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.932, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19820 Training loss 0.05047882720828056 Validation loss 0.051543228328228 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.925, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19830 Training loss 0.05071883648633957 Validation loss 0.05142897367477417 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.952, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19840 Training loss 0.04959629848599434 Validation loss 0.05137014016509056 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.956, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19850 Training loss 0.053646620362997055 Validation loss 0.051413457840681076 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.946, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19860 Training loss 0.05287854000926018 Validation loss 0.05131039023399353 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.968, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19870 Training loss 0.053315114229917526 Validation loss 0.051264386624097824 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.968, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19880 Training loss 0.04997587576508522 Validation loss 0.051506511867046356 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.941, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19890 Training loss 0.05038659647107124 Validation loss 0.05135868117213249 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.943, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19900 Training loss 0.05459388345479965 Validation loss 0.051409825682640076 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.942, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19910 Training loss 0.04895675554871559 Validation loss 0.05183660238981247 Accuracy 0.47940000891685486 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.93, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19920 Training loss 0.04992726817727089 Validation loss 0.05169019475579262 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.965, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19930 Training loss 0.05236273258924484 Validation loss 0.05156397074460983 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.925, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19940 Training loss 0.052278533577919006 Validation loss 0.05158008262515068 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.933, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19950 Training loss 0.052677784115076065 Validation loss 0.051604390144348145 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.949, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19960 Training loss 0.05255146697163582 Validation loss 0.051667626947164536 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.966, dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19970 Training loss 0.05224618688225746 Validation loss 0.05184086039662361 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.968, dtype=float32), array(0.924, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19980 Training loss 0.050297852605581284 Validation loss 0.051362548023462296 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.967, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 19990 Training loss 0.04990769922733307 Validation loss 0.051339637488126755 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.97, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20000 Training loss 0.04948887974023819 Validation loss 0.05147210881114006 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.97, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20010 Training loss 0.051383402198553085 Validation loss 0.051486268639564514 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.969, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20020 Training loss 0.053855545818805695 Validation loss 0.05136966332793236 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.967, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20030 Training loss 0.052235689014196396 Validation loss 0.05144699290394783 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.974, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20040 Training loss 0.050690531730651855 Validation loss 0.05143621191382408 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.967, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20050 Training loss 0.05111198499798775 Validation loss 0.052160587161779404 Accuracy 0.476500004529953 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.982, dtype=float32), array(0.843, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20060 Training loss 0.0535581149160862 Validation loss 0.05173244699835777 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.971, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20070 Training loss 0.0493772029876709 Validation loss 0.051590923219919205 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.971, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20080 Training loss 0.05136112868785858 Validation loss 0.05149204656481743 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.967, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20090 Training loss 0.049548666924238205 Validation loss 0.0513809509575367 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.952, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20100 Training loss 0.051411185413599014 Validation loss 0.05143626779317856 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.976, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20110 Training loss 0.04942740499973297 Validation loss 0.0514773428440094 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.952, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20120 Training loss 0.054017405956983566 Validation loss 0.05149754881858826 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.967, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20130 Training loss 0.05089642480015755 Validation loss 0.0514318123459816 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.955, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20140 Training loss 0.05052066221833229 Validation loss 0.051409196108579636 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.953, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20150 Training loss 0.050570011138916016 Validation loss 0.051675811409950256 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.928, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20160 Training loss 0.047788411378860474 Validation loss 0.051363199949264526 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.947, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20170 Training loss 0.05308625102043152 Validation loss 0.05141003429889679 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.961, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20180 Training loss 0.04964737966656685 Validation loss 0.051434315741062164 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.947, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20190 Training loss 0.05720483511686325 Validation loss 0.051417943090200424 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.948, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20200 Training loss 0.05434154346585274 Validation loss 0.0513400100171566 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.961, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20210 Training loss 0.053665678948163986 Validation loss 0.051515161991119385 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.965, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20220 Training loss 0.05209561809897423 Validation loss 0.05157315358519554 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.977, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20230 Training loss 0.05315010994672775 Validation loss 0.05148865655064583 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.972, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20240 Training loss 0.04820489138364792 Validation loss 0.05132264643907547 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.969, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20250 Training loss 0.05299416556954384 Validation loss 0.051486436277627945 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.975, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20260 Training loss 0.04845611751079559 Validation loss 0.05164849013090134 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.977, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20270 Training loss 0.05226213485002518 Validation loss 0.05154154449701309 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.937, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20280 Training loss 0.05082791671156883 Validation loss 0.051405761390924454 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.966, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20290 Training loss 0.050323545932769775 Validation loss 0.051322925835847855 Accuracy 0.4851999878883362 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.968, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20300 Training loss 0.052024900913238525 Validation loss 0.05137725546956062 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.949, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20310 Training loss 0.04980141296982765 Validation loss 0.05151394382119179 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.953, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20320 Training loss 0.05196231231093407 Validation loss 0.05145173519849777 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.953, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20330 Training loss 0.0536225326359272 Validation loss 0.05167650431394577 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.974, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20340 Training loss 0.052702922374010086 Validation loss 0.05155410245060921 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.976, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20350 Training loss 0.04899080470204353 Validation loss 0.05150265246629715 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.974, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20360 Training loss 0.05212770029902458 Validation loss 0.05151506885886192 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.975, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20370 Training loss 0.049637142568826675 Validation loss 0.05146396532654762 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.972, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20380 Training loss 0.04919293150305748 Validation loss 0.05143800377845764 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.95, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20390 Training loss 0.05121268332004547 Validation loss 0.05146762356162071 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.964, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20400 Training loss 0.04964163154363632 Validation loss 0.05135137587785721 Accuracy 0.48539999127388 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.95, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20410 Training loss 0.05224703997373581 Validation loss 0.05147622153162956 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.946, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20420 Training loss 0.05170324072241783 Validation loss 0.05134113132953644 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.959, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20430 Training loss 0.048493001610040665 Validation loss 0.05207313597202301 Accuracy 0.4763999879360199 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.9, dtype=float32), array(0.917, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20440 Training loss 0.050420887768268585 Validation loss 0.05144059658050537 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.928, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20450 Training loss 0.04974435269832611 Validation loss 0.05141337588429451 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.942, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20460 Training loss 0.05053451657295227 Validation loss 0.05147067457437515 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.921, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20470 Training loss 0.053628869354724884 Validation loss 0.05127288028597832 Accuracy 0.48570001125335693 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.964, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20480 Training loss 0.05077735334634781 Validation loss 0.05138862878084183 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.962, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20490 Training loss 0.055608104914426804 Validation loss 0.05149093270301819 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.977, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20500 Training loss 0.056778691709041595 Validation loss 0.05145380273461342 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.972, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20510 Training loss 0.05120401084423065 Validation loss 0.05136493593454361 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.962, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20520 Training loss 0.05254727229475975 Validation loss 0.05146599933505058 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.939, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20530 Training loss 0.04903434216976166 Validation loss 0.051481690257787704 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.94, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20540 Training loss 0.05212653428316116 Validation loss 0.051439158618450165 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.953, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20550 Training loss 0.05107603594660759 Validation loss 0.051624052226543427 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.949, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20560 Training loss 0.05314909294247627 Validation loss 0.05189496651291847 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.945, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20570 Training loss 0.05305177718400955 Validation loss 0.052348554134368896 Accuracy 0.4756999909877777 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.868, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20580 Training loss 0.053821370005607605 Validation loss 0.0517951138317585 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.913, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20590 Training loss 0.05133168399333954 Validation loss 0.0513109490275383 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.949, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20600 Training loss 0.05047445744276047 Validation loss 0.052065160125494 Accuracy 0.4763999879360199 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.851, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20610 Training loss 0.051510538905858994 Validation loss 0.05141004920005798 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.941, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20620 Training loss 0.05491390824317932 Validation loss 0.05139975622296333 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.94, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20630 Training loss 0.04925179108977318 Validation loss 0.051499996334314346 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.934, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20640 Training loss 0.048244547098875046 Validation loss 0.05143370106816292 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.954, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20650 Training loss 0.052967753261327744 Validation loss 0.05153999105095863 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.968, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20660 Training loss 0.05187654495239258 Validation loss 0.05198035016655922 Accuracy 0.47870001196861267 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.976, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.906, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20670 Training loss 0.052358515560626984 Validation loss 0.05167641490697861 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.966, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20680 Training loss 0.05330554023385048 Validation loss 0.051415156573057175 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.951, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20690 Training loss 0.050886549055576324 Validation loss 0.051487524062395096 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.93, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20700 Training loss 0.04791107028722763 Validation loss 0.051440585404634476 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.946, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20710 Training loss 0.05342628061771393 Validation loss 0.05240403488278389 Accuracy 0.4742000102996826 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.955, dtype=float32), array(0.854, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20720 Training loss 0.04833105951547623 Validation loss 0.05137854814529419 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.967, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20730 Training loss 0.04953585937619209 Validation loss 0.051374610513448715 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.966, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20740 Training loss 0.05018877238035202 Validation loss 0.05143127962946892 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.96, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20750 Training loss 0.05295854061841965 Validation loss 0.05165807157754898 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.961, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20760 Training loss 0.05241929739713669 Validation loss 0.05187944695353508 Accuracy 0.4790000021457672 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.975, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20770 Training loss 0.053969960659742355 Validation loss 0.05169787257909775 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.966, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20780 Training loss 0.05540354549884796 Validation loss 0.051588866859674454 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.966, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20790 Training loss 0.049548130482435226 Validation loss 0.051372379064559937 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.953, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20800 Training loss 0.05198761075735092 Validation loss 0.05153922736644745 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.95, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20810 Training loss 0.04923004284501076 Validation loss 0.05155237391591072 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.932, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20820 Training loss 0.05256359279155731 Validation loss 0.0514150969684124 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.959, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20830 Training loss 0.05247367173433304 Validation loss 0.05154822766780853 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.935, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20840 Training loss 0.05227047577500343 Validation loss 0.05153613165020943 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.922, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20850 Training loss 0.05135013908147812 Validation loss 0.05139263719320297 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.943, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20860 Training loss 0.052093036472797394 Validation loss 0.051444537937641144 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.948, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20870 Training loss 0.05257497727870941 Validation loss 0.05171584337949753 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.914, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20880 Training loss 0.05123942717909813 Validation loss 0.051557112485170364 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.942, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20890 Training loss 0.04878617823123932 Validation loss 0.051477596163749695 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.965, dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20900 Training loss 0.04937796667218208 Validation loss 0.05151096358895302 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.958, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20910 Training loss 0.04893313720822334 Validation loss 0.0514175184071064 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.963, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20920 Training loss 0.04967320337891579 Validation loss 0.05145829916000366 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.972, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20930 Training loss 0.051558252424001694 Validation loss 0.05165693536400795 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.96, dtype=float32), array(0.899, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20940 Training loss 0.04940541461110115 Validation loss 0.051445186138153076 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.967, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20950 Training loss 0.054682694375514984 Validation loss 0.05150919780135155 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.977, dtype=float32), array(0.919, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20960 Training loss 0.048187367618083954 Validation loss 0.051406778395175934 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.955, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20970 Training loss 0.04624681547284126 Validation loss 0.05152079090476036 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.946, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20980 Training loss 0.05096254125237465 Validation loss 0.05189960077404976 Accuracy 0.478300005197525 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.946, dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 20990 Training loss 0.05098465457558632 Validation loss 0.051544804126024246 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.954, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21000 Training loss 0.048631250858306885 Validation loss 0.05163627117872238 Accuracy 0.48089998960494995 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.957, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21010 Training loss 0.054173801094293594 Validation loss 0.05174635723233223 Accuracy 0.4794999957084656 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.966, dtype=float32), array(0.892, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21020 Training loss 0.048159707337617874 Validation loss 0.0514167957007885 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.958, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21030 Training loss 0.050489917397499084 Validation loss 0.051602162420749664 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.937, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21040 Training loss 0.053832605481147766 Validation loss 0.051631636917591095 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.939, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21050 Training loss 0.05236128345131874 Validation loss 0.05141996592283249 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.961, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21060 Training loss 0.05373009294271469 Validation loss 0.05166783556342125 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.974, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21070 Training loss 0.0530756413936615 Validation loss 0.051687464118003845 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.954, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21080 Training loss 0.051032766699790955 Validation loss 0.051564786583185196 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.966, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21090 Training loss 0.053106505423784256 Validation loss 0.051557544618844986 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.965, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21100 Training loss 0.05069959536194801 Validation loss 0.05145092308521271 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.972, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21110 Training loss 0.05085901916027069 Validation loss 0.051374658942222595 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.95, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21120 Training loss 0.05197101831436157 Validation loss 0.05167558044195175 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.925, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21130 Training loss 0.051299113780260086 Validation loss 0.05167680233716965 Accuracy 0.48069998621940613 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.958, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21140 Training loss 0.054699983447790146 Validation loss 0.05155747011303902 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.952, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21150 Training loss 0.047823332250118256 Validation loss 0.05143948644399643 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.962, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21160 Training loss 0.05385353043675423 Validation loss 0.05147897079586983 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.956, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21170 Training loss 0.05024849623441696 Validation loss 0.051393523812294006 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.948, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21180 Training loss 0.04842139035463333 Validation loss 0.05150894448161125 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.95, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21190 Training loss 0.05232692509889603 Validation loss 0.051858704537153244 Accuracy 0.47909998893737793 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.972, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21200 Training loss 0.04842134565114975 Validation loss 0.05154316499829292 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.973, dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21210 Training loss 0.05014582350850105 Validation loss 0.05157632753252983 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.975, dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21220 Training loss 0.049769140779972076 Validation loss 0.052033450454473495 Accuracy 0.4772000014781952 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.958, dtype=float32), array(0.878, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21230 Training loss 0.04805518686771393 Validation loss 0.05135176703333855 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.97, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21240 Training loss 0.05087094008922577 Validation loss 0.05131753534078598 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.963, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21250 Training loss 0.0528671070933342 Validation loss 0.05140814557671547 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.975, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21260 Training loss 0.052064199000597 Validation loss 0.05151183530688286 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.973, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21270 Training loss 0.053687892854213715 Validation loss 0.051380958408117294 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.964, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21280 Training loss 0.05268358439207077 Validation loss 0.051333360373973846 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21290 Training loss 0.0537315309047699 Validation loss 0.051512714475393295 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.972, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21300 Training loss 0.05199520289897919 Validation loss 0.051278311759233475 Accuracy 0.48510000109672546 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.968, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21310 Training loss 0.05168037489056587 Validation loss 0.05142541602253914 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.957, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21320 Training loss 0.05001967400312424 Validation loss 0.0514938086271286 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.949, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21330 Training loss 0.050377096980810165 Validation loss 0.05153413489460945 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.938, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21340 Training loss 0.05151809751987457 Validation loss 0.051415957510471344 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.946, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21350 Training loss 0.05045798793435097 Validation loss 0.05148408189415932 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.95, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21360 Training loss 0.0520731620490551 Validation loss 0.05140130966901779 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.965, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21370 Training loss 0.05408461019396782 Validation loss 0.05150037258863449 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.947, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21380 Training loss 0.04855038970708847 Validation loss 0.05155795440077782 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.942, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21390 Training loss 0.05304702743887901 Validation loss 0.05143531411886215 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.964, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21400 Training loss 0.051202353090047836 Validation loss 0.05156322196125984 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.914, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21410 Training loss 0.05378877744078636 Validation loss 0.05158348008990288 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.923, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21420 Training loss 0.05045592412352562 Validation loss 0.05150873214006424 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.948, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21430 Training loss 0.04815647378563881 Validation loss 0.051350366324186325 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.956, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21440 Training loss 0.050614167004823685 Validation loss 0.05151355639100075 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.937, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21450 Training loss 0.04841189458966255 Validation loss 0.051462311297655106 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.944, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21460 Training loss 0.04894036427140236 Validation loss 0.05147496238350868 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.951, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21470 Training loss 0.049737099558115005 Validation loss 0.05137773975729942 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.942, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21480 Training loss 0.05226121097803116 Validation loss 0.051423680037260056 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.957, dtype=float32), array(0.93, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21490 Training loss 0.049787748605012894 Validation loss 0.05134735256433487 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.944, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21500 Training loss 0.05263237655162811 Validation loss 0.05149807780981064 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.929, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21510 Training loss 0.047325968742370605 Validation loss 0.051592420786619186 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.923, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21520 Training loss 0.05295586213469505 Validation loss 0.051498789340257645 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.96, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21530 Training loss 0.04682176560163498 Validation loss 0.05166884884238243 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.936, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21540 Training loss 0.04917167127132416 Validation loss 0.051542729139328 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.928, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21550 Training loss 0.050842784345149994 Validation loss 0.05134650319814682 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21560 Training loss 0.0494241826236248 Validation loss 0.05144096165895462 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.963, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21570 Training loss 0.05389362573623657 Validation loss 0.051381915807724 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.969, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21580 Training loss 0.05337332561612129 Validation loss 0.05134446173906326 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.952, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21590 Training loss 0.049829620867967606 Validation loss 0.051471445709466934 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.952, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21600 Training loss 0.04960985854268074 Validation loss 0.0515596866607666 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.925, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21610 Training loss 0.05632120743393898 Validation loss 0.05180329456925392 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.908, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21620 Training loss 0.0519971065223217 Validation loss 0.05185366049408913 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.915, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21630 Training loss 0.050985172390937805 Validation loss 0.05181543901562691 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.908, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21640 Training loss 0.052374135702848434 Validation loss 0.05169272422790527 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.904, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21650 Training loss 0.052722372114658356 Validation loss 0.05214058607816696 Accuracy 0.47519999742507935 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.976, dtype=float32), array(0.831, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21660 Training loss 0.05383455380797386 Validation loss 0.051345545798540115 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.958, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21670 Training loss 0.05184542387723923 Validation loss 0.05127991363406181 Accuracy 0.4851999878883362 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.958, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21680 Training loss 0.05092108994722366 Validation loss 0.05130336806178093 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.954, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21690 Training loss 0.050921618938446045 Validation loss 0.05174876004457474 Accuracy 0.48019999265670776 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.949, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21700 Training loss 0.05055944249033928 Validation loss 0.05190116539597511 Accuracy 0.4787999987602234 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.951, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21710 Training loss 0.051225729286670685 Validation loss 0.05141392722725868 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.946, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21720 Training loss 0.05142653360962868 Validation loss 0.051381874829530716 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.962, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21730 Training loss 0.050240207463502884 Validation loss 0.051392246037721634 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.967, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21740 Training loss 0.047508835792541504 Validation loss 0.051459237933158875 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.971, dtype=float32), array(0.937, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21750 Training loss 0.04805752635002136 Validation loss 0.05162964016199112 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.977, dtype=float32), array(0.901, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21760 Training loss 0.05162518471479416 Validation loss 0.051338113844394684 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.966, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21770 Training loss 0.04893112555146217 Validation loss 0.05137968808412552 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.958, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21780 Training loss 0.052240051329135895 Validation loss 0.05152732506394386 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.979, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21790 Training loss 0.049952391535043716 Validation loss 0.051750123500823975 Accuracy 0.4796999990940094 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.978, dtype=float32), array(0.895, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21800 Training loss 0.05230046436190605 Validation loss 0.051306817680597305 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.971, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21810 Training loss 0.04992934316396713 Validation loss 0.051354970782995224 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.963, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21820 Training loss 0.051930494606494904 Validation loss 0.05139503255486488 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.943, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21830 Training loss 0.0502532534301281 Validation loss 0.051386844366788864 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.964, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21840 Training loss 0.049743760377168655 Validation loss 0.051340289413928986 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.97, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21850 Training loss 0.0490652360022068 Validation loss 0.05139468237757683 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.955, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21860 Training loss 0.0529182069003582 Validation loss 0.051380936056375504 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.95, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21870 Training loss 0.04971970617771149 Validation loss 0.05134918913245201 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.944, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21880 Training loss 0.05062675103545189 Validation loss 0.05140552669763565 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.943, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21890 Training loss 0.05046436935663223 Validation loss 0.05135969817638397 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.955, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21900 Training loss 0.05391028895974159 Validation loss 0.05142796039581299 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.957, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21910 Training loss 0.0506913959980011 Validation loss 0.05140022933483124 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.96, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21920 Training loss 0.05175541341304779 Validation loss 0.051509395241737366 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.963, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21930 Training loss 0.04858017340302467 Validation loss 0.05142057687044144 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.96, dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21940 Training loss 0.04653574526309967 Validation loss 0.051273416727781296 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.969, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21950 Training loss 0.05047935992479324 Validation loss 0.05147348716855049 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.957, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21960 Training loss 0.05009598284959793 Validation loss 0.051477156579494476 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.951, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21970 Training loss 0.05089140683412552 Validation loss 0.051526639610528946 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.945, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21980 Training loss 0.05202379450201988 Validation loss 0.05146324262022972 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.958, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 21990 Training loss 0.050495926290750504 Validation loss 0.05132044479250908 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.965, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22000 Training loss 0.05023307353258133 Validation loss 0.05142218619585037 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.951, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22010 Training loss 0.04883366450667381 Validation loss 0.0514579601585865 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.961, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22020 Training loss 0.04812930151820183 Validation loss 0.05140186846256256 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.954, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22030 Training loss 0.052002135664224625 Validation loss 0.05156815052032471 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.972, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22040 Training loss 0.05017758533358574 Validation loss 0.051344871520996094 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.966, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22050 Training loss 0.05049048736691475 Validation loss 0.05144399404525757 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.964, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22060 Training loss 0.05120273679494858 Validation loss 0.05141746625304222 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.964, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22070 Training loss 0.04917053133249283 Validation loss 0.05153931304812431 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.979, dtype=float32), array(0.924, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22080 Training loss 0.05185671150684357 Validation loss 0.05143214017152786 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.966, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22090 Training loss 0.05049673467874527 Validation loss 0.0514010451734066 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.97, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22100 Training loss 0.05437905713915825 Validation loss 0.05149389058351517 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.951, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22110 Training loss 0.04863276332616806 Validation loss 0.05141337960958481 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.954, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22120 Training loss 0.04948170855641365 Validation loss 0.05150372534990311 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.936, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22130 Training loss 0.05071800574660301 Validation loss 0.051486168056726456 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.941, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22140 Training loss 0.05298938602209091 Validation loss 0.05140912905335426 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.948, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22150 Training loss 0.04782744497060776 Validation loss 0.05160357058048248 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.945, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22160 Training loss 0.048383962363004684 Validation loss 0.05135134607553482 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.97, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22170 Training loss 0.050242651253938675 Validation loss 0.0515761636197567 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.98, dtype=float32), array(0.91, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22180 Training loss 0.0534575954079628 Validation loss 0.051480382680892944 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.976, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22190 Training loss 0.05233156308531761 Validation loss 0.051372937858104706 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.967, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22200 Training loss 0.055185794830322266 Validation loss 0.05144502595067024 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.96, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22210 Training loss 0.05344970524311066 Validation loss 0.05143491178750992 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.946, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22220 Training loss 0.04939420521259308 Validation loss 0.05146465450525284 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.934, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22230 Training loss 0.05103679746389389 Validation loss 0.05169903486967087 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.915, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22240 Training loss 0.053674887865781784 Validation loss 0.05144737288355827 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.938, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22250 Training loss 0.050944581627845764 Validation loss 0.05137548968195915 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.94, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22260 Training loss 0.05307582765817642 Validation loss 0.0513317734003067 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.944, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22270 Training loss 0.051032453775405884 Validation loss 0.05135953798890114 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.953, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22280 Training loss 0.05374753847718239 Validation loss 0.05140242725610733 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.941, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22290 Training loss 0.05225410312414169 Validation loss 0.051343560218811035 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.948, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22300 Training loss 0.05424046143889427 Validation loss 0.05135478079319 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.966, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22310 Training loss 0.04834761470556259 Validation loss 0.05146797373890877 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.97, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22320 Training loss 0.05324631929397583 Validation loss 0.05136460065841675 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.968, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22330 Training loss 0.05335937440395355 Validation loss 0.051369745284318924 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.968, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22340 Training loss 0.05143532156944275 Validation loss 0.05127137154340744 Accuracy 0.4855000078678131 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22350 Training loss 0.05080796778202057 Validation loss 0.05129176750779152 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.962, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22360 Training loss 0.050242066383361816 Validation loss 0.05132124200463295 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.965, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22370 Training loss 0.05225617438554764 Validation loss 0.05142267048358917 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.959, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22380 Training loss 0.05016050860285759 Validation loss 0.051383133977651596 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.96, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22390 Training loss 0.05115910992026329 Validation loss 0.051377519965171814 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.97, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22400 Training loss 0.053725987672805786 Validation loss 0.0516672357916832 Accuracy 0.4812000095844269 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.971, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22410 Training loss 0.049590617418289185 Validation loss 0.0513167679309845 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.973, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22420 Training loss 0.048937827348709106 Validation loss 0.05201295018196106 Accuracy 0.4767000079154968 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.985, dtype=float32), array(0.895, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22430 Training loss 0.05172789469361305 Validation loss 0.0518706776201725 Accuracy 0.47929999232292175 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.982, dtype=float32), array(0.901, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22440 Training loss 0.0530933178961277 Validation loss 0.05155833065509796 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.974, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22450 Training loss 0.048835866153240204 Validation loss 0.051599688827991486 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.973, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22460 Training loss 0.04962436482310295 Validation loss 0.05161084979772568 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.968, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22470 Training loss 0.048310186713933945 Validation loss 0.051541075110435486 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.94, dtype=float32), array(0.958, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22480 Training loss 0.050441354513168335 Validation loss 0.0516170896589756 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.944, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22490 Training loss 0.0539214164018631 Validation loss 0.051565155386924744 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.94, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22500 Training loss 0.05018554627895355 Validation loss 0.051620662212371826 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.945, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22510 Training loss 0.04991947486996651 Validation loss 0.05132688581943512 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.956, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22520 Training loss 0.04949120432138443 Validation loss 0.05148271471261978 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.962, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22530 Training loss 0.05131741240620613 Validation loss 0.05146632716059685 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.967, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22540 Training loss 0.05149225890636444 Validation loss 0.0513279102742672 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.957, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22550 Training loss 0.04935244843363762 Validation loss 0.0514850877225399 Accuracy 0.48249998688697815 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.928, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22560 Training loss 0.05320979282259941 Validation loss 0.05173894017934799 Accuracy 0.48010000586509705 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.907, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22570 Training loss 0.05097836256027222 Validation loss 0.0512809231877327 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.945, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22580 Training loss 0.051954448223114014 Validation loss 0.05138939991593361 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.951, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22590 Training loss 0.049802154302597046 Validation loss 0.05131656676530838 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.95, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22600 Training loss 0.053605977445840836 Validation loss 0.05130649358034134 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22610 Training loss 0.04893508926033974 Validation loss 0.05137322098016739 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.957, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22620 Training loss 0.04644540324807167 Validation loss 0.051213350147008896 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.952, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22630 Training loss 0.05479790270328522 Validation loss 0.05125992372632027 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.957, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22640 Training loss 0.045708950608968735 Validation loss 0.05125251039862633 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.962, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22650 Training loss 0.051975399255752563 Validation loss 0.05149663984775543 Accuracy 0.4828000068664551 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.933, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22660 Training loss 0.05154566466808319 Validation loss 0.05171937495470047 Accuracy 0.48030000925064087 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.91, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22670 Training loss 0.05217272788286209 Validation loss 0.05181010812520981 Accuracy 0.47929999232292175 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.889, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22680 Training loss 0.051280293613672256 Validation loss 0.05166421830654144 Accuracy 0.4810999929904938 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.902, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22690 Training loss 0.05130652338266373 Validation loss 0.05166427791118622 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.905, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22700 Training loss 0.049421049654483795 Validation loss 0.05173928663134575 Accuracy 0.47999998927116394 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.903, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22710 Training loss 0.05205197632312775 Validation loss 0.0515320748090744 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.933, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22720 Training loss 0.04970952495932579 Validation loss 0.0513618178665638 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.959, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22730 Training loss 0.05028635635972023 Validation loss 0.05144922807812691 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.949, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22740 Training loss 0.04883192852139473 Validation loss 0.05164186656475067 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.927, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22750 Training loss 0.052609700709581375 Validation loss 0.051396600902080536 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.946, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22760 Training loss 0.05201765522360802 Validation loss 0.05143152177333832 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.944, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22770 Training loss 0.05114353820681572 Validation loss 0.0514751560986042 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.948, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22780 Training loss 0.053682729601860046 Validation loss 0.051335159689188004 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.964, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22790 Training loss 0.049788035452365875 Validation loss 0.05128994584083557 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.963, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22800 Training loss 0.050308242440223694 Validation loss 0.051317669451236725 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.95, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22810 Training loss 0.05154978483915329 Validation loss 0.05132756754755974 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.964, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22820 Training loss 0.0479825995862484 Validation loss 0.051416587084531784 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.94, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22830 Training loss 0.05108720436692238 Validation loss 0.0513119101524353 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.962, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22840 Training loss 0.05176049843430519 Validation loss 0.051252949982881546 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.966, dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22850 Training loss 0.05453969165682793 Validation loss 0.05124656856060028 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.948, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22860 Training loss 0.05263263359665871 Validation loss 0.051323290914297104 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.953, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22870 Training loss 0.05141492187976837 Validation loss 0.0514678955078125 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.97, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22880 Training loss 0.05185910686850548 Validation loss 0.05151887238025665 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.971, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22890 Training loss 0.05284270644187927 Validation loss 0.05138086527585983 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.968, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22900 Training loss 0.048365235328674316 Validation loss 0.05127270519733429 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.964, dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22910 Training loss 0.05266987159848213 Validation loss 0.051262788474559784 Accuracy 0.48500001430511475 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.957, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22920 Training loss 0.050722889602184296 Validation loss 0.05135324224829674 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.947, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22930 Training loss 0.04551001638174057 Validation loss 0.05129041150212288 Accuracy 0.48510000109672546 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.962, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22940 Training loss 0.047684021294116974 Validation loss 0.051454003900289536 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.948, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22950 Training loss 0.050807442516088486 Validation loss 0.05155395343899727 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.932, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22960 Training loss 0.04877253249287605 Validation loss 0.05130307748913765 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.96, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22970 Training loss 0.05153835937380791 Validation loss 0.05129427835345268 Accuracy 0.4851999878883362 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.961, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22980 Training loss 0.04861951246857643 Validation loss 0.05123709887266159 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.965, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 22990 Training loss 0.05016165226697922 Validation loss 0.051290057599544525 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.957, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23000 Training loss 0.04955705627799034 Validation loss 0.05138916149735451 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.958, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23010 Training loss 0.05212636664509773 Validation loss 0.05133003741502762 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.967, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23020 Training loss 0.050105586647987366 Validation loss 0.0514594204723835 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.975, dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23030 Training loss 0.0559343658387661 Validation loss 0.051683180034160614 Accuracy 0.48080000281333923 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.976, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23040 Training loss 0.05029720440506935 Validation loss 0.05214056745171547 Accuracy 0.4767000079154968 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.974, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23050 Training loss 0.050273001194000244 Validation loss 0.051796045154333115 Accuracy 0.4805000126361847 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.97, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23060 Training loss 0.04802432283759117 Validation loss 0.05183762311935425 Accuracy 0.47940000891685486 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.982, dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23070 Training loss 0.053684454411268234 Validation loss 0.05128205567598343 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.966, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23080 Training loss 0.05134348198771477 Validation loss 0.051340535283088684 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.96, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23090 Training loss 0.04960137605667114 Validation loss 0.05137309432029724 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.952, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23100 Training loss 0.05037069693207741 Validation loss 0.05141095817089081 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.949, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23110 Training loss 0.05369880050420761 Validation loss 0.05233392491936684 Accuracy 0.47350001335144043 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.955, dtype=float32), array(0.836, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23120 Training loss 0.0558888278901577 Validation loss 0.051544226706027985 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.976, dtype=float32), array(0.952, dtype=float32), array(0.924, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23130 Training loss 0.052765730768442154 Validation loss 0.0514054149389267 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.95, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23140 Training loss 0.04978469759225845 Validation loss 0.05131328105926514 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.943, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23150 Training loss 0.05504009872674942 Validation loss 0.051509544253349304 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.949, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23160 Training loss 0.048919860273599625 Validation loss 0.051235318183898926 Accuracy 0.48510000109672546 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.958, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23170 Training loss 0.05014701187610626 Validation loss 0.051369912922382355 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.95, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23180 Training loss 0.052068933844566345 Validation loss 0.051348261535167694 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.934, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23190 Training loss 0.046375822275877 Validation loss 0.051316846162080765 Accuracy 0.4851999878883362 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.955, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23200 Training loss 0.046317946165800095 Validation loss 0.05145911127328873 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.956, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23210 Training loss 0.0524313859641552 Validation loss 0.051619697362184525 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.952, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23220 Training loss 0.052408408373594284 Validation loss 0.05143164098262787 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.949, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23230 Training loss 0.05039435252547264 Validation loss 0.05143982917070389 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.951, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23240 Training loss 0.05144743248820305 Validation loss 0.05136880278587341 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.957, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23250 Training loss 0.049021799117326736 Validation loss 0.05144702270627022 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.953, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23260 Training loss 0.05072677135467529 Validation loss 0.051689065992832184 Accuracy 0.4812999963760376 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.945, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23270 Training loss 0.05170051008462906 Validation loss 0.051425594836473465 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.957, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23280 Training loss 0.04647627845406532 Validation loss 0.05152418836951256 Accuracy 0.4821000099182129 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.971, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23290 Training loss 0.047511473298072815 Validation loss 0.05131208896636963 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.954, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23300 Training loss 0.04992559924721718 Validation loss 0.05144912004470825 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.938, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23310 Training loss 0.05161555856466293 Validation loss 0.05163819342851639 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.908, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23320 Training loss 0.054383136332035065 Validation loss 0.05170004069805145 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.921, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23330 Training loss 0.050512105226516724 Validation loss 0.05151505768299103 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.941, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23340 Training loss 0.05337909981608391 Validation loss 0.05146240070462227 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.945, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23350 Training loss 0.05111333355307579 Validation loss 0.05143095552921295 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.951, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23360 Training loss 0.04852834716439247 Validation loss 0.05138001963496208 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.95, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23370 Training loss 0.04868451878428459 Validation loss 0.051453717052936554 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.976, dtype=float32), array(0.915, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23380 Training loss 0.052608661353588104 Validation loss 0.05134342238306999 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.941, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23390 Training loss 0.05285371467471123 Validation loss 0.05143941938877106 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.952, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23400 Training loss 0.05489230528473854 Validation loss 0.05139382928609848 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.964, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23410 Training loss 0.05075840651988983 Validation loss 0.05141095817089081 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.953, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23420 Training loss 0.05345754325389862 Validation loss 0.05139685049653053 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.949, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23430 Training loss 0.049073074012994766 Validation loss 0.0516810342669487 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.932, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23440 Training loss 0.048888009041547775 Validation loss 0.05140646547079086 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.945, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23450 Training loss 0.05058253929018974 Validation loss 0.05149601399898529 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.946, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23460 Training loss 0.052643097937107086 Validation loss 0.05137379840016365 Accuracy 0.48510000109672546 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.96, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23470 Training loss 0.05007977411150932 Validation loss 0.05143429711461067 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.947, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23480 Training loss 0.051967184990644455 Validation loss 0.051347315311431885 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.952, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23490 Training loss 0.050426945090293884 Validation loss 0.05140644684433937 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.945, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23500 Training loss 0.05241808667778969 Validation loss 0.051332950592041016 Accuracy 0.4855000078678131 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.957, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23510 Training loss 0.05141477286815643 Validation loss 0.051339369267225266 Accuracy 0.4853000044822693 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.953, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23520 Training loss 0.05062669888138771 Validation loss 0.05128664895892143 Accuracy 0.48539999127388 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.952, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23530 Training loss 0.053104106336832047 Validation loss 0.051271211355924606 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23540 Training loss 0.05160610005259514 Validation loss 0.05137241259217262 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.957, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23550 Training loss 0.04870952293276787 Validation loss 0.05152785778045654 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.949, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23560 Training loss 0.0509364977478981 Validation loss 0.05152009800076485 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.961, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23570 Training loss 0.049904339015483856 Validation loss 0.05140908062458038 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.965, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23580 Training loss 0.04910294711589813 Validation loss 0.051530297845602036 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.971, dtype=float32), array(0.914, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23590 Training loss 0.048794765025377274 Validation loss 0.051393233239650726 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.973, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23600 Training loss 0.05495668575167656 Validation loss 0.05150846764445305 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.957, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23610 Training loss 0.053141966462135315 Validation loss 0.05150148272514343 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.946, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23620 Training loss 0.04901954531669617 Validation loss 0.05147615075111389 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.925, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23630 Training loss 0.05337829142808914 Validation loss 0.051313407719135284 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.944, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23640 Training loss 0.05534813553094864 Validation loss 0.0514686293900013 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.969, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23650 Training loss 0.05101411044597626 Validation loss 0.05136580392718315 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.974, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23660 Training loss 0.04849874973297119 Validation loss 0.05150974169373512 Accuracy 0.4823000133037567 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.969, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23670 Training loss 0.05065863952040672 Validation loss 0.051592715084552765 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.963, dtype=float32), array(0.925, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23680 Training loss 0.05117948353290558 Validation loss 0.05148007720708847 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.972, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23690 Training loss 0.05536722391843796 Validation loss 0.05139981955289841 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.957, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23700 Training loss 0.049387186765670776 Validation loss 0.051493994891643524 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.955, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23710 Training loss 0.052733827382326126 Validation loss 0.05203207954764366 Accuracy 0.4778999984264374 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.959, dtype=float32), array(0.89, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23720 Training loss 0.05345699191093445 Validation loss 0.05126681551337242 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.968, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23730 Training loss 0.04854710027575493 Validation loss 0.05145829916000366 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.96, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23740 Training loss 0.05375238507986069 Validation loss 0.051462847739458084 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.961, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23750 Training loss 0.05354407802224159 Validation loss 0.05143863335251808 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.952, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23760 Training loss 0.05088658630847931 Validation loss 0.051595576107501984 Accuracy 0.4819999933242798 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.957, dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23770 Training loss 0.04510565102100372 Validation loss 0.05142664164304733 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.943, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23780 Training loss 0.05454669147729874 Validation loss 0.05165709927678108 Accuracy 0.48190000653266907 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.938, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23790 Training loss 0.047607146203517914 Validation loss 0.05157582834362984 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.964, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23800 Training loss 0.05179944261908531 Validation loss 0.05139422416687012 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.941, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23810 Training loss 0.052536774426698685 Validation loss 0.051241546869277954 Accuracy 0.48510000109672546 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.969, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23820 Training loss 0.05036028474569321 Validation loss 0.051354531198740005 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.946, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23830 Training loss 0.04976405203342438 Validation loss 0.05134379863739014 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.944, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23840 Training loss 0.05270756036043167 Validation loss 0.051494017243385315 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.955, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23850 Training loss 0.04905247315764427 Validation loss 0.05148524418473244 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.963, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23860 Training loss 0.053020112216472626 Validation loss 0.05133236199617386 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.959, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23870 Training loss 0.0525522343814373 Validation loss 0.05141586437821388 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.975, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23880 Training loss 0.048326391726732254 Validation loss 0.05140332877635956 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.965, dtype=float32), array(0.928, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23890 Training loss 0.05286023020744324 Validation loss 0.051353614777326584 Accuracy 0.48429998755455017 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.955, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23900 Training loss 0.051504381000995636 Validation loss 0.05137425661087036 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.974, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23910 Training loss 0.05035862326622009 Validation loss 0.05149096995592117 Accuracy 0.4830999970436096 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.973, dtype=float32), array(0.932, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23920 Training loss 0.051769938319921494 Validation loss 0.05148480460047722 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.973, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23930 Training loss 0.048611801117658615 Validation loss 0.051303520798683167 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.969, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23940 Training loss 0.054352402687072754 Validation loss 0.05142576992511749 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.958, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23950 Training loss 0.04992258548736572 Validation loss 0.05160791054368019 Accuracy 0.4814000129699707 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.945, dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23960 Training loss 0.04901319369673729 Validation loss 0.051329355686903 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.971, dtype=float32), array(0.934, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23970 Training loss 0.05384031683206558 Validation loss 0.051304854452610016 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.934, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23980 Training loss 0.04969286173582077 Validation loss 0.051576465368270874 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.928, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 23990 Training loss 0.051910191774368286 Validation loss 0.05165103077888489 Accuracy 0.4805999994277954 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.906, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24000 Training loss 0.055370405316352844 Validation loss 0.05153118073940277 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.931, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24010 Training loss 0.049903504550457 Validation loss 0.051309119910001755 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.946, dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24020 Training loss 0.052261460572481155 Validation loss 0.05153006315231323 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.932, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24030 Training loss 0.050146572291851044 Validation loss 0.051256489008665085 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.945, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24040 Training loss 0.0518602691590786 Validation loss 0.05145039036870003 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.952, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24050 Training loss 0.048692066222429276 Validation loss 0.05139988660812378 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.95, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24060 Training loss 0.04983232542872429 Validation loss 0.051334962248802185 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.948, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24070 Training loss 0.04944774881005287 Validation loss 0.052217770367860794 Accuracy 0.47450000047683716 Accuracies by class [array(0., dtype=float32), array(0.979, dtype=float32), array(0.952, dtype=float32), array(0.863, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24080 Training loss 0.047877948731184006 Validation loss 0.051398348063230515 Accuracy 0.4828999936580658 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.938, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24090 Training loss 0.05574425309896469 Validation loss 0.051447849720716476 Accuracy 0.482699990272522 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.938, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24100 Training loss 0.05079003795981407 Validation loss 0.051293592900037766 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.945, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24110 Training loss 0.0513809509575367 Validation loss 0.05150830000638962 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.948, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24120 Training loss 0.05155843123793602 Validation loss 0.051375288516283035 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.945, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24130 Training loss 0.05007292330265045 Validation loss 0.05131978541612625 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.951, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24140 Training loss 0.050657693296670914 Validation loss 0.05141798406839371 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.963, dtype=float32), array(0.929, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24150 Training loss 0.04866955056786537 Validation loss 0.05142046883702278 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.939, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24160 Training loss 0.0553789883852005 Validation loss 0.051350757479667664 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.956, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24170 Training loss 0.051039114594459534 Validation loss 0.05140425264835358 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.951, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24180 Training loss 0.053914520889520645 Validation loss 0.05140234902501106 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.97, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24190 Training loss 0.04823973774909973 Validation loss 0.051531895995140076 Accuracy 0.483599990606308 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.958, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24200 Training loss 0.051085155457258224 Validation loss 0.051371991634368896 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.973, dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24210 Training loss 0.04942202940583229 Validation loss 0.051508188247680664 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.948, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24220 Training loss 0.05279376357793808 Validation loss 0.051737021654844284 Accuracy 0.48100000619888306 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.907, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24230 Training loss 0.05161159858107567 Validation loss 0.05146130919456482 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.956, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24240 Training loss 0.04799381643533707 Validation loss 0.051638148725032806 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.979, dtype=float32), array(0.902, dtype=float32), array(0.001, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24250 Training loss 0.046362243592739105 Validation loss 0.051831986755132675 Accuracy 0.4797999858856201 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.982, dtype=float32), array(0.881, dtype=float32), array(0.001, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24260 Training loss 0.04899555817246437 Validation loss 0.051345352083444595 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.964, dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24270 Training loss 0.050418656319379807 Validation loss 0.051505353301763535 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.95, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24280 Training loss 0.04918958619236946 Validation loss 0.051440369337797165 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.963, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24290 Training loss 0.051810525357723236 Validation loss 0.05140549689531326 Accuracy 0.4844000041484833 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.973, dtype=float32), array(0.932, dtype=float32), array(0.001, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24300 Training loss 0.0514826662838459 Validation loss 0.05155738443136215 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.973, dtype=float32), array(0.918, dtype=float32), array(0.001, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24310 Training loss 0.05122506618499756 Validation loss 0.051524288952350616 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.977, dtype=float32), array(0.923, dtype=float32), array(0.001, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24320 Training loss 0.053027380257844925 Validation loss 0.051546934992074966 Accuracy 0.4821999967098236 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.978, dtype=float32), array(0.911, dtype=float32), array(0.001, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24330 Training loss 0.052210189402103424 Validation loss 0.05137075111269951 Accuracy 0.48339998722076416 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.971, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24340 Training loss 0.05246519297361374 Validation loss 0.05122366175055504 Accuracy 0.48539999127388 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.971, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24350 Training loss 0.0505894310772419 Validation loss 0.05131997913122177 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.972, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24360 Training loss 0.048118311911821365 Validation loss 0.051346439868211746 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.968, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24370 Training loss 0.04944327846169472 Validation loss 0.05129159614443779 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.965, dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24380 Training loss 0.048343557864427567 Validation loss 0.0513165220618248 Accuracy 0.48539999127388 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.964, dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24390 Training loss 0.048997558653354645 Validation loss 0.05140061676502228 Accuracy 0.48420000076293945 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.968, dtype=float32), array(0.938, dtype=float32), array(0.001, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24400 Training loss 0.050519999116659164 Validation loss 0.05137118324637413 Accuracy 0.4846000075340271 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.974, dtype=float32), array(0.933, dtype=float32), array(0.001, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24410 Training loss 0.052365683019161224 Validation loss 0.05128420516848564 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.965, dtype=float32), array(0.935, dtype=float32), array(0.001, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24420 Training loss 0.05025358498096466 Validation loss 0.051292967051267624 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.968, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24430 Training loss 0.052087850868701935 Validation loss 0.05131158232688904 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.968, dtype=float32), array(0.924, dtype=float32), array(0.001, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24440 Training loss 0.045596893876791 Validation loss 0.05129973217844963 Accuracy 0.484499990940094 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.968, dtype=float32), array(0.94, dtype=float32), array(0.001, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24450 Training loss 0.050231024622917175 Validation loss 0.05126337334513664 Accuracy 0.4848000109195709 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.966, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24460 Training loss 0.05346352979540825 Validation loss 0.05156465992331505 Accuracy 0.48170000314712524 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.956, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24470 Training loss 0.052139170467853546 Validation loss 0.05139541998505592 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.946, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24480 Training loss 0.05249704420566559 Validation loss 0.0514240525662899 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.949, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24490 Training loss 0.04931183531880379 Validation loss 0.051486000418663025 Accuracy 0.48330000042915344 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.939, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24500 Training loss 0.05162418633699417 Validation loss 0.051877520978450775 Accuracy 0.47870001196861267 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.892, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24510 Training loss 0.053947724401950836 Validation loss 0.05198679864406586 Accuracy 0.47699999809265137 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.897, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24520 Training loss 0.05270383879542351 Validation loss 0.0513446107506752 Accuracy 0.48410001397132874 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.954, dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24530 Training loss 0.05016819387674332 Validation loss 0.05180545523762703 Accuracy 0.4799000024795532 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.933, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24540 Training loss 0.049772001802921295 Validation loss 0.05142512172460556 Accuracy 0.4830000102519989 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.961, dtype=float32), array(0.94, dtype=float32), array(0.001, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24550 Training loss 0.049500320106744766 Validation loss 0.051277920603752136 Accuracy 0.48510000109672546 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.969, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24560 Training loss 0.055390890687704086 Validation loss 0.05134038254618645 Accuracy 0.4846999943256378 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.974, dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24570 Training loss 0.053512249141931534 Validation loss 0.05163455381989479 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.981, dtype=float32), array(0.906, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24580 Training loss 0.05032365769147873 Validation loss 0.051641449332237244 Accuracy 0.48159998655319214 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.98, dtype=float32), array(0.905, dtype=float32), array(0.001, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24590 Training loss 0.04890073463320732 Validation loss 0.051552664488554 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.979, dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24600 Training loss 0.04983989894390106 Validation loss 0.051636356860399246 Accuracy 0.4814999997615814 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.98, dtype=float32), array(0.906, dtype=float32), array(0.001, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24610 Training loss 0.05408851057291031 Validation loss 0.051384784281253815 Accuracy 0.4837000072002411 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.97, dtype=float32), array(0.927, dtype=float32), array(0.001, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24620 Training loss 0.046504441648721695 Validation loss 0.051322076469659805 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.967, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24630 Training loss 0.05013084039092064 Validation loss 0.05125958472490311 Accuracy 0.48489999771118164 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.96, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24640 Training loss 0.0522647500038147 Validation loss 0.051328614354133606 Accuracy 0.4837999939918518 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.95, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24650 Training loss 0.04943671077489853 Validation loss 0.051614683121442795 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.937, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24660 Training loss 0.04920363798737526 Validation loss 0.05138451233506203 Accuracy 0.4839000105857849 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.946, dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24670 Training loss 0.05157887935638428 Validation loss 0.05153859406709671 Accuracy 0.4832000136375427 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.937, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24680 Training loss 0.048188693821430206 Validation loss 0.051554422825574875 Accuracy 0.48260000348091125 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.922, dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24690 Training loss 0.048260774463415146 Validation loss 0.05160681530833244 Accuracy 0.48179998993873596 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.92, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24700 Training loss 0.051471199840307236 Validation loss 0.05156172811985016 Accuracy 0.48240000009536743 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.913, dtype=float32), array(0.961, dtype=float32), array(0.001, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24710 Training loss 0.05197148770093918 Validation loss 0.0513700395822525 Accuracy 0.48399999737739563 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.942, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24720 Training loss 0.04735301807522774 Validation loss 0.05138219892978668 Accuracy 0.48350000381469727 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.936, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24730 Training loss 0.050800297409296036 Validation loss 0.05081767216324806 Accuracy 0.4893999993801117 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.909, dtype=float32), array(0.935, dtype=float32), array(0.101, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24740 Training loss 0.045033346861600876 Validation loss 0.0490286611020565 Accuracy 0.5051000118255615 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.767, dtype=float32), array(0.918, dtype=float32), array(0.43, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24750 Training loss 0.04796966537833214 Validation loss 0.050127193331718445 Accuracy 0.4959000051021576 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.868, dtype=float32), array(0.972, dtype=float32), array(0.194, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24760 Training loss 0.0540066733956337 Validation loss 0.051032714545726776 Accuracy 0.48730000853538513 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.942, dtype=float32), array(0.969, dtype=float32), array(0.048, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24770 Training loss 0.0478995144367218 Validation loss 0.04799695312976837 Accuracy 0.5163000226020813 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.646, dtype=float32), array(0.872, dtype=float32), array(0.732, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24780 Training loss 0.0469081811606884 Validation loss 0.04836324229836464 Accuracy 0.5133000016212463 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.82, dtype=float32), array(0.913, dtype=float32), array(0.457, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24790 Training loss 0.048782043159008026 Validation loss 0.04965192824602127 Accuracy 0.5001000165939331 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.941, dtype=float32), array(0.93, dtype=float32), array(0.203, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24800 Training loss 0.052847545593976974 Validation loss 0.05103423818945885 Accuracy 0.4869000017642975 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.956, dtype=float32), array(0.965, dtype=float32), array(0.031, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24810 Training loss 0.050542108714580536 Validation loss 0.0506712831556797 Accuracy 0.49000000953674316 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.405, dtype=float32), array(0.676, dtype=float32), array(0.92, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24820 Training loss 0.05000472441315651 Validation loss 0.048641495406627655 Accuracy 0.5091000199317932 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.567, dtype=float32), array(0.77, dtype=float32), array(0.822, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24830 Training loss 0.04669055715203285 Validation loss 0.04740500822663307 Accuracy 0.522599995136261 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.638, dtype=float32), array(0.937, dtype=float32), array(0.716, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24840 Training loss 0.044411905109882355 Validation loss 0.0469241738319397 Accuracy 0.5266000032424927 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.66, dtype=float32), array(0.927, dtype=float32), array(0.75, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24850 Training loss 0.04765913262963295 Validation loss 0.04700284078717232 Accuracy 0.5264999866485596 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.693, dtype=float32), array(0.883, dtype=float32), array(0.751, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24860 Training loss 0.04640566185116768 Validation loss 0.04783619940280914 Accuracy 0.5187000036239624 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.467, dtype=float32), array(0.874, dtype=float32), array(0.917, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24870 Training loss 0.04895741119980812 Validation loss 0.04722897335886955 Accuracy 0.5245000123977661 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.609, dtype=float32), array(0.967, dtype=float32), array(0.739, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24880 Training loss 0.04590881988406181 Validation loss 0.04732784256339073 Accuracy 0.5230000019073486 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.58, dtype=float32), array(0.854, dtype=float32), array(0.868, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24890 Training loss 0.04872220754623413 Validation loss 0.04766278341412544 Accuracy 0.5205000042915344 Accuracies by class [array(0., dtype=float32), array(0.977, dtype=float32), array(0.763, dtype=float32), array(0.852, dtype=float32), array(0.653, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24900 Training loss 0.04798806831240654 Validation loss 0.04688321426510811 Accuracy 0.5281999707221985 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.643, dtype=float32), array(0.909, dtype=float32), array(0.808, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24910 Training loss 0.04562068730592728 Validation loss 0.04672781378030777 Accuracy 0.5299999713897705 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.659, dtype=float32), array(0.876, dtype=float32), array(0.829, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24920 Training loss 0.04778961092233658 Validation loss 0.04722839221358299 Accuracy 0.5252000093460083 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.828, dtype=float32), array(0.89, dtype=float32), array(0.604, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24930 Training loss 0.04649056866765022 Validation loss 0.04698093235492706 Accuracy 0.5281000137329102 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.583, dtype=float32), array(0.895, dtype=float32), array(0.888, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24940 Training loss 0.044734466820955276 Validation loss 0.046622384339571 Accuracy 0.531000018119812 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.694, dtype=float32), array(0.871, dtype=float32), array(0.811, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24950 Training loss 0.044945597648620605 Validation loss 0.047503817826509476 Accuracy 0.5228000283241272 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.827, dtype=float32), array(0.965, dtype=float32), array(0.513, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24960 Training loss 0.051790501922369 Validation loss 0.049434881657361984 Accuracy 0.5026999711990356 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.93, dtype=float32), array(0.956, dtype=float32), array(0.224, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24970 Training loss 0.044682472944259644 Validation loss 0.046432070434093475 Accuracy 0.5322999954223633 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.818, dtype=float32), array(0.905, dtype=float32), array(0.679, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24980 Training loss 0.043359506875276566 Validation loss 0.045784104615449905 Accuracy 0.538100004196167 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.727, dtype=float32), array(0.943, dtype=float32), array(0.798, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 24990 Training loss 0.04858444258570671 Validation loss 0.04604560509324074 Accuracy 0.5353999733924866 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.803, dtype=float32), array(0.945, dtype=float32), array(0.698, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25000 Training loss 0.04601971060037613 Validation loss 0.04751832038164139 Accuracy 0.521399974822998 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.468, dtype=float32), array(0.925, dtype=float32), array(0.924, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25010 Training loss 0.045411717146635056 Validation loss 0.047525517642498016 Accuracy 0.5217000246047974 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.863, dtype=float32), array(0.956, dtype=float32), array(0.483, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25020 Training loss 0.045485373586416245 Validation loss 0.04573464021086693 Accuracy 0.5389000177383423 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.686, dtype=float32), array(0.946, dtype=float32), array(0.834, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25030 Training loss 0.045678090304136276 Validation loss 0.0487837977707386 Accuracy 0.5088000297546387 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.937, dtype=float32), array(0.942, dtype=float32), array(0.294, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25040 Training loss 0.04847053065896034 Validation loss 0.04694747552275658 Accuracy 0.5268999934196472 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.534, dtype=float32), array(0.919, dtype=float32), array(0.92, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25050 Training loss 0.045586004853248596 Validation loss 0.04672125354409218 Accuracy 0.5289000272750854 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.563, dtype=float32), array(0.883, dtype=float32), array(0.923, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25060 Training loss 0.04592934250831604 Validation loss 0.04610553756356239 Accuracy 0.5346999764442444 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.824, dtype=float32), array(0.938, dtype=float32), array(0.663, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25070 Training loss 0.04380469769239426 Validation loss 0.045757073909044266 Accuracy 0.5393000245094299 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.751, dtype=float32), array(0.925, dtype=float32), array(0.797, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25080 Training loss 0.04611615464091301 Validation loss 0.046647269278764725 Accuracy 0.5296000242233276 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.86, dtype=float32), array(0.934, dtype=float32), array(0.572, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25090 Training loss 0.04873529449105263 Validation loss 0.04700962081551552 Accuracy 0.5266000032424927 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.871, dtype=float32), array(0.949, dtype=float32), array(0.521, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25100 Training loss 0.04818979278206825 Validation loss 0.04659876972436905 Accuracy 0.5307000279426575 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.551, dtype=float32), array(0.911, dtype=float32), array(0.922, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25110 Training loss 0.045074425637722015 Validation loss 0.045840609818696976 Accuracy 0.536899983882904 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.703, dtype=float32), array(0.874, dtype=float32), array(0.85, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25120 Training loss 0.04620566964149475 Validation loss 0.04689915478229523 Accuracy 0.5267000198364258 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.567, dtype=float32), array(0.885, dtype=float32), array(0.875, dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25130 Training loss 0.044937971979379654 Validation loss 0.04715896397829056 Accuracy 0.524399995803833 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.897, dtype=float32), array(0.896, dtype=float32), array(0.531, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25140 Training loss 0.04386523365974426 Validation loss 0.045777540653944016 Accuracy 0.5375999808311462 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.743, dtype=float32), array(0.886, dtype=float32), array(0.83, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25150 Training loss 0.04517432302236557 Validation loss 0.04693928360939026 Accuracy 0.5275999903678894 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.611, dtype=float32), array(0.86, dtype=float32), array(0.934, dtype=float32), array(0.938, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25160 Training loss 0.04519572854042053 Validation loss 0.0456235446035862 Accuracy 0.5396000146865845 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.765, dtype=float32), array(0.894, dtype=float32), array(0.822, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25170 Training loss 0.04633835703134537 Validation loss 0.04674972966313362 Accuracy 0.527999997138977 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.893, dtype=float32), array(0.929, dtype=float32), array(0.542, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25180 Training loss 0.04965147003531456 Validation loss 0.04849906638264656 Accuracy 0.5112000107765198 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.416, dtype=float32), array(0.836, dtype=float32), array(0.966, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.936, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25190 Training loss 0.04298729822039604 Validation loss 0.04537985846400261 Accuracy 0.5429999828338623 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.794, dtype=float32), array(0.918, dtype=float32), array(0.79, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25200 Training loss 0.04350936412811279 Validation loss 0.04526505246758461 Accuracy 0.5436000227928162 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.741, dtype=float32), array(0.908, dtype=float32), array(0.855, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25210 Training loss 0.04624032974243164 Validation loss 0.047950826585292816 Accuracy 0.5170000195503235 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.542, dtype=float32), array(0.737, dtype=float32), array(0.967, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25220 Training loss 0.04429389908909798 Validation loss 0.046201180666685104 Accuracy 0.5335999727249146 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.793, dtype=float32), array(0.916, dtype=float32), array(0.707, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25230 Training loss 0.04428306221961975 Validation loss 0.04602265730500221 Accuracy 0.5361999869346619 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.811, dtype=float32), array(0.847, dtype=float32), array(0.791, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25240 Training loss 0.04376114904880524 Validation loss 0.04620038717985153 Accuracy 0.5347999930381775 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.7, dtype=float32), array(0.822, dtype=float32), array(0.913, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25250 Training loss 0.0483999140560627 Validation loss 0.045827481895685196 Accuracy 0.5386000275611877 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.782, dtype=float32), array(0.867, dtype=float32), array(0.824, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25260 Training loss 0.0459347739815712 Validation loss 0.045887500047683716 Accuracy 0.5382999777793884 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.665, dtype=float32), array(0.922, dtype=float32), array(0.883, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25270 Training loss 0.04803221672773361 Validation loss 0.04591435566544533 Accuracy 0.5385000109672546 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.739, dtype=float32), array(0.853, dtype=float32), array(0.883, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25280 Training loss 0.046384911984205246 Validation loss 0.04559962451457977 Accuracy 0.5404999852180481 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.792, dtype=float32), array(0.914, dtype=float32), array(0.775, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25290 Training loss 0.04323526844382286 Validation loss 0.04660385102033615 Accuracy 0.5309000015258789 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.589, dtype=float32), array(0.859, dtype=float32), array(0.939, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25300 Training loss 0.043448470532894135 Validation loss 0.045956723392009735 Accuracy 0.5367000102996826 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.817, dtype=float32), array(0.956, dtype=float32), array(0.67, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25310 Training loss 0.04271233081817627 Validation loss 0.045261576771736145 Accuracy 0.5442000031471252 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.786, dtype=float32), array(0.917, dtype=float32), array(0.807, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25320 Training loss 0.048300568014383316 Validation loss 0.045767005532979965 Accuracy 0.5394999980926514 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.759, dtype=float32), array(0.859, dtype=float32), array(0.865, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25330 Training loss 0.043652985244989395 Validation loss 0.0464978963136673 Accuracy 0.5325000286102295 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.592, dtype=float32), array(0.901, dtype=float32), array(0.923, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25340 Training loss 0.04461671784520149 Validation loss 0.04556511715054512 Accuracy 0.5410000085830688 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.82, dtype=float32), array(0.951, dtype=float32), array(0.708, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25350 Training loss 0.044036462903022766 Validation loss 0.04566745087504387 Accuracy 0.5407999753952026 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.744, dtype=float32), array(0.886, dtype=float32), array(0.874, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25360 Training loss 0.04528779163956642 Validation loss 0.045739393681287766 Accuracy 0.5400999784469604 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.743, dtype=float32), array(0.868, dtype=float32), array(0.878, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25370 Training loss 0.04464837536215782 Validation loss 0.04615161567926407 Accuracy 0.5364999771118164 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.709, dtype=float32), array(0.825, dtype=float32), array(0.913, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25380 Training loss 0.0451427698135376 Validation loss 0.045131344348192215 Accuracy 0.5450000166893005 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.774, dtype=float32), array(0.903, dtype=float32), array(0.835, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25390 Training loss 0.047642167657613754 Validation loss 0.04580020159482956 Accuracy 0.5383999943733215 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.83, dtype=float32), array(0.933, dtype=float32), array(0.694, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25400 Training loss 0.04387651011347771 Validation loss 0.04591449350118637 Accuracy 0.5376999974250793 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.724, dtype=float32), array(0.883, dtype=float32), array(0.84, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25410 Training loss 0.04609168320894241 Validation loss 0.04540576785802841 Accuracy 0.5422999858856201 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.723, dtype=float32), array(0.938, dtype=float32), array(0.835, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25420 Training loss 0.045559387654066086 Validation loss 0.04534369707107544 Accuracy 0.5436000227928162 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.758, dtype=float32), array(0.953, dtype=float32), array(0.81, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25430 Training loss 0.047155655920505524 Validation loss 0.047657109797000885 Accuracy 0.5199000239372253 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.5, dtype=float32), array(0.829, dtype=float32), array(0.967, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25440 Training loss 0.047264229506254196 Validation loss 0.045967891812324524 Accuracy 0.5374000072479248 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.639, dtype=float32), array(0.901, dtype=float32), array(0.925, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25450 Training loss 0.041705064475536346 Validation loss 0.045644424855709076 Accuracy 0.5412999987602234 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.668, dtype=float32), array(0.944, dtype=float32), array(0.878, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25460 Training loss 0.043889038264751434 Validation loss 0.047117460519075394 Accuracy 0.5260999798774719 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.502, dtype=float32), array(0.895, dtype=float32), array(0.943, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25470 Training loss 0.048844706267118454 Validation loss 0.046284258365631104 Accuracy 0.534500002861023 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.661, dtype=float32), array(0.829, dtype=float32), array(0.933, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25480 Training loss 0.04571321979165077 Validation loss 0.045447979122400284 Accuracy 0.5432000160217285 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.707, dtype=float32), array(0.918, dtype=float32), array(0.884, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25490 Training loss 0.04911385104060173 Validation loss 0.046273231506347656 Accuracy 0.5349000096321106 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.901, dtype=float32), array(0.924, dtype=float32), array(0.593, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25500 Training loss 0.045284148305654526 Validation loss 0.04545369744300842 Accuracy 0.5422000288963318 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.703, dtype=float32), array(0.93, dtype=float32), array(0.864, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25510 Training loss 0.04535435512661934 Validation loss 0.04535617679357529 Accuracy 0.5435000061988831 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.766, dtype=float32), array(0.939, dtype=float32), array(0.8, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25520 Training loss 0.04605559632182121 Validation loss 0.04644494503736496 Accuracy 0.5325000286102295 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.596, dtype=float32), array(0.866, dtype=float32), array(0.944, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25530 Training loss 0.042994607239961624 Validation loss 0.04602498933672905 Accuracy 0.5364999771118164 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.753, dtype=float32), array(0.97, dtype=float32), array(0.729, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25540 Training loss 0.04295323044061661 Validation loss 0.04596522077918053 Accuracy 0.5368000268936157 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.73, dtype=float32), array(0.85, dtype=float32), array(0.877, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25550 Training loss 0.043813105672597885 Validation loss 0.0461554192006588 Accuracy 0.5351999998092651 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.898, dtype=float32), array(0.93, dtype=float32), array(0.627, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25560 Training loss 0.04839232563972473 Validation loss 0.04529346153140068 Accuracy 0.5437999963760376 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.856, dtype=float32), array(0.938, dtype=float32), array(0.729, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25570 Training loss 0.04724618047475815 Validation loss 0.04514355957508087 Accuracy 0.5439000129699707 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.806, dtype=float32), array(0.929, dtype=float32), array(0.795, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25580 Training loss 0.04742387309670448 Validation loss 0.045397039502859116 Accuracy 0.5425999760627747 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.815, dtype=float32), array(0.925, dtype=float32), array(0.797, dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25590 Training loss 0.044184427708387375 Validation loss 0.045633018016815186 Accuracy 0.5388000011444092 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.896, dtype=float32), array(0.934, dtype=float32), array(0.65, dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25600 Training loss 0.04614065960049629 Validation loss 0.04613614082336426 Accuracy 0.5346999764442444 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.699, dtype=float32), array(0.803, dtype=float32), array(0.924, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25610 Training loss 0.04442823305726051 Validation loss 0.04521263763308525 Accuracy 0.5440000295639038 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.707, dtype=float32), array(0.906, dtype=float32), array(0.902, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25620 Training loss 0.04552657529711723 Validation loss 0.045478951185941696 Accuracy 0.5422000288963318 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.896, dtype=float32), array(0.927, dtype=float32), array(0.668, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25630 Training loss 0.04099268466234207 Validation loss 0.045763369649648666 Accuracy 0.5393999814987183 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.857, dtype=float32), array(0.964, dtype=float32), array(0.653, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25640 Training loss 0.04599723219871521 Validation loss 0.04750463739037514 Accuracy 0.5202000141143799 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.92, dtype=float32), array(0.967, dtype=float32), array(0.397, dtype=float32), array(0.99, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25650 Training loss 0.04334695264697075 Validation loss 0.04604959115386009 Accuracy 0.5360999703407288 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.637, dtype=float32), array(0.962, dtype=float32), array(0.833, dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.986, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25660 Training loss 0.045880503952503204 Validation loss 0.04584094136953354 Accuracy 0.538100004196167 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.832, dtype=float32), array(0.958, dtype=float32), array(0.689, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25670 Training loss 0.04308241605758667 Validation loss 0.04535238444805145 Accuracy 0.5425999760627747 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.824, dtype=float32), array(0.92, dtype=float32), array(0.764, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25680 Training loss 0.04302775114774704 Validation loss 0.044933583587408066 Accuracy 0.5462999939918518 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.779, dtype=float32), array(0.9, dtype=float32), array(0.852, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25690 Training loss 0.042182572185993195 Validation loss 0.04547860473394394 Accuracy 0.541700005531311 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.856, dtype=float32), array(0.954, dtype=float32), array(0.675, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25700 Training loss 0.04558883607387543 Validation loss 0.04570188373327255 Accuracy 0.5408999919891357 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.674, dtype=float32), array(0.948, dtype=float32), array(0.853, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25710 Training loss 0.046593036502599716 Validation loss 0.04521525278687477 Accuracy 0.5460000038146973 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.789, dtype=float32), array(0.944, dtype=float32), array(0.79, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25720 Training loss 0.04496311396360397 Validation loss 0.0455968864262104 Accuracy 0.5421000123023987 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.711, dtype=float32), array(0.895, dtype=float32), array(0.884, dtype=float32), array(0.999, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25730 Training loss 0.03978199139237404 Validation loss 0.04516094550490379 Accuracy 0.5460000038146973 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.808, dtype=float32), array(0.914, dtype=float32), array(0.819, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25740 Training loss 0.043612804263830185 Validation loss 0.04500552639365196 Accuracy 0.5476999878883362 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.81, dtype=float32), array(0.938, dtype=float32), array(0.8, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25750 Training loss 0.04949037730693817 Validation loss 0.04644337296485901 Accuracy 0.5322999954223633 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.57, dtype=float32), array(0.865, dtype=float32), array(0.942, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25760 Training loss 0.044220246374607086 Validation loss 0.04538913071155548 Accuracy 0.5436000227928162 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.837, dtype=float32), array(0.886, dtype=float32), array(0.771, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25770 Training loss 0.04252009838819504 Validation loss 0.04531257972121239 Accuracy 0.5450000166893005 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.762, dtype=float32), array(0.954, dtype=float32), array(0.799, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25780 Training loss 0.046245746314525604 Validation loss 0.04667023569345474 Accuracy 0.5306000113487244 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.566, dtype=float32), array(0.852, dtype=float32), array(0.95, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25790 Training loss 0.04650061950087547 Validation loss 0.04522077739238739 Accuracy 0.5450999736785889 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.836, dtype=float32), array(0.937, dtype=float32), array(0.759, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25800 Training loss 0.04538087546825409 Validation loss 0.04515710100531578 Accuracy 0.5450000166893005 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.713, dtype=float32), array(0.901, dtype=float32), array(0.894, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25810 Training loss 0.04621826112270355 Validation loss 0.04646420106291771 Accuracy 0.5321000218391418 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.538, dtype=float32), array(0.946, dtype=float32), array(0.905, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25820 Training loss 0.04413735866546631 Validation loss 0.045473359525203705 Accuracy 0.5410000085830688 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.858, dtype=float32), array(0.942, dtype=float32), array(0.688, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25830 Training loss 0.04937468469142914 Validation loss 0.04662194848060608 Accuracy 0.5303999781608582 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.598, dtype=float32), array(0.856, dtype=float32), array(0.944, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25840 Training loss 0.04672890156507492 Validation loss 0.04520160332322121 Accuracy 0.5450999736785889 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.751, dtype=float32), array(0.944, dtype=float32), array(0.836, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25850 Training loss 0.046379975974559784 Validation loss 0.04586886242032051 Accuracy 0.5390999913215637 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.7, dtype=float32), array(0.847, dtype=float32), array(0.918, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25860 Training loss 0.04224194213747978 Validation loss 0.045817743986845016 Accuracy 0.5390999913215637 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.868, dtype=float32), array(0.885, dtype=float32), array(0.724, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25870 Training loss 0.04935935139656067 Validation loss 0.04624086245894432 Accuracy 0.5350000262260437 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.612, dtype=float32), array(0.875, dtype=float32), array(0.937, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25880 Training loss 0.04630337655544281 Validation loss 0.04697665944695473 Accuracy 0.527400016784668 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.57, dtype=float32), array(0.826, dtype=float32), array(0.959, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25890 Training loss 0.04447650536894798 Validation loss 0.046122174710035324 Accuracy 0.5357999801635742 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.608, dtype=float32), array(0.899, dtype=float32), array(0.93, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25900 Training loss 0.04191137105226517 Validation loss 0.04538347199559212 Accuracy 0.5435000061988831 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.862, dtype=float32), array(0.913, dtype=float32), array(0.752, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25910 Training loss 0.0456496886909008 Validation loss 0.04573974758386612 Accuracy 0.5403000116348267 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.883, dtype=float32), array(0.876, dtype=float32), array(0.733, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25920 Training loss 0.05070667341351509 Validation loss 0.048921432346105576 Accuracy 0.5084999799728394 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.955, dtype=float32), array(0.868, dtype=float32), array(0.334, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25930 Training loss 0.04278254136443138 Validation loss 0.04528701305389404 Accuracy 0.5436999797821045 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.713, dtype=float32), array(0.887, dtype=float32), array(0.891, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25940 Training loss 0.04056591913104057 Validation loss 0.045336343348026276 Accuracy 0.5442000031471252 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.734, dtype=float32), array(0.87, dtype=float32), array(0.9, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25950 Training loss 0.04537450149655342 Validation loss 0.046031661331653595 Accuracy 0.5370000004768372 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.634, dtype=float32), array(0.866, dtype=float32), array(0.937, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25960 Training loss 0.04434889554977417 Validation loss 0.04507600516080856 Accuracy 0.5464000105857849 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.776, dtype=float32), array(0.906, dtype=float32), array(0.848, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25970 Training loss 0.045820679515600204 Validation loss 0.04512590914964676 Accuracy 0.5462999939918518 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.783, dtype=float32), array(0.921, dtype=float32), array(0.823, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25980 Training loss 0.04659559205174446 Validation loss 0.045457471162080765 Accuracy 0.5422999858856201 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.679, dtype=float32), array(0.911, dtype=float32), array(0.907, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 25990 Training loss 0.0445866659283638 Validation loss 0.04530906304717064 Accuracy 0.5435000061988831 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.788, dtype=float32), array(0.95, dtype=float32), array(0.795, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26000 Training loss 0.047931019216775894 Validation loss 0.045958299189805984 Accuracy 0.5378000140190125 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.646, dtype=float32), array(0.892, dtype=float32), array(0.935, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26010 Training loss 0.04736883193254471 Validation loss 0.046275511384010315 Accuracy 0.5347999930381775 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.647, dtype=float32), array(0.855, dtype=float32), array(0.945, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26020 Training loss 0.04379904642701149 Validation loss 0.04592793434858322 Accuracy 0.5382999777793884 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.879, dtype=float32), array(0.913, dtype=float32), array(0.704, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26030 Training loss 0.04284646734595299 Validation loss 0.04512132331728935 Accuracy 0.5458999872207642 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.796, dtype=float32), array(0.947, dtype=float32), array(0.804, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26040 Training loss 0.0440046489238739 Validation loss 0.04571833834052086 Accuracy 0.5401999950408936 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.657, dtype=float32), array(0.871, dtype=float32), array(0.935, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26050 Training loss 0.043454062193632126 Validation loss 0.045691005885601044 Accuracy 0.5407000184059143 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.625, dtype=float32), array(0.927, dtype=float32), array(0.918, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26060 Training loss 0.04262489080429077 Validation loss 0.04526074230670929 Accuracy 0.5442000031471252 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.749, dtype=float32), array(0.955, dtype=float32), array(0.812, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26070 Training loss 0.04511155188083649 Validation loss 0.0453173965215683 Accuracy 0.5446000099182129 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.71, dtype=float32), array(0.956, dtype=float32), array(0.842, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26080 Training loss 0.045266132801771164 Validation loss 0.04559287428855896 Accuracy 0.541100025177002 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.877, dtype=float32), array(0.926, dtype=float32), array(0.683, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26090 Training loss 0.046228501945734024 Validation loss 0.04702232405543327 Accuracy 0.527400016784668 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.934, dtype=float32), array(0.912, dtype=float32), array(0.495, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26100 Training loss 0.04404635727405548 Validation loss 0.04552661255002022 Accuracy 0.542900025844574 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.742, dtype=float32), array(0.967, dtype=float32), array(0.794, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26110 Training loss 0.045696526765823364 Validation loss 0.04599651321768761 Accuracy 0.5382999777793884 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.746, dtype=float32), array(0.967, dtype=float32), array(0.745, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26120 Training loss 0.04702986776828766 Validation loss 0.045095376670360565 Accuracy 0.5454999804496765 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.82, dtype=float32), array(0.925, dtype=float32), array(0.774, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26130 Training loss 0.04163298383355141 Validation loss 0.045012105256319046 Accuracy 0.546999990940094 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.775, dtype=float32), array(0.942, dtype=float32), array(0.816, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26140 Training loss 0.0458662249147892 Validation loss 0.04579208046197891 Accuracy 0.5394999980926514 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.899, dtype=float32), array(0.923, dtype=float32), array(0.637, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26150 Training loss 0.04520711302757263 Validation loss 0.04540069028735161 Accuracy 0.5424000024795532 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.796, dtype=float32), array(0.953, dtype=float32), array(0.746, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26160 Training loss 0.04416617751121521 Validation loss 0.04564114660024643 Accuracy 0.5404000282287598 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.797, dtype=float32), array(0.961, dtype=float32), array(0.718, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26170 Training loss 0.04659895971417427 Validation loss 0.04508492723107338 Accuracy 0.5460000038146973 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.788, dtype=float32), array(0.942, dtype=float32), array(0.806, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26180 Training loss 0.04771753028035164 Validation loss 0.04548032209277153 Accuracy 0.5424000024795532 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.675, dtype=float32), array(0.952, dtype=float32), array(0.876, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26190 Training loss 0.043456949293613434 Validation loss 0.04638390988111496 Accuracy 0.5335000157356262 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.588, dtype=float32), array(0.909, dtype=float32), array(0.93, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26200 Training loss 0.04075735807418823 Validation loss 0.04541216790676117 Accuracy 0.5425000190734863 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.766, dtype=float32), array(0.955, dtype=float32), array(0.758, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26210 Training loss 0.04239094629883766 Validation loss 0.04519020393490791 Accuracy 0.5446000099182129 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.734, dtype=float32), array(0.9, dtype=float32), array(0.886, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26220 Training loss 0.047778788954019547 Validation loss 0.04688674211502075 Accuracy 0.527899980545044 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.919, dtype=float32), array(0.93, dtype=float32), array(0.51, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26230 Training loss 0.043761659413576126 Validation loss 0.04532764479517937 Accuracy 0.5443000197410583 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.723, dtype=float32), array(0.922, dtype=float32), array(0.875, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26240 Training loss 0.04244815185666084 Validation loss 0.045181166380643845 Accuracy 0.5454000234603882 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.779, dtype=float32), array(0.95, dtype=float32), array(0.803, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26250 Training loss 0.04544271156191826 Validation loss 0.04524311050772667 Accuracy 0.5446000099182129 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.814, dtype=float32), array(0.947, dtype=float32), array(0.77, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26260 Training loss 0.0429677739739418 Validation loss 0.0450611412525177 Accuracy 0.5464000105857849 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.82, dtype=float32), array(0.905, dtype=float32), array(0.815, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26270 Training loss 0.04322347044944763 Validation loss 0.0454125739634037 Accuracy 0.5429999828338623 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.692, dtype=float32), array(0.91, dtype=float32), array(0.902, dtype=float32), array(0.987, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26280 Training loss 0.0435786135494709 Validation loss 0.04507557302713394 Accuracy 0.5461999773979187 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.815, dtype=float32), array(0.917, dtype=float32), array(0.812, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26290 Training loss 0.048993803560733795 Validation loss 0.045001834630966187 Accuracy 0.5472999811172485 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.784, dtype=float32), array(0.93, dtype=float32), array(0.836, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26300 Training loss 0.04170753434300423 Validation loss 0.04547276347875595 Accuracy 0.5425999760627747 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.674, dtype=float32), array(0.928, dtype=float32), array(0.907, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26310 Training loss 0.04380831867456436 Validation loss 0.04565533250570297 Accuracy 0.5407999753952026 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.701, dtype=float32), array(0.872, dtype=float32), array(0.924, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26320 Training loss 0.047389283776283264 Validation loss 0.04675925895571709 Accuracy 0.5296000242233276 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.93, dtype=float32), array(0.842, dtype=float32), array(0.621, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26330 Training loss 0.04428749158978462 Validation loss 0.04571853205561638 Accuracy 0.539900004863739 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.712, dtype=float32), array(0.874, dtype=float32), array(0.924, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.933, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26340 Training loss 0.04816370829939842 Validation loss 0.04654891416430473 Accuracy 0.5315999984741211 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.914, dtype=float32), array(0.958, dtype=float32), array(0.524, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26350 Training loss 0.04177255555987358 Validation loss 0.046283312141895294 Accuracy 0.534500002861023 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.928, dtype=float32), array(0.919, dtype=float32), array(0.584, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26360 Training loss 0.04578154534101486 Validation loss 0.04552049934864044 Accuracy 0.5419999957084656 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.725, dtype=float32), array(0.865, dtype=float32), array(0.915, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26370 Training loss 0.045998092740774155 Validation loss 0.04619506746530533 Accuracy 0.5360999703407288 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.929, dtype=float32), array(0.894, dtype=float32), array(0.634, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26380 Training loss 0.04250694811344147 Validation loss 0.04549989104270935 Accuracy 0.5426999926567078 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.731, dtype=float32), array(0.957, dtype=float32), array(0.838, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26390 Training loss 0.0436151884496212 Validation loss 0.04527824744582176 Accuracy 0.5443999767303467 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.752, dtype=float32), array(0.938, dtype=float32), array(0.848, dtype=float32), array(0.996, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26400 Training loss 0.04486707225441933 Validation loss 0.0449332632124424 Accuracy 0.5475000143051147 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.78, dtype=float32), array(0.925, dtype=float32), array(0.845, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26410 Training loss 0.045985329896211624 Validation loss 0.04577164724469185 Accuracy 0.5396000146865845 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.692, dtype=float32), array(0.935, dtype=float32), array(0.892, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.926, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26420 Training loss 0.04612989351153374 Validation loss 0.04629482328891754 Accuracy 0.5339000225067139 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.924, dtype=float32), array(0.941, dtype=float32), array(0.567, dtype=float32), array(0.993, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26430 Training loss 0.043040160089731216 Validation loss 0.04662131518125534 Accuracy 0.5306000113487244 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.929, dtype=float32), array(0.919, dtype=float32), array(0.528, dtype=float32), array(0.992, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26440 Training loss 0.04503525421023369 Validation loss 0.04502400755882263 Accuracy 0.5467000007629395 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.769, dtype=float32), array(0.911, dtype=float32), array(0.855, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26450 Training loss 0.04534260183572769 Validation loss 0.044975027441978455 Accuracy 0.5468999743461609 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.792, dtype=float32), array(0.912, dtype=float32), array(0.837, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26460 Training loss 0.04428860545158386 Validation loss 0.045119982212781906 Accuracy 0.5454999804496765 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.764, dtype=float32), array(0.918, dtype=float32), array(0.845, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26470 Training loss 0.04259404167532921 Validation loss 0.045573890209198 Accuracy 0.5418999791145325 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.673, dtype=float32), array(0.943, dtype=float32), array(0.894, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26480 Training loss 0.04531614109873772 Validation loss 0.04514555633068085 Accuracy 0.5462999939918518 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.721, dtype=float32), array(0.953, dtype=float32), array(0.873, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26490 Training loss 0.0484127551317215 Validation loss 0.045914024114608765 Accuracy 0.5385000109672546 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.864, dtype=float32), array(0.948, dtype=float32), array(0.701, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26500 Training loss 0.04309350252151489 Validation loss 0.04511139541864395 Accuracy 0.545799970626831 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.805, dtype=float32), array(0.916, dtype=float32), array(0.827, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26510 Training loss 0.04497983306646347 Validation loss 0.04527083411812782 Accuracy 0.544700026512146 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.74, dtype=float32), array(0.897, dtype=float32), array(0.896, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26520 Training loss 0.041714757680892944 Validation loss 0.0452444814145565 Accuracy 0.5443999767303467 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.864, dtype=float32), array(0.896, dtype=float32), array(0.772, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26530 Training loss 0.046749603003263474 Validation loss 0.04525778442621231 Accuracy 0.5439000129699707 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.841, dtype=float32), array(0.909, dtype=float32), array(0.781, dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26540 Training loss 0.044934652745723724 Validation loss 0.045120902359485626 Accuracy 0.545799970626831 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.758, dtype=float32), array(0.922, dtype=float32), array(0.866, dtype=float32), array(0.989, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26550 Training loss 0.04274752736091614 Validation loss 0.045561082661151886 Accuracy 0.5418000221252441 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.865, dtype=float32), array(0.875, dtype=float32), array(0.772, dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26560 Training loss 0.04413419961929321 Validation loss 0.045258622616529465 Accuracy 0.5447999835014343 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.819, dtype=float32), array(0.898, dtype=float32), array(0.823, dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26570 Training loss 0.04703446105122566 Validation loss 0.04552292823791504 Accuracy 0.5418999791145325 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.86, dtype=float32), array(0.858, dtype=float32), array(0.778, dtype=float32), array(0.994, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26580 Training loss 0.04637011140584946 Validation loss 0.04523531347513199 Accuracy 0.5444999933242798 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.846, dtype=float32), array(0.937, dtype=float32), array(0.741, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26590 Training loss 0.04479099065065384 Validation loss 0.045173291116952896 Accuracy 0.545199990272522 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.769, dtype=float32), array(0.93, dtype=float32), array(0.836, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26600 Training loss 0.04561922699213028 Validation loss 0.045163221657276154 Accuracy 0.5461000204086304 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.809, dtype=float32), array(0.878, dtype=float32), array(0.843, dtype=float32), array(0.998, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26610 Training loss 0.04664992168545723 Validation loss 0.04609464481472969 Accuracy 0.5357000231742859 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.636, dtype=float32), array(0.871, dtype=float32), array(0.93, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26620 Training loss 0.044769540429115295 Validation loss 0.04522458091378212 Accuracy 0.545199990272522 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.76, dtype=float32), array(0.883, dtype=float32), array(0.879, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26630 Training loss 0.042384956032037735 Validation loss 0.045675136148929596 Accuracy 0.541100025177002 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.667, dtype=float32), array(0.937, dtype=float32), array(0.885, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26640 Training loss 0.04381157085299492 Validation loss 0.04512372240424156 Accuracy 0.5454000234603882 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.843, dtype=float32), array(0.924, dtype=float32), array(0.754, dtype=float32), array(0.998, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26650 Training loss 0.04405418038368225 Validation loss 0.04520036280155182 Accuracy 0.5450000166893005 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.742, dtype=float32), array(0.884, dtype=float32), array(0.891, dtype=float32), array(0.994, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26660 Training loss 0.04415517300367355 Validation loss 0.045355428010225296 Accuracy 0.5430999994277954 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.758, dtype=float32), array(0.934, dtype=float32), array(0.83, dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26670 Training loss 0.045793160796165466 Validation loss 0.04581126943230629 Accuracy 0.5389999747276306 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.769, dtype=float32), array(0.967, dtype=float32), array(0.744, dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26680 Training loss 0.04353172704577446 Validation loss 0.04527430608868599 Accuracy 0.5447999835014343 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.804, dtype=float32), array(0.93, dtype=float32), array(0.809, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26690 Training loss 0.042751867324113846 Validation loss 0.04576415568590164 Accuracy 0.5393000245094299 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.671, dtype=float32), array(0.926, dtype=float32), array(0.896, dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26700 Training loss 0.04743267223238945 Validation loss 0.04686065390706062 Accuracy 0.5278000235557556 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.894, dtype=float32), array(0.958, dtype=float32), array(0.533, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26710 Training loss 0.04276307299733162 Validation loss 0.04534486308693886 Accuracy 0.5430999994277954 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.75, dtype=float32), array(0.881, dtype=float32), array(0.889, dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26720 Training loss 0.04447491466999054 Validation loss 0.045184873044490814 Accuracy 0.5442000031471252 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.821, dtype=float32), array(0.91, dtype=float32), array(0.784, dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26730 Training loss 0.046081364154815674 Validation loss 0.04559125378727913 Accuracy 0.5419999957084656 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.762, dtype=float32), array(0.918, dtype=float32), array(0.85, dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26740 Training loss 0.04217629134654999 Validation loss 0.045124027878046036 Accuracy 0.5444999933242798 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.832, dtype=float32), array(0.911, dtype=float32), array(0.781, dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26750 Training loss 0.04639580100774765 Validation loss 0.04524150490760803 Accuracy 0.5436999797821045 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.722, dtype=float32), array(0.896, dtype=float32), array(0.899, dtype=float32), array(0.988, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26760 Training loss 0.04427620396018028 Validation loss 0.04520730301737785 Accuracy 0.5443999767303467 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.831, dtype=float32), array(0.899, dtype=float32), array(0.798, dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26770 Training loss 0.04291730746626854 Validation loss 0.0452919639647007 Accuracy 0.5432999730110168 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.771, dtype=float32), array(0.855, dtype=float32), array(0.873, dtype=float32), array(0.991, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26780 Training loss 0.04396560788154602 Validation loss 0.04610326141119003 Accuracy 0.5361999869346619 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.916, dtype=float32), array(0.89, dtype=float32), array(0.621, dtype=float32), array(0.996, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26790 Training loss 0.04714237526059151 Validation loss 0.04518038034439087 Accuracy 0.54339998960495 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.772, dtype=float32), array(0.925, dtype=float32), array(0.801, dtype=float32), array(0.985, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26800 Training loss 0.04262058809399605 Validation loss 0.04502030834555626 Accuracy 0.5460000038146973 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.823, dtype=float32), array(0.895, dtype=float32), array(0.805, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26810 Training loss 0.04224153235554695 Validation loss 0.04529775306582451 Accuracy 0.5440999865531921 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.868, dtype=float32), array(0.905, dtype=float32), array(0.742, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26820 Training loss 0.04491112753748894 Validation loss 0.045822806656360626 Accuracy 0.5394999980926514 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.811, dtype=float32), array(0.806, dtype=float32), array(0.858, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26830 Training loss 0.04558344557881355 Validation loss 0.04527148976922035 Accuracy 0.5454000234603882 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.807, dtype=float32), array(0.889, dtype=float32), array(0.851, dtype=float32), array(0.997, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26840 Training loss 0.04324055835604668 Validation loss 0.04493587836623192 Accuracy 0.5468999743461609 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.803, dtype=float32), array(0.94, dtype=float32), array(0.804, dtype=float32), array(0.995, dtype=float32), array(0., dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26850 Training loss 0.04476573318243027 Validation loss 0.04540364071726799 Accuracy 0.5432999730110168 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.725, dtype=float32), array(0.952, dtype=float32), array(0.829, dtype=float32), array(0.995, dtype=float32), array(0.004, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26860 Training loss 0.0427512601017952 Validation loss 0.0442776158452034 Accuracy 0.5529999732971191 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.781, dtype=float32), array(0.933, dtype=float32), array(0.821, dtype=float32), array(0.994, dtype=float32), array(0.063, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26870 Training loss 0.04642488434910774 Validation loss 0.04519784078001976 Accuracy 0.5440999865531921 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.816, dtype=float32), array(0.943, dtype=float32), array(0.774, dtype=float32), array(0.984, dtype=float32), array(0.001, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26880 Training loss 0.042917054146528244 Validation loss 0.04511679708957672 Accuracy 0.5453000068664551 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.684, dtype=float32), array(0.898, dtype=float32), array(0.914, dtype=float32), array(0.97, dtype=float32), array(0.048, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26890 Training loss 0.041788600385189056 Validation loss 0.04439064860343933 Accuracy 0.5511999726295471 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.87, dtype=float32), array(0.892, dtype=float32), array(0.707, dtype=float32), array(0.989, dtype=float32), array(0.124, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26900 Training loss 0.04231610149145126 Validation loss 0.044496089220047 Accuracy 0.550000011920929 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.791, dtype=float32), array(0.883, dtype=float32), array(0.862, dtype=float32), array(0.992, dtype=float32), array(0.037, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26910 Training loss 0.04321205988526344 Validation loss 0.045506566762924194 Accuracy 0.5408999919891357 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.775, dtype=float32), array(0.817, dtype=float32), array(0.878, dtype=float32), array(0.997, dtype=float32), array(0.003, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26920 Training loss 0.03854832053184509 Validation loss 0.04416544362902641 Accuracy 0.5547999739646912 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.814, dtype=float32), array(0.886, dtype=float32), array(0.821, dtype=float32), array(0.998, dtype=float32), array(0.1, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26930 Training loss 0.039440546184778214 Validation loss 0.04424954950809479 Accuracy 0.553600013256073 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.844, dtype=float32), array(0.86, dtype=float32), array(0.764, dtype=float32), array(0.997, dtype=float32), array(0.15, dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26940 Training loss 0.04647887125611305 Validation loss 0.04389190301299095 Accuracy 0.5568000078201294 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.746, dtype=float32), array(0.89, dtype=float32), array(0.882, dtype=float32), array(0.998, dtype=float32), array(0.12, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26950 Training loss 0.0431034192442894 Validation loss 0.0432901456952095 Accuracy 0.5631999969482422 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.782, dtype=float32), array(0.891, dtype=float32), array(0.812, dtype=float32), array(0.996, dtype=float32), array(0.228, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26960 Training loss 0.047444697469472885 Validation loss 0.043434493243694305 Accuracy 0.5616000294685364 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.813, dtype=float32), array(0.898, dtype=float32), array(0.801, dtype=float32), array(0.994, dtype=float32), array(0.191, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26970 Training loss 0.04549746587872505 Validation loss 0.043990522623062134 Accuracy 0.555400013923645 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.638, dtype=float32), array(0.822, dtype=float32), array(0.898, dtype=float32), array(0.983, dtype=float32), array(0.299, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26980 Training loss 0.040822144597768784 Validation loss 0.0435064435005188 Accuracy 0.5609999895095825 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.853, dtype=float32), array(0.902, dtype=float32), array(0.718, dtype=float32), array(0.994, dtype=float32), array(0.212, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 26990 Training loss 0.043577734380960464 Validation loss 0.04308813437819481 Accuracy 0.566100001335144 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.797, dtype=float32), array(0.864, dtype=float32), array(0.8, dtype=float32), array(0.997, dtype=float32), array(0.277, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27000 Training loss 0.0402672216296196 Validation loss 0.04320080578327179 Accuracy 0.5644999742507935 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.787, dtype=float32), array(0.873, dtype=float32), array(0.849, dtype=float32), array(0.997, dtype=float32), array(0.212, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27010 Training loss 0.04250599816441536 Validation loss 0.043499916791915894 Accuracy 0.5611000061035156 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.749, dtype=float32), array(0.866, dtype=float32), array(0.894, dtype=float32), array(0.983, dtype=float32), array(0.177, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27020 Training loss 0.042600564658641815 Validation loss 0.04270569980144501 Accuracy 0.5677000284194946 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.714, dtype=float32), array(0.875, dtype=float32), array(0.808, dtype=float32), array(0.984, dtype=float32), array(0.363, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27030 Training loss 0.04586797207593918 Validation loss 0.043638721108436584 Accuracy 0.5593000054359436 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.825, dtype=float32), array(0.941, dtype=float32), array(0.756, dtype=float32), array(0.987, dtype=float32), array(0.151, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27040 Training loss 0.037372786551713943 Validation loss 0.04275387153029442 Accuracy 0.567300021648407 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.816, dtype=float32), array(0.931, dtype=float32), array(0.545, dtype=float32), array(0.993, dtype=float32), array(0.461, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27050 Training loss 0.041538938879966736 Validation loss 0.04463701695203781 Accuracy 0.5498999953269958 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.744, dtype=float32), array(0.845, dtype=float32), array(0.307, dtype=float32), array(0.996, dtype=float32), array(0.688, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27060 Training loss 0.03950938582420349 Validation loss 0.042768966406583786 Accuracy 0.5683000087738037 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.854, dtype=float32), array(0.89, dtype=float32), array(0.496, dtype=float32), array(0.997, dtype=float32), array(0.526, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27070 Training loss 0.03996124118566513 Validation loss 0.043478768318891525 Accuracy 0.5612999796867371 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.856, dtype=float32), array(0.921, dtype=float32), array(0.355, dtype=float32), array(0.996, dtype=float32), array(0.572, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27080 Training loss 0.04038064554333687 Validation loss 0.04151204228401184 Accuracy 0.5806999802589417 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.747, dtype=float32), array(0.869, dtype=float32), array(0.692, dtype=float32), array(0.996, dtype=float32), array(0.581, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27090 Training loss 0.04263269156217575 Validation loss 0.043344829231500626 Accuracy 0.5640000104904175 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.909, dtype=float32), array(0.909, dtype=float32), array(0.527, dtype=float32), array(0.998, dtype=float32), array(0.375, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27100 Training loss 0.04146243631839752 Validation loss 0.04168828949332237 Accuracy 0.5802000164985657 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.681, dtype=float32), array(0.85, dtype=float32), array(0.847, dtype=float32), array(0.998, dtype=float32), array(0.488, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27110 Training loss 0.039183516055345535 Validation loss 0.0415361225605011 Accuracy 0.5809999704360962 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.69, dtype=float32), array(0.89, dtype=float32), array(0.817, dtype=float32), array(0.994, dtype=float32), array(0.476, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27120 Training loss 0.0416240394115448 Validation loss 0.04323069378733635 Accuracy 0.5637000203132629 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.58, dtype=float32), array(0.862, dtype=float32), array(0.919, dtype=float32), array(0.985, dtype=float32), array(0.358, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27130 Training loss 0.047000691294670105 Validation loss 0.047097377479076385 Accuracy 0.5246999859809875 Accuracies by class [array(0., dtype=float32), array(0.939, dtype=float32), array(0.424, dtype=float32), array(0.851, dtype=float32), array(0.283, dtype=float32), array(0.995, dtype=float32), array(0.801, dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27140 Training loss 0.036491308361291885 Validation loss 0.04138372093439102 Accuracy 0.5835999846458435 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.829, dtype=float32), array(0.892, dtype=float32), array(0.722, dtype=float32), array(0.998, dtype=float32), array(0.466, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27150 Training loss 0.042061999440193176 Validation loss 0.040983520448207855 Accuracy 0.5871000289916992 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.816, dtype=float32), array(0.91, dtype=float32), array(0.703, dtype=float32), array(0.997, dtype=float32), array(0.506, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27160 Training loss 0.04320038855075836 Validation loss 0.04143557697534561 Accuracy 0.58160001039505 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.806, dtype=float32), array(0.882, dtype=float32), array(0.787, dtype=float32), array(0.996, dtype=float32), array(0.406, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27170 Training loss 0.04127455875277519 Validation loss 0.04233424365520477 Accuracy 0.5738999843597412 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.687, dtype=float32), array(0.873, dtype=float32), array(0.598, dtype=float32), array(0.998, dtype=float32), array(0.668, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27180 Training loss 0.04036874324083328 Validation loss 0.04230542480945587 Accuracy 0.5734000205993652 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.806, dtype=float32), array(0.869, dtype=float32), array(0.839, dtype=float32), array(0.994, dtype=float32), array(0.302, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27190 Training loss 0.04280278459191322 Validation loss 0.04101767763495445 Accuracy 0.5873000025749207 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.828, dtype=float32), array(0.907, dtype=float32), array(0.676, dtype=float32), array(0.998, dtype=float32), array(0.537, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27200 Training loss 0.04380982369184494 Validation loss 0.04384186118841171 Accuracy 0.5580000281333923 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.336, dtype=float32), array(0.905, dtype=float32), array(0.727, dtype=float32), array(0.992, dtype=float32), array(0.718, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27210 Training loss 0.037009403109550476 Validation loss 0.041372403502464294 Accuracy 0.5828999876976013 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.696, dtype=float32), array(0.896, dtype=float32), array(0.723, dtype=float32), array(0.994, dtype=float32), array(0.612, dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27220 Training loss 0.03951844200491905 Validation loss 0.04147736355662346 Accuracy 0.582099974155426 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.699, dtype=float32), array(0.812, dtype=float32), array(0.848, dtype=float32), array(0.996, dtype=float32), array(0.553, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27230 Training loss 0.039112091064453125 Validation loss 0.044183507561683655 Accuracy 0.5540000200271606 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.933, dtype=float32), array(0.833, dtype=float32), array(0.405, dtype=float32), array(0.996, dtype=float32), array(0.464, dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27240 Training loss 0.04418473318219185 Validation loss 0.04148639738559723 Accuracy 0.5812000036239624 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.844, dtype=float32), array(0.896, dtype=float32), array(0.759, dtype=float32), array(0.994, dtype=float32), array(0.394, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27250 Training loss 0.03847847133874893 Validation loss 0.04079544544219971 Accuracy 0.5885999798774719 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.697, dtype=float32), array(0.896, dtype=float32), array(0.814, dtype=float32), array(0.985, dtype=float32), array(0.561, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27260 Training loss 0.039405662566423416 Validation loss 0.04329593852162361 Accuracy 0.5627999901771545 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.487, dtype=float32), array(0.904, dtype=float32), array(0.604, dtype=float32), array(0.996, dtype=float32), array(0.733, dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27270 Training loss 0.03970445692539215 Validation loss 0.04299408569931984 Accuracy 0.5665000081062317 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.441, dtype=float32), array(0.883, dtype=float32), array(0.902, dtype=float32), array(0.985, dtype=float32), array(0.533, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27280 Training loss 0.040997277945280075 Validation loss 0.041832152754068375 Accuracy 0.5777999758720398 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.826, dtype=float32), array(0.939, dtype=float32), array(0.569, dtype=float32), array(0.98, dtype=float32), array(0.538, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27290 Training loss 0.047094013541936874 Validation loss 0.04555291682481766 Accuracy 0.541100025177002 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.946, dtype=float32), array(0.932, dtype=float32), array(0.266, dtype=float32), array(0.997, dtype=float32), array(0.358, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27300 Training loss 0.04398262873291969 Validation loss 0.042077455669641495 Accuracy 0.5763000249862671 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.771, dtype=float32), array(0.937, dtype=float32), array(0.601, dtype=float32), array(0.965, dtype=float32), array(0.559, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27310 Training loss 0.04107275977730751 Validation loss 0.042084578424692154 Accuracy 0.5759999752044678 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.726, dtype=float32), array(0.93, dtype=float32), array(0.859, dtype=float32), array(0.941, dtype=float32), array(0.383, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27320 Training loss 0.03983420878648758 Validation loss 0.041259121149778366 Accuracy 0.5835000276565552 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.716, dtype=float32), array(0.92, dtype=float32), array(0.86, dtype=float32), array(0.986, dtype=float32), array(0.439, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27330 Training loss 0.03811553120613098 Validation loss 0.04170742258429527 Accuracy 0.5788999795913696 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.73, dtype=float32), array(0.902, dtype=float32), array(0.874, dtype=float32), array(0.982, dtype=float32), array(0.367, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27340 Training loss 0.043217193335294724 Validation loss 0.042408574372529984 Accuracy 0.5720000267028809 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.857, dtype=float32), array(0.951, dtype=float32), array(0.677, dtype=float32), array(0.995, dtype=float32), array(0.32, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27350 Training loss 0.03860240802168846 Validation loss 0.04170432686805725 Accuracy 0.5791000127792358 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.672, dtype=float32), array(0.92, dtype=float32), array(0.886, dtype=float32), array(0.995, dtype=float32), array(0.388, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27360 Training loss 0.04178629070520401 Validation loss 0.04168416187167168 Accuracy 0.5795000195503235 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.659, dtype=float32), array(0.961, dtype=float32), array(0.66, dtype=float32), array(0.991, dtype=float32), array(0.61, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27370 Training loss 0.039577849209308624 Validation loss 0.041766367852687836 Accuracy 0.579200029373169 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.706, dtype=float32), array(0.904, dtype=float32), array(0.872, dtype=float32), array(0.974, dtype=float32), array(0.389, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27380 Training loss 0.040945105254650116 Validation loss 0.041735462844371796 Accuracy 0.5794000029563904 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.852, dtype=float32), array(0.929, dtype=float32), array(0.518, dtype=float32), array(0.998, dtype=float32), array(0.562, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27390 Training loss 0.03666336461901665 Validation loss 0.04119959846138954 Accuracy 0.5842000246047974 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.755, dtype=float32), array(0.931, dtype=float32), array(0.821, dtype=float32), array(0.993, dtype=float32), array(0.416, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27400 Training loss 0.04450590908527374 Validation loss 0.0414007231593132 Accuracy 0.5838000178337097 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.795, dtype=float32), array(0.883, dtype=float32), array(0.823, dtype=float32), array(0.999, dtype=float32), array(0.434, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27410 Training loss 0.040028441697359085 Validation loss 0.040705606341362 Accuracy 0.5900999903678894 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.822, dtype=float32), array(0.877, dtype=float32), array(0.732, dtype=float32), array(0.997, dtype=float32), array(0.539, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27420 Training loss 0.03960225358605385 Validation loss 0.04185451567173004 Accuracy 0.5776000022888184 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.752, dtype=float32), array(0.852, dtype=float32), array(0.879, dtype=float32), array(0.995, dtype=float32), array(0.374, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27430 Training loss 0.04139813408255577 Validation loss 0.04133147373795509 Accuracy 0.583899974822998 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.793, dtype=float32), array(0.871, dtype=float32), array(0.844, dtype=float32), array(0.979, dtype=float32), array(0.42, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27440 Training loss 0.038024913519620895 Validation loss 0.04149017482995987 Accuracy 0.5823000073432922 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.584, dtype=float32), array(0.891, dtype=float32), array(0.782, dtype=float32), array(0.995, dtype=float32), array(0.663, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27450 Training loss 0.04204508289694786 Validation loss 0.04067955166101456 Accuracy 0.5903000235557556 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.779, dtype=float32), array(0.922, dtype=float32), array(0.747, dtype=float32), array(0.998, dtype=float32), array(0.548, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27460 Training loss 0.03826026991009712 Validation loss 0.040989093482494354 Accuracy 0.5873000025749207 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.745, dtype=float32), array(0.887, dtype=float32), array(0.856, dtype=float32), array(0.999, dtype=float32), array(0.46, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27470 Training loss 0.04116559401154518 Validation loss 0.04305611923336983 Accuracy 0.5662999749183655 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.657, dtype=float32), array(0.842, dtype=float32), array(0.491, dtype=float32), array(0.998, dtype=float32), array(0.778, dtype=float32), array(0., dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27480 Training loss 0.04339136928319931 Validation loss 0.0419524721801281 Accuracy 0.5777000188827515 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.587, dtype=float32), array(0.887, dtype=float32), array(0.684, dtype=float32), array(0.998, dtype=float32), array(0.745, dtype=float32), array(0., dtype=float32), array(0.92, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27490 Training loss 0.04169129580259323 Validation loss 0.04122500866651535 Accuracy 0.5838000178337097 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.677, dtype=float32), array(0.864, dtype=float32), array(0.885, dtype=float32), array(0.996, dtype=float32), array(0.498, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27500 Training loss 0.03863715007901192 Validation loss 0.04058804735541344 Accuracy 0.5910000205039978 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.671, dtype=float32), array(0.859, dtype=float32), array(0.857, dtype=float32), array(0.987, dtype=float32), array(0.594, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27510 Training loss 0.04153113439679146 Validation loss 0.044810112565755844 Accuracy 0.5476999878883362 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.645, dtype=float32), array(0.893, dtype=float32), array(0.267, dtype=float32), array(0.996, dtype=float32), array(0.775, dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27520 Training loss 0.040729034692049026 Validation loss 0.04099951311945915 Accuracy 0.5856000185012817 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.856, dtype=float32), array(0.849, dtype=float32), array(0.785, dtype=float32), array(0.995, dtype=float32), array(0.431, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27530 Training loss 0.039734359830617905 Validation loss 0.04098508134484291 Accuracy 0.5859000086784363 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.799, dtype=float32), array(0.86, dtype=float32), array(0.827, dtype=float32), array(0.978, dtype=float32), array(0.45, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27540 Training loss 0.04019305109977722 Validation loss 0.040978603065013885 Accuracy 0.586899995803833 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.68, dtype=float32), array(0.77, dtype=float32), array(0.828, dtype=float32), array(0.99, dtype=float32), array(0.666, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27550 Training loss 0.04087720066308975 Validation loss 0.040390193462371826 Accuracy 0.5925999879837036 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.732, dtype=float32), array(0.857, dtype=float32), array(0.842, dtype=float32), array(0.986, dtype=float32), array(0.567, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27560 Training loss 0.03625034540891647 Validation loss 0.04100215435028076 Accuracy 0.5855000019073486 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.871, dtype=float32), array(0.868, dtype=float32), array(0.736, dtype=float32), array(0.993, dtype=float32), array(0.444, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27570 Training loss 0.040994659066200256 Validation loss 0.043055977672338486 Accuracy 0.5648000240325928 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.817, dtype=float32), array(0.893, dtype=float32), array(0.815, dtype=float32), array(0.994, dtype=float32), array(0.185, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27580 Training loss 0.04059735685586929 Validation loss 0.042471129447221756 Accuracy 0.5712000131607056 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.605, dtype=float32), array(0.855, dtype=float32), array(0.56, dtype=float32), array(0.992, dtype=float32), array(0.781, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27590 Training loss 0.04096030071377754 Validation loss 0.04160225763916969 Accuracy 0.5795999765396118 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.735, dtype=float32), array(0.834, dtype=float32), array(0.882, dtype=float32), array(0.978, dtype=float32), array(0.415, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27600 Training loss 0.036452874541282654 Validation loss 0.040430232882499695 Accuracy 0.5920000076293945 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.817, dtype=float32), array(0.922, dtype=float32), array(0.77, dtype=float32), array(0.994, dtype=float32), array(0.48, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27610 Training loss 0.0405299998819828 Validation loss 0.04195662960410118 Accuracy 0.5767999887466431 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.675, dtype=float32), array(0.808, dtype=float32), array(0.611, dtype=float32), array(0.988, dtype=float32), array(0.78, dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27620 Training loss 0.03841331973671913 Validation loss 0.04157392680644989 Accuracy 0.58160001039505 Accuracies by class [array(0., dtype=float32), array(0.928, dtype=float32), array(0.636, dtype=float32), array(0.914, dtype=float32), array(0.677, dtype=float32), array(0.971, dtype=float32), array(0.713, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27630 Training loss 0.04022231325507164 Validation loss 0.0412883386015892 Accuracy 0.5837000012397766 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.766, dtype=float32), array(0.917, dtype=float32), array(0.812, dtype=float32), array(0.974, dtype=float32), array(0.444, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27640 Training loss 0.0410943403840065 Validation loss 0.04266844689846039 Accuracy 0.5695000290870667 Accuracies by class [array(0., dtype=float32), array(0.934, dtype=float32), array(0.537, dtype=float32), array(0.887, dtype=float32), array(0.626, dtype=float32), array(0.985, dtype=float32), array(0.781, dtype=float32), array(0., dtype=float32), array(0.945, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27650 Training loss 0.039938997477293015 Validation loss 0.04202806204557419 Accuracy 0.5766000151634216 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.714, dtype=float32), array(0.893, dtype=float32), array(0.536, dtype=float32), array(0.976, dtype=float32), array(0.742, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27660 Training loss 0.035715509206056595 Validation loss 0.04028419777750969 Accuracy 0.5932000279426575 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.712, dtype=float32), array(0.868, dtype=float32), array(0.856, dtype=float32), array(0.994, dtype=float32), array(0.561, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27670 Training loss 0.03975759446620941 Validation loss 0.04039746895432472 Accuracy 0.5938000082969666 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.82, dtype=float32), array(0.857, dtype=float32), array(0.709, dtype=float32), array(0.999, dtype=float32), array(0.64, dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27680 Training loss 0.03872441500425339 Validation loss 0.04055027663707733 Accuracy 0.5906999707221985 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.751, dtype=float32), array(0.826, dtype=float32), array(0.676, dtype=float32), array(0.994, dtype=float32), array(0.72, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27690 Training loss 0.039117977023124695 Validation loss 0.040198709815740585 Accuracy 0.5942000150680542 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.682, dtype=float32), array(0.841, dtype=float32), array(0.858, dtype=float32), array(0.996, dtype=float32), array(0.625, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27700 Training loss 0.0408242829144001 Validation loss 0.040172357112169266 Accuracy 0.5946000218391418 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.841, dtype=float32), array(0.883, dtype=float32), array(0.765, dtype=float32), array(0.994, dtype=float32), array(0.533, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27710 Training loss 0.039410997182130814 Validation loss 0.04062409698963165 Accuracy 0.5906000137329102 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.751, dtype=float32), array(0.872, dtype=float32), array(0.851, dtype=float32), array(0.998, dtype=float32), array(0.526, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27720 Training loss 0.044182006269693375 Validation loss 0.0401674248278141 Accuracy 0.5947999954223633 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.794, dtype=float32), array(0.921, dtype=float32), array(0.74, dtype=float32), array(0.976, dtype=float32), array(0.6, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27730 Training loss 0.038427334278821945 Validation loss 0.04233555495738983 Accuracy 0.5738000273704529 Accuracies by class [array(0., dtype=float32), array(0.938, dtype=float32), array(0.513, dtype=float32), array(0.925, dtype=float32), array(0.875, dtype=float32), array(0.937, dtype=float32), array(0.573, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27740 Training loss 0.03821360692381859 Validation loss 0.040337055921554565 Accuracy 0.5921000242233276 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.826, dtype=float32), array(0.913, dtype=float32), array(0.713, dtype=float32), array(0.991, dtype=float32), array(0.551, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27750 Training loss 0.037595536559820175 Validation loss 0.04089184105396271 Accuracy 0.586899995803833 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.647, dtype=float32), array(0.85, dtype=float32), array(0.884, dtype=float32), array(0.967, dtype=float32), array(0.592, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27760 Training loss 0.03982510790228844 Validation loss 0.041398756206035614 Accuracy 0.5817000269889832 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.551, dtype=float32), array(0.85, dtype=float32), array(0.747, dtype=float32), array(0.988, dtype=float32), array(0.762, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27770 Training loss 0.03946814313530922 Validation loss 0.04034080356359482 Accuracy 0.5947999954223633 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.803, dtype=float32), array(0.906, dtype=float32), array(0.75, dtype=float32), array(0.999, dtype=float32), array(0.573, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27780 Training loss 0.03834361955523491 Validation loss 0.04096722975373268 Accuracy 0.588100016117096 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.75, dtype=float32), array(0.916, dtype=float32), array(0.819, dtype=float32), array(0.999, dtype=float32), array(0.469, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27790 Training loss 0.04309897869825363 Validation loss 0.040438488125801086 Accuracy 0.5925999879837036 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.822, dtype=float32), array(0.91, dtype=float32), array(0.612, dtype=float32), array(0.999, dtype=float32), array(0.653, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27800 Training loss 0.03778309002518654 Validation loss 0.03981731832027435 Accuracy 0.5986999869346619 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.734, dtype=float32), array(0.901, dtype=float32), array(0.8, dtype=float32), array(0.999, dtype=float32), array(0.617, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27810 Training loss 0.03986844792962074 Validation loss 0.0404580794274807 Accuracy 0.5928000211715698 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.771, dtype=float32), array(0.931, dtype=float32), array(0.766, dtype=float32), array(0.999, dtype=float32), array(0.527, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27820 Training loss 0.0417274534702301 Validation loss 0.04108092188835144 Accuracy 0.5873000025749207 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.576, dtype=float32), array(0.852, dtype=float32), array(0.846, dtype=float32), array(0.999, dtype=float32), array(0.666, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27830 Training loss 0.0409175269305706 Validation loss 0.04052429273724556 Accuracy 0.5924000144004822 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.664, dtype=float32), array(0.892, dtype=float32), array(0.846, dtype=float32), array(0.999, dtype=float32), array(0.606, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27840 Training loss 0.0419318787753582 Validation loss 0.04008011892437935 Accuracy 0.5956000089645386 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.685, dtype=float32), array(0.837, dtype=float32), array(0.813, dtype=float32), array(0.995, dtype=float32), array(0.681, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27850 Training loss 0.03920774161815643 Validation loss 0.04012677073478699 Accuracy 0.5950000286102295 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.842, dtype=float32), array(0.84, dtype=float32), array(0.757, dtype=float32), array(0.998, dtype=float32), array(0.584, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27860 Training loss 0.039152756333351135 Validation loss 0.041467417031526566 Accuracy 0.5803999900817871 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.635, dtype=float32), array(0.799, dtype=float32), array(0.704, dtype=float32), array(0.978, dtype=float32), array(0.774, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27870 Training loss 0.04056774824857712 Validation loss 0.0413687564432621 Accuracy 0.582099974155426 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.741, dtype=float32), array(0.826, dtype=float32), array(0.608, dtype=float32), array(0.986, dtype=float32), array(0.746, dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27880 Training loss 0.03870124742388725 Validation loss 0.040126241743564606 Accuracy 0.5961999893188477 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.705, dtype=float32), array(0.852, dtype=float32), array(0.867, dtype=float32), array(0.999, dtype=float32), array(0.602, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27890 Training loss 0.0391269326210022 Validation loss 0.0405980683863163 Accuracy 0.5900999903678894 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.806, dtype=float32), array(0.852, dtype=float32), array(0.83, dtype=float32), array(0.996, dtype=float32), array(0.492, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27900 Training loss 0.03891925886273384 Validation loss 0.040619462728500366 Accuracy 0.5896999835968018 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.691, dtype=float32), array(0.899, dtype=float32), array(0.703, dtype=float32), array(0.975, dtype=float32), array(0.709, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27910 Training loss 0.03942016139626503 Validation loss 0.04107297956943512 Accuracy 0.5859000086784363 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.741, dtype=float32), array(0.914, dtype=float32), array(0.63, dtype=float32), array(0.951, dtype=float32), array(0.697, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27920 Training loss 0.0382353700697422 Validation loss 0.040203578770160675 Accuracy 0.5939000248908997 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.76, dtype=float32), array(0.812, dtype=float32), array(0.84, dtype=float32), array(0.967, dtype=float32), array(0.616, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27930 Training loss 0.03924635425209999 Validation loss 0.041870489716529846 Accuracy 0.5788000226020813 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.825, dtype=float32), array(0.882, dtype=float32), array(0.459, dtype=float32), array(0.999, dtype=float32), array(0.714, dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27940 Training loss 0.0361933633685112 Validation loss 0.039937589317560196 Accuracy 0.5968999862670898 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.8, dtype=float32), array(0.883, dtype=float32), array(0.702, dtype=float32), array(0.992, dtype=float32), array(0.669, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27950 Training loss 0.04449835792183876 Validation loss 0.041023701429367065 Accuracy 0.5870000123977661 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.662, dtype=float32), array(0.844, dtype=float32), array(0.914, dtype=float32), array(0.998, dtype=float32), array(0.505, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27960 Training loss 0.04020516574382782 Validation loss 0.042148202657699585 Accuracy 0.5751000046730042 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.639, dtype=float32), array(0.828, dtype=float32), array(0.937, dtype=float32), array(0.998, dtype=float32), array(0.425, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27970 Training loss 0.037597618997097015 Validation loss 0.03999654948711395 Accuracy 0.5968000292778015 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.712, dtype=float32), array(0.864, dtype=float32), array(0.86, dtype=float32), array(0.991, dtype=float32), array(0.597, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27980 Training loss 0.03892478346824646 Validation loss 0.03956589475274086 Accuracy 0.6010000109672546 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.794, dtype=float32), array(0.893, dtype=float32), array(0.771, dtype=float32), array(0.998, dtype=float32), array(0.613, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 27990 Training loss 0.038302529603242874 Validation loss 0.03988343104720116 Accuracy 0.5976999998092651 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.742, dtype=float32), array(0.909, dtype=float32), array(0.717, dtype=float32), array(0.993, dtype=float32), array(0.678, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28000 Training loss 0.0361972413957119 Validation loss 0.0415877103805542 Accuracy 0.5810999870300293 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.452, dtype=float32), array(0.857, dtype=float32), array(0.881, dtype=float32), array(0.99, dtype=float32), array(0.698, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28010 Training loss 0.040654927492141724 Validation loss 0.041099291294813156 Accuracy 0.585099995136261 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.681, dtype=float32), array(0.865, dtype=float32), array(0.892, dtype=float32), array(0.995, dtype=float32), array(0.478, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28020 Training loss 0.03559539467096329 Validation loss 0.03995141014456749 Accuracy 0.5972999930381775 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.774, dtype=float32), array(0.891, dtype=float32), array(0.776, dtype=float32), array(0.999, dtype=float32), array(0.602, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28030 Training loss 0.038377076387405396 Validation loss 0.040064629167318344 Accuracy 0.5964000225067139 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.862, dtype=float32), array(0.872, dtype=float32), array(0.721, dtype=float32), array(0.995, dtype=float32), array(0.573, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28040 Training loss 0.03809567168354988 Validation loss 0.040028758347034454 Accuracy 0.5963000059127808 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.841, dtype=float32), array(0.9, dtype=float32), array(0.765, dtype=float32), array(0.998, dtype=float32), array(0.53, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28050 Training loss 0.0373104102909565 Validation loss 0.03958528861403465 Accuracy 0.6011999845504761 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.841, dtype=float32), array(0.89, dtype=float32), array(0.723, dtype=float32), array(0.998, dtype=float32), array(0.622, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28060 Training loss 0.04114300757646561 Validation loss 0.041429921984672546 Accuracy 0.5831999778747559 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.904, dtype=float32), array(0.869, dtype=float32), array(0.555, dtype=float32), array(0.999, dtype=float32), array(0.573, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28070 Training loss 0.04058544337749481 Validation loss 0.04027293622493744 Accuracy 0.593500018119812 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.602, dtype=float32), array(0.89, dtype=float32), array(0.813, dtype=float32), array(0.997, dtype=float32), array(0.699, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28080 Training loss 0.037539634853601456 Validation loss 0.03992637246847153 Accuracy 0.5971999764442444 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.691, dtype=float32), array(0.898, dtype=float32), array(0.772, dtype=float32), array(0.988, dtype=float32), array(0.685, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28090 Training loss 0.037440892308950424 Validation loss 0.03988659009337425 Accuracy 0.5972999930381775 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.681, dtype=float32), array(0.888, dtype=float32), array(0.857, dtype=float32), array(0.996, dtype=float32), array(0.61, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28100 Training loss 0.03859575837850571 Validation loss 0.03971629962325096 Accuracy 0.5990999937057495 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.835, dtype=float32), array(0.912, dtype=float32), array(0.677, dtype=float32), array(0.992, dtype=float32), array(0.637, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28110 Training loss 0.03826365992426872 Validation loss 0.03964897245168686 Accuracy 0.5996999740600586 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.832, dtype=float32), array(0.921, dtype=float32), array(0.714, dtype=float32), array(0.997, dtype=float32), array(0.605, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28120 Training loss 0.04483404383063316 Validation loss 0.03979484736919403 Accuracy 0.5985000133514404 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.673, dtype=float32), array(0.897, dtype=float32), array(0.802, dtype=float32), array(0.999, dtype=float32), array(0.687, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28130 Training loss 0.04128120094537735 Validation loss 0.04036476090550423 Accuracy 0.5936999917030334 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.769, dtype=float32), array(0.853, dtype=float32), array(0.847, dtype=float32), array(0.999, dtype=float32), array(0.535, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28140 Training loss 0.036567896604537964 Validation loss 0.04041503742337227 Accuracy 0.5927000045776367 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.647, dtype=float32), array(0.882, dtype=float32), array(0.772, dtype=float32), array(0.989, dtype=float32), array(0.723, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28150 Training loss 0.03691555932164192 Validation loss 0.04054109379649162 Accuracy 0.5917999744415283 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.657, dtype=float32), array(0.892, dtype=float32), array(0.729, dtype=float32), array(0.988, dtype=float32), array(0.744, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28160 Training loss 0.04122689738869667 Validation loss 0.0393303819000721 Accuracy 0.6032000184059143 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.824, dtype=float32), array(0.897, dtype=float32), array(0.762, dtype=float32), array(0.999, dtype=float32), array(0.618, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28170 Training loss 0.03713778033852577 Validation loss 0.03972984477877617 Accuracy 0.5983999967575073 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.793, dtype=float32), array(0.929, dtype=float32), array(0.782, dtype=float32), array(0.997, dtype=float32), array(0.543, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28180 Training loss 0.039387449622154236 Validation loss 0.040588539093732834 Accuracy 0.5906000137329102 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.797, dtype=float32), array(0.919, dtype=float32), array(0.825, dtype=float32), array(0.999, dtype=float32), array(0.444, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28190 Training loss 0.03588677570223808 Validation loss 0.04078732803463936 Accuracy 0.5892999768257141 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.566, dtype=float32), array(0.878, dtype=float32), array(0.82, dtype=float32), array(0.999, dtype=float32), array(0.725, dtype=float32), array(0., dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28200 Training loss 0.039136819541454315 Validation loss 0.04025263339281082 Accuracy 0.5946999788284302 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.753, dtype=float32), array(0.867, dtype=float32), array(0.69, dtype=float32), array(0.999, dtype=float32), array(0.737, dtype=float32), array(0., dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28210 Training loss 0.03865031898021698 Validation loss 0.04020032659173012 Accuracy 0.5956000089645386 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.626, dtype=float32), array(0.884, dtype=float32), array(0.849, dtype=float32), array(0.999, dtype=float32), array(0.691, dtype=float32), array(0., dtype=float32), array(0.941, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28220 Training loss 0.036513276398181915 Validation loss 0.039504166692495346 Accuracy 0.6018000245094299 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.824, dtype=float32), array(0.909, dtype=float32), array(0.744, dtype=float32), array(0.998, dtype=float32), array(0.616, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28230 Training loss 0.03714212402701378 Validation loss 0.040133602917194366 Accuracy 0.5950000286102295 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.744, dtype=float32), array(0.895, dtype=float32), array(0.855, dtype=float32), array(0.994, dtype=float32), array(0.536, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28240 Training loss 0.04062855616211891 Validation loss 0.04003205522894859 Accuracy 0.5960999727249146 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.734, dtype=float32), array(0.923, dtype=float32), array(0.825, dtype=float32), array(0.994, dtype=float32), array(0.563, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28250 Training loss 0.039890676736831665 Validation loss 0.039659272879362106 Accuracy 0.5997999906539917 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.79, dtype=float32), array(0.918, dtype=float32), array(0.799, dtype=float32), array(0.993, dtype=float32), array(0.559, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28260 Training loss 0.03706340491771698 Validation loss 0.03945216163992882 Accuracy 0.6014000177383423 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.769, dtype=float32), array(0.866, dtype=float32), array(0.824, dtype=float32), array(0.994, dtype=float32), array(0.621, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28270 Training loss 0.03941915184259415 Validation loss 0.040119994431734085 Accuracy 0.5953999757766724 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.764, dtype=float32), array(0.897, dtype=float32), array(0.828, dtype=float32), array(0.999, dtype=float32), array(0.537, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28280 Training loss 0.03853892534971237 Validation loss 0.040129952132701874 Accuracy 0.5956000089645386 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.701, dtype=float32), array(0.894, dtype=float32), array(0.742, dtype=float32), array(0.999, dtype=float32), array(0.712, dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28290 Training loss 0.038529086858034134 Validation loss 0.04034799709916115 Accuracy 0.5936999917030334 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.786, dtype=float32), array(0.916, dtype=float32), array(0.655, dtype=float32), array(0.999, dtype=float32), array(0.658, dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28300 Training loss 0.03681650012731552 Validation loss 0.04014473780989647 Accuracy 0.5964999794960022 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.816, dtype=float32), array(0.88, dtype=float32), array(0.647, dtype=float32), array(0.999, dtype=float32), array(0.701, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28310 Training loss 0.03910711035132408 Validation loss 0.039611198008060455 Accuracy 0.6017000079154968 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.82, dtype=float32), array(0.862, dtype=float32), array(0.775, dtype=float32), array(0.999, dtype=float32), array(0.626, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28320 Training loss 0.03654864802956581 Validation loss 0.039494287222623825 Accuracy 0.6011000275611877 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.833, dtype=float32), array(0.874, dtype=float32), array(0.708, dtype=float32), array(0.999, dtype=float32), array(0.658, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28330 Training loss 0.04037753492593765 Validation loss 0.04024544358253479 Accuracy 0.593999981880188 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.672, dtype=float32), array(0.897, dtype=float32), array(0.891, dtype=float32), array(0.998, dtype=float32), array(0.543, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28340 Training loss 0.039744142442941666 Validation loss 0.040063392370939255 Accuracy 0.595300018787384 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.848, dtype=float32), array(0.913, dtype=float32), array(0.772, dtype=float32), array(0.997, dtype=float32), array(0.491, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28350 Training loss 0.041018951684236526 Validation loss 0.03960097208619118 Accuracy 0.6008999943733215 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.775, dtype=float32), array(0.845, dtype=float32), array(0.778, dtype=float32), array(0.999, dtype=float32), array(0.68, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28360 Training loss 0.037731800228357315 Validation loss 0.04051663726568222 Accuracy 0.5916000008583069 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.722, dtype=float32), array(0.867, dtype=float32), array(0.889, dtype=float32), array(0.999, dtype=float32), array(0.502, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28370 Training loss 0.04093104600906372 Validation loss 0.041033048182725906 Accuracy 0.5864999890327454 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.784, dtype=float32), array(0.852, dtype=float32), array(0.539, dtype=float32), array(0.999, dtype=float32), array(0.765, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28380 Training loss 0.039683710783720016 Validation loss 0.040117330849170685 Accuracy 0.5950999855995178 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.751, dtype=float32), array(0.839, dtype=float32), array(0.857, dtype=float32), array(0.995, dtype=float32), array(0.57, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28390 Training loss 0.03985803574323654 Validation loss 0.03951041027903557 Accuracy 0.6014999747276306 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.794, dtype=float32), array(0.897, dtype=float32), array(0.815, dtype=float32), array(0.992, dtype=float32), array(0.59, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28400 Training loss 0.03742356598377228 Validation loss 0.04077678918838501 Accuracy 0.5891000032424927 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.894, dtype=float32), array(0.911, dtype=float32), array(0.538, dtype=float32), array(0.992, dtype=float32), array(0.637, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28410 Training loss 0.039595331996679306 Validation loss 0.04007042944431305 Accuracy 0.5960999727249146 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.698, dtype=float32), array(0.817, dtype=float32), array(0.785, dtype=float32), array(0.997, dtype=float32), array(0.732, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28420 Training loss 0.03974803537130356 Validation loss 0.03947928920388222 Accuracy 0.6025000214576721 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.858, dtype=float32), array(0.905, dtype=float32), array(0.711, dtype=float32), array(0.999, dtype=float32), array(0.629, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28430 Training loss 0.040127966552972794 Validation loss 0.03935404494404793 Accuracy 0.6021000146865845 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.82, dtype=float32), array(0.885, dtype=float32), array(0.789, dtype=float32), array(0.996, dtype=float32), array(0.598, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28440 Training loss 0.037293821573257446 Validation loss 0.04035181179642677 Accuracy 0.5921000242233276 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.864, dtype=float32), array(0.86, dtype=float32), array(0.795, dtype=float32), array(0.996, dtype=float32), array(0.479, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28450 Training loss 0.03816992789506912 Validation loss 0.039484038949012756 Accuracy 0.6018000245094299 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.825, dtype=float32), array(0.87, dtype=float32), array(0.719, dtype=float32), array(0.993, dtype=float32), array(0.671, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28460 Training loss 0.03515048325061798 Validation loss 0.03999438136816025 Accuracy 0.5968000292778015 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.735, dtype=float32), array(0.893, dtype=float32), array(0.698, dtype=float32), array(0.999, dtype=float32), array(0.732, dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28470 Training loss 0.038551412522792816 Validation loss 0.03940051794052124 Accuracy 0.6021000146865845 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.836, dtype=float32), array(0.883, dtype=float32), array(0.706, dtype=float32), array(0.99, dtype=float32), array(0.673, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28480 Training loss 0.03617965430021286 Validation loss 0.04003625363111496 Accuracy 0.59579998254776 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.645, dtype=float32), array(0.86, dtype=float32), array(0.878, dtype=float32), array(0.992, dtype=float32), array(0.686, dtype=float32), array(0., dtype=float32), array(0.948, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28490 Training loss 0.03357691317796707 Validation loss 0.039459872990846634 Accuracy 0.6026999950408936 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.748, dtype=float32), array(0.903, dtype=float32), array(0.828, dtype=float32), array(0.999, dtype=float32), array(0.628, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28500 Training loss 0.038345251232385635 Validation loss 0.03979437053203583 Accuracy 0.5996000170707703 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.663, dtype=float32), array(0.877, dtype=float32), array(0.869, dtype=float32), array(0.999, dtype=float32), array(0.649, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28510 Training loss 0.036862656474113464 Validation loss 0.03964845836162567 Accuracy 0.6011999845504761 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.773, dtype=float32), array(0.899, dtype=float32), array(0.766, dtype=float32), array(0.999, dtype=float32), array(0.66, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28520 Training loss 0.038894303143024445 Validation loss 0.0398307703435421 Accuracy 0.5983999967575073 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.704, dtype=float32), array(0.885, dtype=float32), array(0.879, dtype=float32), array(0.997, dtype=float32), array(0.583, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28530 Training loss 0.0426596999168396 Validation loss 0.039653435349464417 Accuracy 0.5996999740600586 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.839, dtype=float32), array(0.86, dtype=float32), array(0.667, dtype=float32), array(0.994, dtype=float32), array(0.7, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28540 Training loss 0.03974143788218498 Validation loss 0.03965870290994644 Accuracy 0.5996999740600586 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.768, dtype=float32), array(0.883, dtype=float32), array(0.702, dtype=float32), array(0.997, dtype=float32), array(0.71, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28550 Training loss 0.040472082793712616 Validation loss 0.03951343894004822 Accuracy 0.6025000214576721 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.703, dtype=float32), array(0.867, dtype=float32), array(0.864, dtype=float32), array(0.999, dtype=float32), array(0.649, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28560 Training loss 0.03492378070950508 Validation loss 0.03957001864910126 Accuracy 0.6010000109672546 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.812, dtype=float32), array(0.881, dtype=float32), array(0.793, dtype=float32), array(0.998, dtype=float32), array(0.58, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28570 Training loss 0.03791601210832596 Validation loss 0.04006785526871681 Accuracy 0.5961999893188477 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.784, dtype=float32), array(0.89, dtype=float32), array(0.641, dtype=float32), array(0.998, dtype=float32), array(0.742, dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28580 Training loss 0.03941911458969116 Validation loss 0.040302250534296036 Accuracy 0.5942999720573425 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.575, dtype=float32), array(0.855, dtype=float32), array(0.872, dtype=float32), array(0.998, dtype=float32), array(0.722, dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28590 Training loss 0.04060305655002594 Validation loss 0.03994869813323021 Accuracy 0.5968000292778015 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.825, dtype=float32), array(0.916, dtype=float32), array(0.779, dtype=float32), array(0.996, dtype=float32), array(0.514, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28600 Training loss 0.04344206675887108 Validation loss 0.040794488042593 Accuracy 0.5885000228881836 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.799, dtype=float32), array(0.92, dtype=float32), array(0.53, dtype=float32), array(0.994, dtype=float32), array(0.743, dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28610 Training loss 0.03771413490176201 Validation loss 0.04077097028493881 Accuracy 0.5893999934196472 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.641, dtype=float32), array(0.881, dtype=float32), array(0.711, dtype=float32), array(0.997, dtype=float32), array(0.787, dtype=float32), array(0., dtype=float32), array(0.923, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28620 Training loss 0.042565129697322845 Validation loss 0.03935067728161812 Accuracy 0.6032999753952026 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.811, dtype=float32), array(0.892, dtype=float32), array(0.766, dtype=float32), array(0.998, dtype=float32), array(0.634, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28630 Training loss 0.03762730956077576 Validation loss 0.039595622569322586 Accuracy 0.5996000170707703 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.739, dtype=float32), array(0.884, dtype=float32), array(0.814, dtype=float32), array(0.985, dtype=float32), array(0.626, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28640 Training loss 0.03743855282664299 Validation loss 0.03946151211857796 Accuracy 0.6015999913215637 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.798, dtype=float32), array(0.888, dtype=float32), array(0.795, dtype=float32), array(0.993, dtype=float32), array(0.607, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28650 Training loss 0.04161277413368225 Validation loss 0.03961542621254921 Accuracy 0.600600004196167 Accuracies by class [array(0., dtype=float32), array(0.947, dtype=float32), array(0.788, dtype=float32), array(0.922, dtype=float32), array(0.756, dtype=float32), array(0.985, dtype=float32), array(0.641, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28660 Training loss 0.03875475749373436 Validation loss 0.0399414598941803 Accuracy 0.5967000126838684 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.721, dtype=float32), array(0.878, dtype=float32), array(0.816, dtype=float32), array(0.955, dtype=float32), array(0.672, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28670 Training loss 0.03722795471549034 Validation loss 0.039934661239385605 Accuracy 0.597100019454956 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.745, dtype=float32), array(0.891, dtype=float32), array(0.85, dtype=float32), array(0.981, dtype=float32), array(0.578, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28680 Training loss 0.04087240621447563 Validation loss 0.03984970599412918 Accuracy 0.597000002861023 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.752, dtype=float32), array(0.904, dtype=float32), array(0.659, dtype=float32), array(0.985, dtype=float32), array(0.744, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28690 Training loss 0.036707136780023575 Validation loss 0.039974913001060486 Accuracy 0.5967000126838684 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.829, dtype=float32), array(0.918, dtype=float32), array(0.644, dtype=float32), array(0.981, dtype=float32), array(0.665, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28700 Training loss 0.03742479905486107 Validation loss 0.040074098855257034 Accuracy 0.59579998254776 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.767, dtype=float32), array(0.911, dtype=float32), array(0.782, dtype=float32), array(0.962, dtype=float32), array(0.597, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28710 Training loss 0.037191782146692276 Validation loss 0.04021294042468071 Accuracy 0.5943999886512756 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.765, dtype=float32), array(0.896, dtype=float32), array(0.824, dtype=float32), array(0.978, dtype=float32), array(0.545, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28720 Training loss 0.03893459588289261 Validation loss 0.03926656022667885 Accuracy 0.6037999987602234 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.786, dtype=float32), array(0.874, dtype=float32), array(0.789, dtype=float32), array(0.995, dtype=float32), array(0.668, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28730 Training loss 0.03963158652186394 Validation loss 0.03959380090236664 Accuracy 0.6007999777793884 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.753, dtype=float32), array(0.892, dtype=float32), array(0.816, dtype=float32), array(0.998, dtype=float32), array(0.632, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28740 Training loss 0.03867090493440628 Validation loss 0.03944355621933937 Accuracy 0.6029999852180481 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.776, dtype=float32), array(0.904, dtype=float32), array(0.751, dtype=float32), array(0.998, dtype=float32), array(0.671, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28750 Training loss 0.03587493300437927 Validation loss 0.03959964960813522 Accuracy 0.6004999876022339 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.672, dtype=float32), array(0.874, dtype=float32), array(0.82, dtype=float32), array(0.999, dtype=float32), array(0.707, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28760 Training loss 0.038199570029973984 Validation loss 0.04018564894795418 Accuracy 0.5945000052452087 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.819, dtype=float32), array(0.943, dtype=float32), array(0.75, dtype=float32), array(0.995, dtype=float32), array(0.509, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28770 Training loss 0.03767460212111473 Validation loss 0.03996617719531059 Accuracy 0.5960000157356262 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.731, dtype=float32), array(0.918, dtype=float32), array(0.855, dtype=float32), array(0.983, dtype=float32), array(0.533, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28780 Training loss 0.037950143218040466 Validation loss 0.039564382284879684 Accuracy 0.6007999777793884 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.857, dtype=float32), array(0.892, dtype=float32), array(0.705, dtype=float32), array(0.991, dtype=float32), array(0.618, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28790 Training loss 0.03958237171173096 Validation loss 0.03977810963988304 Accuracy 0.5982999801635742 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.778, dtype=float32), array(0.879, dtype=float32), array(0.832, dtype=float32), array(0.996, dtype=float32), array(0.552, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28800 Training loss 0.03925463557243347 Validation loss 0.0400446355342865 Accuracy 0.5953999757766724 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.681, dtype=float32), array(0.855, dtype=float32), array(0.878, dtype=float32), array(0.99, dtype=float32), array(0.606, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28810 Training loss 0.03756038472056389 Validation loss 0.03943340852856636 Accuracy 0.6017000079154968 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.724, dtype=float32), array(0.887, dtype=float32), array(0.834, dtype=float32), array(0.993, dtype=float32), array(0.646, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28820 Training loss 0.03915615752339363 Validation loss 0.040884844958782196 Accuracy 0.5866000056266785 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.613, dtype=float32), array(0.843, dtype=float32), array(0.913, dtype=float32), array(0.987, dtype=float32), array(0.566, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28830 Training loss 0.03922434151172638 Validation loss 0.04091334342956543 Accuracy 0.5882999897003174 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.7, dtype=float32), array(0.848, dtype=float32), array(0.904, dtype=float32), array(0.998, dtype=float32), array(0.512, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28840 Training loss 0.03752599656581879 Validation loss 0.039471324533224106 Accuracy 0.6025999784469604 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.707, dtype=float32), array(0.906, dtype=float32), array(0.8, dtype=float32), array(0.998, dtype=float32), array(0.7, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28850 Training loss 0.04331904649734497 Validation loss 0.04094390943646431 Accuracy 0.5885999798774719 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.908, dtype=float32), array(0.894, dtype=float32), array(0.612, dtype=float32), array(0.999, dtype=float32), array(0.556, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28860 Training loss 0.041009221225976944 Validation loss 0.04036902263760567 Accuracy 0.5918999910354614 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.837, dtype=float32), array(0.861, dtype=float32), array(0.57, dtype=float32), array(0.995, dtype=float32), array(0.732, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28870 Training loss 0.03825356438755989 Validation loss 0.04020838439464569 Accuracy 0.5935999751091003 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.648, dtype=float32), array(0.82, dtype=float32), array(0.873, dtype=float32), array(0.989, dtype=float32), array(0.687, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28880 Training loss 0.03807506710290909 Validation loss 0.03943150117993355 Accuracy 0.6014999747276306 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.829, dtype=float32), array(0.865, dtype=float32), array(0.8, dtype=float32), array(0.99, dtype=float32), array(0.598, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28890 Training loss 0.03708924353122711 Validation loss 0.0399504192173481 Accuracy 0.597100019454956 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.671, dtype=float32), array(0.853, dtype=float32), array(0.824, dtype=float32), array(0.952, dtype=float32), array(0.733, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28900 Training loss 0.037802137434482574 Validation loss 0.03915698453783989 Accuracy 0.6039000153541565 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.729, dtype=float32), array(0.881, dtype=float32), array(0.803, dtype=float32), array(0.99, dtype=float32), array(0.707, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28910 Training loss 0.038223061710596085 Validation loss 0.04059116914868355 Accuracy 0.5893999934196472 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.85, dtype=float32), array(0.885, dtype=float32), array(0.785, dtype=float32), array(0.97, dtype=float32), array(0.468, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28920 Training loss 0.04320504888892174 Validation loss 0.04027324542403221 Accuracy 0.5924999713897705 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.742, dtype=float32), array(0.884, dtype=float32), array(0.869, dtype=float32), array(0.984, dtype=float32), array(0.5, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28930 Training loss 0.04027016833424568 Validation loss 0.04062847048044205 Accuracy 0.5895000100135803 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.811, dtype=float32), array(0.938, dtype=float32), array(0.698, dtype=float32), array(0.952, dtype=float32), array(0.553, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28940 Training loss 0.03977483510971069 Validation loss 0.04041776433587074 Accuracy 0.59170001745224 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.586, dtype=float32), array(0.871, dtype=float32), array(0.879, dtype=float32), array(0.992, dtype=float32), array(0.648, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28950 Training loss 0.03927052393555641 Validation loss 0.0394061803817749 Accuracy 0.6011999845504761 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.769, dtype=float32), array(0.936, dtype=float32), array(0.78, dtype=float32), array(0.995, dtype=float32), array(0.596, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28960 Training loss 0.039320800453424454 Validation loss 0.039856214076280594 Accuracy 0.5989999771118164 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.747, dtype=float32), array(0.887, dtype=float32), array(0.676, dtype=float32), array(0.997, dtype=float32), array(0.738, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28970 Training loss 0.041006822139024734 Validation loss 0.039841778576374054 Accuracy 0.5989999771118164 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.848, dtype=float32), array(0.87, dtype=float32), array(0.685, dtype=float32), array(0.997, dtype=float32), array(0.654, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28980 Training loss 0.035262659192085266 Validation loss 0.03932807222008705 Accuracy 0.6037999987602234 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.722, dtype=float32), array(0.866, dtype=float32), array(0.824, dtype=float32), array(0.997, dtype=float32), array(0.684, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 28990 Training loss 0.039017897099256516 Validation loss 0.04009224474430084 Accuracy 0.5936999917030334 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.785, dtype=float32), array(0.908, dtype=float32), array(0.809, dtype=float32), array(0.986, dtype=float32), array(0.508, dtype=float32), array(0., dtype=float32), array(0.984, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29000 Training loss 0.037665948271751404 Validation loss 0.03958519175648689 Accuracy 0.5996000170707703 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.747, dtype=float32), array(0.934, dtype=float32), array(0.728, dtype=float32), array(0.995, dtype=float32), array(0.665, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29010 Training loss 0.03999040648341179 Validation loss 0.040199246257543564 Accuracy 0.5952000021934509 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.854, dtype=float32), array(0.885, dtype=float32), array(0.758, dtype=float32), array(0.999, dtype=float32), array(0.522, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29020 Training loss 0.03841156139969826 Validation loss 0.039796728640794754 Accuracy 0.5967000126838684 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.671, dtype=float32), array(0.869, dtype=float32), array(0.738, dtype=float32), array(0.984, dtype=float32), array(0.765, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29030 Training loss 0.035306189209222794 Validation loss 0.040339525789022446 Accuracy 0.5921000242233276 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.763, dtype=float32), array(0.877, dtype=float32), array(0.847, dtype=float32), array(0.998, dtype=float32), array(0.513, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29040 Training loss 0.038087520748376846 Validation loss 0.040282610803842545 Accuracy 0.5929999947547913 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.645, dtype=float32), array(0.881, dtype=float32), array(0.903, dtype=float32), array(0.998, dtype=float32), array(0.579, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29050 Training loss 0.0374559685587883 Validation loss 0.03938520327210426 Accuracy 0.6010000109672546 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.774, dtype=float32), array(0.935, dtype=float32), array(0.775, dtype=float32), array(0.994, dtype=float32), array(0.622, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29060 Training loss 0.03706243634223938 Validation loss 0.03975332900881767 Accuracy 0.5989999771118164 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.816, dtype=float32), array(0.88, dtype=float32), array(0.645, dtype=float32), array(0.999, dtype=float32), array(0.756, dtype=float32), array(0., dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29070 Training loss 0.03732716292142868 Validation loss 0.03931056335568428 Accuracy 0.6019999980926514 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.776, dtype=float32), array(0.91, dtype=float32), array(0.818, dtype=float32), array(0.977, dtype=float32), array(0.607, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29080 Training loss 0.036571431905031204 Validation loss 0.03961438685655594 Accuracy 0.598800003528595 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.808, dtype=float32), array(0.947, dtype=float32), array(0.684, dtype=float32), array(0.979, dtype=float32), array(0.651, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29090 Training loss 0.03663000836968422 Validation loss 0.039540089666843414 Accuracy 0.5999000072479248 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.77, dtype=float32), array(0.855, dtype=float32), array(0.855, dtype=float32), array(0.989, dtype=float32), array(0.597, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29100 Training loss 0.036002468317747116 Validation loss 0.040006816387176514 Accuracy 0.5956000089645386 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.687, dtype=float32), array(0.9, dtype=float32), array(0.693, dtype=float32), array(0.997, dtype=float32), array(0.777, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29110 Training loss 0.03778476268053055 Validation loss 0.0394357368350029 Accuracy 0.6031000018119812 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.817, dtype=float32), array(0.904, dtype=float32), array(0.731, dtype=float32), array(0.999, dtype=float32), array(0.661, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29120 Training loss 0.03478255867958069 Validation loss 0.03913024440407753 Accuracy 0.6037999987602234 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.711, dtype=float32), array(0.891, dtype=float32), array(0.831, dtype=float32), array(0.997, dtype=float32), array(0.677, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29130 Training loss 0.03916417062282562 Validation loss 0.04017990827560425 Accuracy 0.5927000045776367 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.569, dtype=float32), array(0.894, dtype=float32), array(0.791, dtype=float32), array(0.992, dtype=float32), array(0.756, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29140 Training loss 0.039893995970487595 Validation loss 0.039693962782621384 Accuracy 0.5992000102996826 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.87, dtype=float32), array(0.911, dtype=float32), array(0.641, dtype=float32), array(0.994, dtype=float32), array(0.652, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29150 Training loss 0.03981863334774971 Validation loss 0.039178114384412766 Accuracy 0.6037999987602234 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.761, dtype=float32), array(0.871, dtype=float32), array(0.758, dtype=float32), array(0.995, dtype=float32), array(0.709, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29160 Training loss 0.0409485325217247 Validation loss 0.039379045367240906 Accuracy 0.6010000109672546 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.692, dtype=float32), array(0.908, dtype=float32), array(0.803, dtype=float32), array(0.984, dtype=float32), array(0.679, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29170 Training loss 0.040167611092329025 Validation loss 0.03912325203418732 Accuracy 0.6035000085830688 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.765, dtype=float32), array(0.866, dtype=float32), array(0.816, dtype=float32), array(0.978, dtype=float32), array(0.66, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29180 Training loss 0.03661826252937317 Validation loss 0.04124628007411957 Accuracy 0.583899974822998 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.911, dtype=float32), array(0.893, dtype=float32), array(0.593, dtype=float32), array(0.997, dtype=float32), array(0.529, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29190 Training loss 0.03770199418067932 Validation loss 0.03898760303854942 Accuracy 0.6057999730110168 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.751, dtype=float32), array(0.899, dtype=float32), array(0.778, dtype=float32), array(0.995, dtype=float32), array(0.71, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29200 Training loss 0.038427747786045074 Validation loss 0.03930698335170746 Accuracy 0.6031000018119812 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.722, dtype=float32), array(0.917, dtype=float32), array(0.813, dtype=float32), array(0.974, dtype=float32), array(0.668, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29210 Training loss 0.03856253623962402 Validation loss 0.040289923548698425 Accuracy 0.5924999713897705 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.867, dtype=float32), array(0.896, dtype=float32), array(0.708, dtype=float32), array(0.999, dtype=float32), array(0.526, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29220 Training loss 0.03616206347942352 Validation loss 0.039238687604665756 Accuracy 0.6043999791145325 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.746, dtype=float32), array(0.845, dtype=float32), array(0.816, dtype=float32), array(0.998, dtype=float32), array(0.704, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29230 Training loss 0.03552570939064026 Validation loss 0.03879344090819359 Accuracy 0.6071000099182129 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.756, dtype=float32), array(0.894, dtype=float32), array(0.798, dtype=float32), array(0.995, dtype=float32), array(0.697, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29240 Training loss 0.03741779178380966 Validation loss 0.039026424288749695 Accuracy 0.6051999926567078 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.772, dtype=float32), array(0.889, dtype=float32), array(0.803, dtype=float32), array(0.99, dtype=float32), array(0.664, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29250 Training loss 0.03851088881492615 Validation loss 0.039536137133836746 Accuracy 0.6007999777793884 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.804, dtype=float32), array(0.91, dtype=float32), array(0.747, dtype=float32), array(0.956, dtype=float32), array(0.653, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29260 Training loss 0.04065266624093056 Validation loss 0.03907566890120506 Accuracy 0.6061000227928162 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.806, dtype=float32), array(0.886, dtype=float32), array(0.765, dtype=float32), array(0.997, dtype=float32), array(0.671, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29270 Training loss 0.038091398775577545 Validation loss 0.03892719745635986 Accuracy 0.6071000099182129 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.799, dtype=float32), array(0.889, dtype=float32), array(0.77, dtype=float32), array(0.993, dtype=float32), array(0.679, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29280 Training loss 0.036620233207941055 Validation loss 0.039154089987277985 Accuracy 0.6047999858856201 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.783, dtype=float32), array(0.891, dtype=float32), array(0.815, dtype=float32), array(0.997, dtype=float32), array(0.624, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29290 Training loss 0.04167826473712921 Validation loss 0.0394558310508728 Accuracy 0.6011000275611877 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.754, dtype=float32), array(0.906, dtype=float32), array(0.805, dtype=float32), array(0.994, dtype=float32), array(0.601, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29300 Training loss 0.03951188176870346 Validation loss 0.03938581421971321 Accuracy 0.6025999784469604 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.851, dtype=float32), array(0.886, dtype=float32), array(0.719, dtype=float32), array(0.997, dtype=float32), array(0.623, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29310 Training loss 0.034539442509412766 Validation loss 0.03944173455238342 Accuracy 0.6014000177383423 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.734, dtype=float32), array(0.875, dtype=float32), array(0.834, dtype=float32), array(0.998, dtype=float32), array(0.623, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29320 Training loss 0.043528199195861816 Validation loss 0.04048154875636101 Accuracy 0.5903000235557556 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.871, dtype=float32), array(0.855, dtype=float32), array(0.792, dtype=float32), array(0.996, dtype=float32), array(0.479, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29330 Training loss 0.03493373468518257 Validation loss 0.03929061442613602 Accuracy 0.6014999747276306 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.763, dtype=float32), array(0.872, dtype=float32), array(0.738, dtype=float32), array(0.992, dtype=float32), array(0.736, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29340 Training loss 0.03827129304409027 Validation loss 0.03947556018829346 Accuracy 0.6011999845504761 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.701, dtype=float32), array(0.86, dtype=float32), array(0.801, dtype=float32), array(0.996, dtype=float32), array(0.742, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29350 Training loss 0.037344157695770264 Validation loss 0.03965830057859421 Accuracy 0.5978999733924866 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.694, dtype=float32), array(0.86, dtype=float32), array(0.879, dtype=float32), array(0.993, dtype=float32), array(0.627, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29360 Training loss 0.03673596680164337 Validation loss 0.0408969447016716 Accuracy 0.5849999785423279 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.885, dtype=float32), array(0.913, dtype=float32), array(0.737, dtype=float32), array(0.987, dtype=float32), array(0.401, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29370 Training loss 0.038946446031332016 Validation loss 0.03931845352053642 Accuracy 0.6007999777793884 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.83, dtype=float32), array(0.944, dtype=float32), array(0.691, dtype=float32), array(0.995, dtype=float32), array(0.631, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29380 Training loss 0.04231948405504227 Validation loss 0.040731046348810196 Accuracy 0.5871999859809875 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.881, dtype=float32), array(0.929, dtype=float32), array(0.509, dtype=float32), array(0.986, dtype=float32), array(0.646, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29390 Training loss 0.043421000242233276 Validation loss 0.040058813989162445 Accuracy 0.5958999991416931 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.69, dtype=float32), array(0.847, dtype=float32), array(0.852, dtype=float32), array(0.934, dtype=float32), array(0.693, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29400 Training loss 0.03980082646012306 Validation loss 0.03998485207557678 Accuracy 0.5949000120162964 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.81, dtype=float32), array(0.842, dtype=float32), array(0.834, dtype=float32), array(0.997, dtype=float32), array(0.538, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29410 Training loss 0.03859930858016014 Validation loss 0.03981681913137436 Accuracy 0.5976999998092651 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.731, dtype=float32), array(0.929, dtype=float32), array(0.849, dtype=float32), array(0.998, dtype=float32), array(0.543, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29420 Training loss 0.03895263001322746 Validation loss 0.038755420595407486 Accuracy 0.6072999835014343 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.769, dtype=float32), array(0.896, dtype=float32), array(0.765, dtype=float32), array(0.998, dtype=float32), array(0.71, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29430 Training loss 0.035898856818675995 Validation loss 0.03902313858270645 Accuracy 0.605400025844574 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.825, dtype=float32), array(0.876, dtype=float32), array(0.727, dtype=float32), array(0.999, dtype=float32), array(0.684, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29440 Training loss 0.0376284196972847 Validation loss 0.03890632838010788 Accuracy 0.6057000160217285 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.739, dtype=float32), array(0.881, dtype=float32), array(0.781, dtype=float32), array(0.998, dtype=float32), array(0.716, dtype=float32), array(0., dtype=float32), array(0.981, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29450 Training loss 0.03354029729962349 Validation loss 0.039240408688783646 Accuracy 0.6029999852180481 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.785, dtype=float32), array(0.889, dtype=float32), array(0.715, dtype=float32), array(0.971, dtype=float32), array(0.73, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29460 Training loss 0.0376274473965168 Validation loss 0.039739787578582764 Accuracy 0.5978000164031982 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.835, dtype=float32), array(0.858, dtype=float32), array(0.832, dtype=float32), array(0.995, dtype=float32), array(0.517, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29470 Training loss 0.03804735094308853 Validation loss 0.03907874599099159 Accuracy 0.6051999926567078 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.828, dtype=float32), array(0.889, dtype=float32), array(0.748, dtype=float32), array(0.999, dtype=float32), array(0.67, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29480 Training loss 0.03825407475233078 Validation loss 0.03990181162953377 Accuracy 0.5964999794960022 Accuracies by class [array(0., dtype=float32), array(0.943, dtype=float32), array(0.686, dtype=float32), array(0.917, dtype=float32), array(0.871, dtype=float32), array(0.971, dtype=float32), array(0.599, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29490 Training loss 0.03753591328859329 Validation loss 0.03970332071185112 Accuracy 0.5979999899864197 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.88, dtype=float32), array(0.829, dtype=float32), array(0.713, dtype=float32), array(0.998, dtype=float32), array(0.638, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29500 Training loss 0.038243867456912994 Validation loss 0.03940797969698906 Accuracy 0.6003999710083008 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.766, dtype=float32), array(0.815, dtype=float32), array(0.819, dtype=float32), array(0.993, dtype=float32), array(0.678, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29510 Training loss 0.03576391562819481 Validation loss 0.03973471373319626 Accuracy 0.5974000096321106 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.791, dtype=float32), array(0.906, dtype=float32), array(0.837, dtype=float32), array(0.995, dtype=float32), array(0.523, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29520 Training loss 0.040963314473629 Validation loss 0.03956051170825958 Accuracy 0.5997999906539917 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.768, dtype=float32), array(0.893, dtype=float32), array(0.671, dtype=float32), array(0.997, dtype=float32), array(0.761, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29530 Training loss 0.03942996636033058 Validation loss 0.040992964059114456 Accuracy 0.5856999754905701 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.91, dtype=float32), array(0.869, dtype=float32), array(0.55, dtype=float32), array(0.994, dtype=float32), array(0.613, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29540 Training loss 0.03833171725273132 Validation loss 0.03998655080795288 Accuracy 0.5947999954223633 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.695, dtype=float32), array(0.88, dtype=float32), array(0.892, dtype=float32), array(0.995, dtype=float32), array(0.558, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29550 Training loss 0.03925104811787605 Validation loss 0.03981941565871239 Accuracy 0.5979999899864197 Accuracies by class [array(0., dtype=float32), array(0.944, dtype=float32), array(0.859, dtype=float32), array(0.942, dtype=float32), array(0.642, dtype=float32), array(0.997, dtype=float32), array(0.645, dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29560 Training loss 0.037563830614089966 Validation loss 0.03925046697258949 Accuracy 0.602400004863739 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.819, dtype=float32), array(0.903, dtype=float32), array(0.778, dtype=float32), array(0.979, dtype=float32), array(0.605, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29570 Training loss 0.0390191487967968 Validation loss 0.039440400898456573 Accuracy 0.6011999845504761 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.847, dtype=float32), array(0.878, dtype=float32), array(0.664, dtype=float32), array(0.999, dtype=float32), array(0.703, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29580 Training loss 0.03709669038653374 Validation loss 0.04098111763596535 Accuracy 0.5846999883651733 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.618, dtype=float32), array(0.88, dtype=float32), array(0.61, dtype=float32), array(0.996, dtype=float32), array(0.838, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29590 Training loss 0.03483911603689194 Validation loss 0.039459649473428726 Accuracy 0.6003000140190125 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.805, dtype=float32), array(0.933, dtype=float32), array(0.724, dtype=float32), array(0.996, dtype=float32), array(0.609, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29600 Training loss 0.03650439530611038 Validation loss 0.03906279429793358 Accuracy 0.6047999858856201 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.739, dtype=float32), array(0.884, dtype=float32), array(0.839, dtype=float32), array(0.997, dtype=float32), array(0.653, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29610 Training loss 0.03872006759047508 Validation loss 0.03900369629263878 Accuracy 0.6050000190734863 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.701, dtype=float32), array(0.878, dtype=float32), array(0.818, dtype=float32), array(0.99, dtype=float32), array(0.736, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29620 Training loss 0.03501887992024422 Validation loss 0.03904881328344345 Accuracy 0.6040999889373779 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.732, dtype=float32), array(0.902, dtype=float32), array(0.752, dtype=float32), array(0.997, dtype=float32), array(0.752, dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29630 Training loss 0.03835543245077133 Validation loss 0.03955265134572983 Accuracy 0.5996000170707703 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.682, dtype=float32), array(0.883, dtype=float32), array(0.743, dtype=float32), array(0.986, dtype=float32), array(0.801, dtype=float32), array(0., dtype=float32), array(0.942, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29640 Training loss 0.034484636038541794 Validation loss 0.03951933979988098 Accuracy 0.5992000102996826 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.805, dtype=float32), array(0.929, dtype=float32), array(0.686, dtype=float32), array(0.992, dtype=float32), array(0.66, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29650 Training loss 0.03794213756918907 Validation loss 0.04021470248699188 Accuracy 0.5921000242233276 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.852, dtype=float32), array(0.921, dtype=float32), array(0.786, dtype=float32), array(0.994, dtype=float32), array(0.449, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29660 Training loss 0.03717887029051781 Validation loss 0.039563894271850586 Accuracy 0.598800003528595 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.818, dtype=float32), array(0.91, dtype=float32), array(0.83, dtype=float32), array(0.994, dtype=float32), array(0.525, dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29670 Training loss 0.03772875666618347 Validation loss 0.04014754295349121 Accuracy 0.5928000211715698 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.832, dtype=float32), array(0.856, dtype=float32), array(0.846, dtype=float32), array(0.992, dtype=float32), array(0.485, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29680 Training loss 0.03577091544866562 Validation loss 0.03992491587996483 Accuracy 0.5965999960899353 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.74, dtype=float32), array(0.774, dtype=float32), array(0.874, dtype=float32), array(0.985, dtype=float32), array(0.656, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29690 Training loss 0.038729287683963776 Validation loss 0.039161358028650284 Accuracy 0.6015999913215637 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.856, dtype=float32), array(0.919, dtype=float32), array(0.716, dtype=float32), array(0.99, dtype=float32), array(0.597, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29700 Training loss 0.0364951454102993 Validation loss 0.039362210780382156 Accuracy 0.6013000011444092 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.816, dtype=float32), array(0.795, dtype=float32), array(0.811, dtype=float32), array(0.986, dtype=float32), array(0.668, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29710 Training loss 0.03766662999987602 Validation loss 0.039669446647167206 Accuracy 0.5978999733924866 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.78, dtype=float32), array(0.822, dtype=float32), array(0.88, dtype=float32), array(0.984, dtype=float32), array(0.582, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29720 Training loss 0.037967052310705185 Validation loss 0.03976763039827347 Accuracy 0.5971999764442444 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.851, dtype=float32), array(0.842, dtype=float32), array(0.818, dtype=float32), array(0.984, dtype=float32), array(0.551, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29730 Training loss 0.03497439622879028 Validation loss 0.038911886513233185 Accuracy 0.6062999963760376 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.826, dtype=float32), array(0.9, dtype=float32), array(0.811, dtype=float32), array(0.994, dtype=float32), array(0.604, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29740 Training loss 0.03827279433608055 Validation loss 0.03877929598093033 Accuracy 0.6065000295639038 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.835, dtype=float32), array(0.927, dtype=float32), array(0.733, dtype=float32), array(0.995, dtype=float32), array(0.647, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29750 Training loss 0.04214756563305855 Validation loss 0.04117148369550705 Accuracy 0.5823000073432922 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.876, dtype=float32), array(0.906, dtype=float32), array(0.393, dtype=float32), array(0.992, dtype=float32), array(0.723, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29760 Training loss 0.039836250245571136 Validation loss 0.039178092032670975 Accuracy 0.6028000116348267 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.86, dtype=float32), array(0.894, dtype=float32), array(0.679, dtype=float32), array(0.996, dtype=float32), array(0.659, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29770 Training loss 0.032365113496780396 Validation loss 0.03903288021683693 Accuracy 0.6044999957084656 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.769, dtype=float32), array(0.93, dtype=float32), array(0.809, dtype=float32), array(0.996, dtype=float32), array(0.608, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29780 Training loss 0.03686895966529846 Validation loss 0.03888735920190811 Accuracy 0.6061000227928162 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.708, dtype=float32), array(0.885, dtype=float32), array(0.814, dtype=float32), array(0.999, dtype=float32), array(0.723, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29790 Training loss 0.040353890508413315 Validation loss 0.040030572563409805 Accuracy 0.5947999954223633 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.888, dtype=float32), array(0.911, dtype=float32), array(0.614, dtype=float32), array(0.998, dtype=float32), array(0.61, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29800 Training loss 0.039703309535980225 Validation loss 0.03952804580330849 Accuracy 0.5999000072479248 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.666, dtype=float32), array(0.844, dtype=float32), array(0.863, dtype=float32), array(0.99, dtype=float32), array(0.702, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29810 Training loss 0.03860120475292206 Validation loss 0.0390407033264637 Accuracy 0.6055999994277954 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.784, dtype=float32), array(0.854, dtype=float32), array(0.806, dtype=float32), array(0.999, dtype=float32), array(0.691, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29820 Training loss 0.03848177194595337 Validation loss 0.03906901553273201 Accuracy 0.6060000061988831 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.799, dtype=float32), array(0.86, dtype=float32), array(0.822, dtype=float32), array(0.999, dtype=float32), array(0.658, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29830 Training loss 0.038399603217840195 Validation loss 0.039671942591667175 Accuracy 0.5993000268936157 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.846, dtype=float32), array(0.861, dtype=float32), array(0.811, dtype=float32), array(0.999, dtype=float32), array(0.539, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29840 Training loss 0.03872184827923775 Validation loss 0.03952091932296753 Accuracy 0.5996000170707703 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.649, dtype=float32), array(0.894, dtype=float32), array(0.901, dtype=float32), array(0.993, dtype=float32), array(0.624, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29850 Training loss 0.04213125631213188 Validation loss 0.039585355669260025 Accuracy 0.5996000170707703 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.827, dtype=float32), array(0.944, dtype=float32), array(0.787, dtype=float32), array(0.998, dtype=float32), array(0.513, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29860 Training loss 0.03871393948793411 Validation loss 0.040019650012254715 Accuracy 0.5964000225067139 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.599, dtype=float32), array(0.903, dtype=float32), array(0.869, dtype=float32), array(0.999, dtype=float32), array(0.683, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29870 Training loss 0.042780034244060516 Validation loss 0.04073510318994522 Accuracy 0.586899995803833 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.911, dtype=float32), array(0.917, dtype=float32), array(0.518, dtype=float32), array(0.994, dtype=float32), array(0.613, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29880 Training loss 0.04034831374883652 Validation loss 0.040718041360378265 Accuracy 0.5871000289916992 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.593, dtype=float32), array(0.881, dtype=float32), array(0.681, dtype=float32), array(0.987, dtype=float32), array(0.818, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29890 Training loss 0.03785731643438339 Validation loss 0.04027291387319565 Accuracy 0.5918999910354614 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.69, dtype=float32), array(0.928, dtype=float32), array(0.888, dtype=float32), array(0.995, dtype=float32), array(0.501, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29900 Training loss 0.03773496672511101 Validation loss 0.03964357450604439 Accuracy 0.5981000065803528 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.645, dtype=float32), array(0.886, dtype=float32), array(0.749, dtype=float32), array(0.996, dtype=float32), array(0.787, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29910 Training loss 0.03648896887898445 Validation loss 0.03936697915196419 Accuracy 0.6010000109672546 Accuracies by class [array(0., dtype=float32), array(0.949, dtype=float32), array(0.784, dtype=float32), array(0.947, dtype=float32), array(0.696, dtype=float32), array(0.993, dtype=float32), array(0.678, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29920 Training loss 0.03578212112188339 Validation loss 0.03899100050330162 Accuracy 0.6053000092506409 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.797, dtype=float32), array(0.902, dtype=float32), array(0.728, dtype=float32), array(0.982, dtype=float32), array(0.729, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29930 Training loss 0.039061982184648514 Validation loss 0.0396750308573246 Accuracy 0.5989999771118164 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.733, dtype=float32), array(0.88, dtype=float32), array(0.884, dtype=float32), array(0.973, dtype=float32), array(0.591, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29940 Training loss 0.04041319712996483 Validation loss 0.03894240781664848 Accuracy 0.6050999760627747 Accuracies by class [array(0., dtype=float32), array(0.945, dtype=float32), array(0.832, dtype=float32), array(0.918, dtype=float32), array(0.791, dtype=float32), array(0.99, dtype=float32), array(0.609, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29950 Training loss 0.03651920333504677 Validation loss 0.038869477808475494 Accuracy 0.60589998960495 Accuracies by class [array(0., dtype=float32), array(0.946, dtype=float32), array(0.782, dtype=float32), array(0.915, dtype=float32), array(0.772, dtype=float32), array(0.984, dtype=float32), array(0.699, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29960 Training loss 0.03983281925320625 Validation loss 0.042991332709789276 Accuracy 0.5651000142097473 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.335, dtype=float32), array(0.823, dtype=float32), array(0.953, dtype=float32), array(0.983, dtype=float32), array(0.649, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29970 Training loss 0.03548859804868698 Validation loss 0.038432031869888306 Accuracy 0.6093000173568726 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.79, dtype=float32), array(0.872, dtype=float32), array(0.804, dtype=float32), array(0.99, dtype=float32), array(0.696, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29980 Training loss 0.03616984561085701 Validation loss 0.03851086273789406 Accuracy 0.6089000105857849 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.77, dtype=float32), array(0.898, dtype=float32), array(0.836, dtype=float32), array(0.99, dtype=float32), array(0.655, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 29990 Training loss 0.039916396141052246 Validation loss 0.03901473060250282 Accuracy 0.6037999987602234 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.827, dtype=float32), array(0.931, dtype=float32), array(0.683, dtype=float32), array(0.987, dtype=float32), array(0.663, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30000 Training loss 0.03546420857310295 Validation loss 0.03864243999123573 Accuracy 0.6079000234603882 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.783, dtype=float32), array(0.922, dtype=float32), array(0.803, dtype=float32), array(0.996, dtype=float32), array(0.65, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30010 Training loss 0.03740484267473221 Validation loss 0.0391964353621006 Accuracy 0.602400004863739 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.861, dtype=float32), array(0.894, dtype=float32), array(0.668, dtype=float32), array(0.995, dtype=float32), array(0.682, dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30020 Training loss 0.03718136250972748 Validation loss 0.039800941944122314 Accuracy 0.59579998254776 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.871, dtype=float32), array(0.912, dtype=float32), array(0.701, dtype=float32), array(0.99, dtype=float32), array(0.557, dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30030 Training loss 0.04211123287677765 Validation loss 0.03918788209557533 Accuracy 0.6044999957084656 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.774, dtype=float32), array(0.869, dtype=float32), array(0.797, dtype=float32), array(0.954, dtype=float32), array(0.709, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30040 Training loss 0.03756137564778328 Validation loss 0.0391908623278141 Accuracy 0.6019999980926514 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.671, dtype=float32), array(0.9, dtype=float32), array(0.761, dtype=float32), array(0.993, dtype=float32), array(0.775, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30050 Training loss 0.03648001700639725 Validation loss 0.03908858820796013 Accuracy 0.6047000288963318 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.841, dtype=float32), array(0.905, dtype=float32), array(0.709, dtype=float32), array(0.969, dtype=float32), array(0.694, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30060 Training loss 0.03951668366789818 Validation loss 0.03902437910437584 Accuracy 0.6046000123023987 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.849, dtype=float32), array(0.876, dtype=float32), array(0.685, dtype=float32), array(0.987, dtype=float32), array(0.722, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30070 Training loss 0.03828645497560501 Validation loss 0.039318446069955826 Accuracy 0.6008999943733215 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.702, dtype=float32), array(0.877, dtype=float32), array(0.716, dtype=float32), array(0.993, dtype=float32), array(0.784, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30080 Training loss 0.03614659979939461 Validation loss 0.038371309638023376 Accuracy 0.6111999750137329 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.824, dtype=float32), array(0.9, dtype=float32), array(0.766, dtype=float32), array(0.999, dtype=float32), array(0.688, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30090 Training loss 0.037460848689079285 Validation loss 0.038589708507061005 Accuracy 0.6083999872207642 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.757, dtype=float32), array(0.896, dtype=float32), array(0.832, dtype=float32), array(0.991, dtype=float32), array(0.664, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30100 Training loss 0.03803079575300217 Validation loss 0.03960142657160759 Accuracy 0.5993000268936157 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.597, dtype=float32), array(0.866, dtype=float32), array(0.877, dtype=float32), array(0.995, dtype=float32), array(0.722, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30110 Training loss 0.03631296753883362 Validation loss 0.03873211145401001 Accuracy 0.6086999773979187 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.767, dtype=float32), array(0.909, dtype=float32), array(0.773, dtype=float32), array(0.998, dtype=float32), array(0.708, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30120 Training loss 0.03723287954926491 Validation loss 0.03914580121636391 Accuracy 0.6043999791145325 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.743, dtype=float32), array(0.895, dtype=float32), array(0.715, dtype=float32), array(0.997, dtype=float32), array(0.764, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30130 Training loss 0.03777037933468819 Validation loss 0.04058697819709778 Accuracy 0.5898000001907349 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.52, dtype=float32), array(0.891, dtype=float32), array(0.737, dtype=float32), array(0.995, dtype=float32), array(0.829, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30140 Training loss 0.03879623860120773 Validation loss 0.03901069611310959 Accuracy 0.6054999828338623 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.82, dtype=float32), array(0.914, dtype=float32), array(0.775, dtype=float32), array(0.995, dtype=float32), array(0.615, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30150 Training loss 0.03594345226883888 Validation loss 0.038744937628507614 Accuracy 0.608299970626831 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.811, dtype=float32), array(0.876, dtype=float32), array(0.76, dtype=float32), array(0.995, dtype=float32), array(0.713, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30160 Training loss 0.03944777697324753 Validation loss 0.03899877890944481 Accuracy 0.6047000288963318 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.779, dtype=float32), array(0.873, dtype=float32), array(0.835, dtype=float32), array(0.989, dtype=float32), array(0.634, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30170 Training loss 0.03218143805861473 Validation loss 0.03895270451903343 Accuracy 0.6062999963760376 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.737, dtype=float32), array(0.861, dtype=float32), array(0.818, dtype=float32), array(0.997, dtype=float32), array(0.721, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30180 Training loss 0.03774582967162132 Validation loss 0.040072374045848846 Accuracy 0.5946000218391418 Accuracies by class [array(0., dtype=float32), array(0.978, dtype=float32), array(0.764, dtype=float32), array(0.845, dtype=float32), array(0.878, dtype=float32), array(0.995, dtype=float32), array(0.526, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30190 Training loss 0.03999880328774452 Validation loss 0.04009651020169258 Accuracy 0.5942999720573425 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.788, dtype=float32), array(0.822, dtype=float32), array(0.875, dtype=float32), array(0.995, dtype=float32), array(0.527, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30200 Training loss 0.03731131926178932 Validation loss 0.040307674556970596 Accuracy 0.5916000008583069 Accuracies by class [array(0., dtype=float32), array(0.952, dtype=float32), array(0.727, dtype=float32), array(0.904, dtype=float32), array(0.597, dtype=float32), array(0.989, dtype=float32), array(0.784, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30210 Training loss 0.04005604609847069 Validation loss 0.038926541805267334 Accuracy 0.6051999926567078 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.83, dtype=float32), array(0.877, dtype=float32), array(0.703, dtype=float32), array(0.994, dtype=float32), array(0.703, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30220 Training loss 0.03860495612025261 Validation loss 0.03914698213338852 Accuracy 0.6033999919891357 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.833, dtype=float32), array(0.892, dtype=float32), array(0.774, dtype=float32), array(0.968, dtype=float32), array(0.629, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30230 Training loss 0.03866038843989372 Validation loss 0.03932449221611023 Accuracy 0.6011000275611877 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.766, dtype=float32), array(0.835, dtype=float32), array(0.728, dtype=float32), array(0.992, dtype=float32), array(0.755, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30240 Training loss 0.036039482802152634 Validation loss 0.038956038653850555 Accuracy 0.6050000190734863 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.827, dtype=float32), array(0.919, dtype=float32), array(0.712, dtype=float32), array(0.991, dtype=float32), array(0.673, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30250 Training loss 0.037264663726091385 Validation loss 0.039231814444065094 Accuracy 0.6031000018119812 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.705, dtype=float32), array(0.84, dtype=float32), array(0.883, dtype=float32), array(0.993, dtype=float32), array(0.665, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30260 Training loss 0.04024088382720947 Validation loss 0.03882879018783569 Accuracy 0.6079999804496765 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.8, dtype=float32), array(0.923, dtype=float32), array(0.76, dtype=float32), array(0.997, dtype=float32), array(0.69, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30270 Training loss 0.035549916326999664 Validation loss 0.03900356590747833 Accuracy 0.6046000123023987 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.862, dtype=float32), array(0.873, dtype=float32), array(0.73, dtype=float32), array(0.98, dtype=float32), array(0.658, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30280 Training loss 0.038954656571149826 Validation loss 0.03940412402153015 Accuracy 0.6004999876022339 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.76, dtype=float32), array(0.835, dtype=float32), array(0.87, dtype=float32), array(0.994, dtype=float32), array(0.593, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30290 Training loss 0.04096626862883568 Validation loss 0.039184246212244034 Accuracy 0.6032999753952026 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.875, dtype=float32), array(0.902, dtype=float32), array(0.697, dtype=float32), array(0.999, dtype=float32), array(0.617, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30300 Training loss 0.03698590770363808 Validation loss 0.03939862921833992 Accuracy 0.6011999845504761 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.819, dtype=float32), array(0.886, dtype=float32), array(0.795, dtype=float32), array(0.998, dtype=float32), array(0.57, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30310 Training loss 0.036300256848335266 Validation loss 0.03897348791360855 Accuracy 0.6054999828338623 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.76, dtype=float32), array(0.885, dtype=float32), array(0.787, dtype=float32), array(0.982, dtype=float32), array(0.701, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30320 Training loss 0.039393261075019836 Validation loss 0.0388699509203434 Accuracy 0.6065999865531921 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.741, dtype=float32), array(0.882, dtype=float32), array(0.85, dtype=float32), array(0.988, dtype=float32), array(0.658, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30330 Training loss 0.03962971270084381 Validation loss 0.03911533206701279 Accuracy 0.603600025177002 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.824, dtype=float32), array(0.937, dtype=float32), array(0.756, dtype=float32), array(0.996, dtype=float32), array(0.584, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30340 Training loss 0.039183054119348526 Validation loss 0.03903180733323097 Accuracy 0.6047000288963318 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.814, dtype=float32), array(0.904, dtype=float32), array(0.763, dtype=float32), array(0.998, dtype=float32), array(0.63, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30350 Training loss 0.036887478083372116 Validation loss 0.03905896097421646 Accuracy 0.604200005531311 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.83, dtype=float32), array(0.893, dtype=float32), array(0.811, dtype=float32), array(0.997, dtype=float32), array(0.573, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30360 Training loss 0.0382029265165329 Validation loss 0.03940240666270256 Accuracy 0.5997999906539917 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.797, dtype=float32), array(0.872, dtype=float32), array(0.852, dtype=float32), array(0.99, dtype=float32), array(0.535, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30370 Training loss 0.0374903678894043 Validation loss 0.03896395489573479 Accuracy 0.6044999957084656 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.792, dtype=float32), array(0.882, dtype=float32), array(0.858, dtype=float32), array(0.989, dtype=float32), array(0.586, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30380 Training loss 0.03510657325387001 Validation loss 0.03979932516813278 Accuracy 0.5968999862670898 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.809, dtype=float32), array(0.902, dtype=float32), array(0.58, dtype=float32), array(0.998, dtype=float32), array(0.767, dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30390 Training loss 0.03741534799337387 Validation loss 0.0407329723238945 Accuracy 0.5875999927520752 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.645, dtype=float32), array(0.858, dtype=float32), array(0.621, dtype=float32), array(0.991, dtype=float32), array(0.847, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30400 Training loss 0.037296779453754425 Validation loss 0.03874429315328598 Accuracy 0.6068999767303467 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.717, dtype=float32), array(0.875, dtype=float32), array(0.854, dtype=float32), array(0.995, dtype=float32), array(0.698, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30410 Training loss 0.03524263575673103 Validation loss 0.03900381177663803 Accuracy 0.6068000197410583 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.845, dtype=float32), array(0.894, dtype=float32), array(0.743, dtype=float32), array(0.999, dtype=float32), array(0.667, dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30420 Training loss 0.03634015470743179 Validation loss 0.03910733014345169 Accuracy 0.6035000085830688 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.716, dtype=float32), array(0.897, dtype=float32), array(0.877, dtype=float32), array(0.996, dtype=float32), array(0.615, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30430 Training loss 0.037634141743183136 Validation loss 0.03865861892700195 Accuracy 0.607200026512146 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.808, dtype=float32), array(0.923, dtype=float32), array(0.73, dtype=float32), array(0.994, dtype=float32), array(0.685, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30440 Training loss 0.03739412501454353 Validation loss 0.03948142006993294 Accuracy 0.5993000268936157 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.782, dtype=float32), array(0.918, dtype=float32), array(0.857, dtype=float32), array(0.992, dtype=float32), array(0.511, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30450 Training loss 0.03435315564274788 Validation loss 0.038823556154966354 Accuracy 0.6057000160217285 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.721, dtype=float32), array(0.91, dtype=float32), array(0.757, dtype=float32), array(0.992, dtype=float32), array(0.754, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30460 Training loss 0.03821416199207306 Validation loss 0.03874203562736511 Accuracy 0.6069999933242798 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.77, dtype=float32), array(0.865, dtype=float32), array(0.836, dtype=float32), array(0.998, dtype=float32), array(0.689, dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30470 Training loss 0.03342956677079201 Validation loss 0.03933000564575195 Accuracy 0.6028000116348267 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.849, dtype=float32), array(0.858, dtype=float32), array(0.784, dtype=float32), array(0.956, dtype=float32), array(0.639, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30480 Training loss 0.03819624334573746 Validation loss 0.03926349803805351 Accuracy 0.6000000238418579 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.753, dtype=float32), array(0.886, dtype=float32), array(0.874, dtype=float32), array(0.996, dtype=float32), array(0.559, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30490 Training loss 0.035780832171440125 Validation loss 0.039628058671951294 Accuracy 0.5974000096321106 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.717, dtype=float32), array(0.91, dtype=float32), array(0.865, dtype=float32), array(0.996, dtype=float32), array(0.555, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30500 Training loss 0.03696518391370773 Validation loss 0.03908190503716469 Accuracy 0.6029000282287598 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.745, dtype=float32), array(0.939, dtype=float32), array(0.808, dtype=float32), array(0.995, dtype=float32), array(0.61, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30510 Training loss 0.038785263895988464 Validation loss 0.038670703768730164 Accuracy 0.608299970626831 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.79, dtype=float32), array(0.896, dtype=float32), array(0.751, dtype=float32), array(0.998, dtype=float32), array(0.725, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30520 Training loss 0.039227306842803955 Validation loss 0.03922196850180626 Accuracy 0.6019999980926514 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.831, dtype=float32), array(0.889, dtype=float32), array(0.808, dtype=float32), array(0.996, dtype=float32), array(0.57, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30530 Training loss 0.03731473535299301 Validation loss 0.03863174840807915 Accuracy 0.6074000000953674 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.786, dtype=float32), array(0.911, dtype=float32), array(0.822, dtype=float32), array(0.993, dtype=float32), array(0.631, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30540 Training loss 0.0368366576731205 Validation loss 0.03903044015169144 Accuracy 0.6043999791145325 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.864, dtype=float32), array(0.869, dtype=float32), array(0.75, dtype=float32), array(0.996, dtype=float32), array(0.637, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30550 Training loss 0.03656133636832237 Validation loss 0.0383295938372612 Accuracy 0.6101999878883362 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.814, dtype=float32), array(0.909, dtype=float32), array(0.777, dtype=float32), array(0.994, dtype=float32), array(0.687, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30560 Training loss 0.03772822022438049 Validation loss 0.038843654096126556 Accuracy 0.6057000160217285 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.862, dtype=float32), array(0.892, dtype=float32), array(0.713, dtype=float32), array(0.998, dtype=float32), array(0.662, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30570 Training loss 0.03935573250055313 Validation loss 0.03952541947364807 Accuracy 0.5981000065803528 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.706, dtype=float32), array(0.869, dtype=float32), array(0.892, dtype=float32), array(0.992, dtype=float32), array(0.575, dtype=float32), array(0., dtype=float32), array(0.982, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30580 Training loss 0.03377664089202881 Validation loss 0.038976989686489105 Accuracy 0.6032000184059143 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.77, dtype=float32), array(0.916, dtype=float32), array(0.792, dtype=float32), array(0.994, dtype=float32), array(0.614, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30590 Training loss 0.03750668093562126 Validation loss 0.03876796364784241 Accuracy 0.6065000295639038 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.819, dtype=float32), array(0.891, dtype=float32), array(0.683, dtype=float32), array(0.994, dtype=float32), array(0.735, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30600 Training loss 0.04081793874502182 Validation loss 0.039287883788347244 Accuracy 0.6013000011444092 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.733, dtype=float32), array(0.879, dtype=float32), array(0.872, dtype=float32), array(0.994, dtype=float32), array(0.583, dtype=float32), array(0., dtype=float32), array(0.983, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30610 Training loss 0.03758757933974266 Validation loss 0.03901207074522972 Accuracy 0.6039999723434448 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.725, dtype=float32), array(0.898, dtype=float32), array(0.727, dtype=float32), array(0.989, dtype=float32), array(0.764, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30620 Training loss 0.039368484169244766 Validation loss 0.03862757235765457 Accuracy 0.6079999804496765 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.846, dtype=float32), array(0.87, dtype=float32), array(0.747, dtype=float32), array(0.991, dtype=float32), array(0.686, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30630 Training loss 0.04106375202536583 Validation loss 0.03851383179426193 Accuracy 0.6093000173568726 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.697, dtype=float32), array(0.921, dtype=float32), array(0.828, dtype=float32), array(0.999, dtype=float32), array(0.716, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30640 Training loss 0.035009123384952545 Validation loss 0.0388626791536808 Accuracy 0.6049000024795532 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.861, dtype=float32), array(0.928, dtype=float32), array(0.69, dtype=float32), array(0.995, dtype=float32), array(0.646, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30650 Training loss 0.035842955112457275 Validation loss 0.03849415481090546 Accuracy 0.6096000075340271 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.808, dtype=float32), array(0.876, dtype=float32), array(0.779, dtype=float32), array(0.996, dtype=float32), array(0.705, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30660 Training loss 0.038713227957487106 Validation loss 0.03854810446500778 Accuracy 0.6079000234603882 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.736, dtype=float32), array(0.898, dtype=float32), array(0.847, dtype=float32), array(0.996, dtype=float32), array(0.662, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30670 Training loss 0.04006342962384224 Validation loss 0.039000820368528366 Accuracy 0.6039000153541565 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.712, dtype=float32), array(0.9, dtype=float32), array(0.853, dtype=float32), array(0.997, dtype=float32), array(0.642, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30680 Training loss 0.03732399642467499 Validation loss 0.03836582973599434 Accuracy 0.6097000241279602 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.82, dtype=float32), array(0.894, dtype=float32), array(0.813, dtype=float32), array(0.995, dtype=float32), array(0.647, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30690 Training loss 0.03320594131946564 Validation loss 0.03851611912250519 Accuracy 0.6098999977111816 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.725, dtype=float32), array(0.881, dtype=float32), array(0.838, dtype=float32), array(0.992, dtype=float32), array(0.732, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30700 Training loss 0.0394049696624279 Validation loss 0.03873994201421738 Accuracy 0.6061000227928162 Accuracies by class [array(0., dtype=float32), array(0.948, dtype=float32), array(0.797, dtype=float32), array(0.938, dtype=float32), array(0.709, dtype=float32), array(0.993, dtype=float32), array(0.701, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30710 Training loss 0.03510903939604759 Validation loss 0.038607992231845856 Accuracy 0.6078000068664551 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.8, dtype=float32), array(0.909, dtype=float32), array(0.737, dtype=float32), array(0.981, dtype=float32), array(0.713, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30720 Training loss 0.03725269064307213 Validation loss 0.03921478986740112 Accuracy 0.6018999814987183 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.842, dtype=float32), array(0.95, dtype=float32), array(0.722, dtype=float32), array(0.996, dtype=float32), array(0.583, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30730 Training loss 0.037777457386255264 Validation loss 0.03956904262304306 Accuracy 0.5964000225067139 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.878, dtype=float32), array(0.913, dtype=float32), array(0.561, dtype=float32), array(0.994, dtype=float32), array(0.701, dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30740 Training loss 0.038434188812971115 Validation loss 0.03974341228604317 Accuracy 0.5954999923706055 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.877, dtype=float32), array(0.929, dtype=float32), array(0.559, dtype=float32), array(0.982, dtype=float32), array(0.676, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30750 Training loss 0.03921149671077728 Validation loss 0.03874549642205238 Accuracy 0.607699990272522 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.806, dtype=float32), array(0.876, dtype=float32), array(0.767, dtype=float32), array(0.989, dtype=float32), array(0.704, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30760 Training loss 0.03930063918232918 Validation loss 0.03894311562180519 Accuracy 0.6032999753952026 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.848, dtype=float32), array(0.936, dtype=float32), array(0.713, dtype=float32), array(0.992, dtype=float32), array(0.615, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30770 Training loss 0.035825833678245544 Validation loss 0.038500890135765076 Accuracy 0.6075999736785889 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.764, dtype=float32), array(0.923, dtype=float32), array(0.795, dtype=float32), array(0.995, dtype=float32), array(0.675, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30780 Training loss 0.03916516527533531 Validation loss 0.03907415643334389 Accuracy 0.6028000116348267 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.874, dtype=float32), array(0.901, dtype=float32), array(0.702, dtype=float32), array(0.989, dtype=float32), array(0.63, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30790 Training loss 0.03584251552820206 Validation loss 0.03843230381608009 Accuracy 0.608299970626831 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.781, dtype=float32), array(0.872, dtype=float32), array(0.805, dtype=float32), array(0.995, dtype=float32), array(0.696, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30800 Training loss 0.03803804889321327 Validation loss 0.03888213634490967 Accuracy 0.6050999760627747 Accuracies by class [array(0., dtype=float32), array(0.974, dtype=float32), array(0.752, dtype=float32), array(0.87, dtype=float32), array(0.774, dtype=float32), array(0.995, dtype=float32), array(0.722, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30810 Training loss 0.045326102524995804 Validation loss 0.039081353694200516 Accuracy 0.6029999852180481 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.752, dtype=float32), array(0.853, dtype=float32), array(0.867, dtype=float32), array(0.995, dtype=float32), array(0.628, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30820 Training loss 0.037704385817050934 Validation loss 0.03874632716178894 Accuracy 0.6057999730110168 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.793, dtype=float32), array(0.844, dtype=float32), array(0.829, dtype=float32), array(0.988, dtype=float32), array(0.659, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30830 Training loss 0.03552604094147682 Validation loss 0.038766734302043915 Accuracy 0.6060000061988831 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.848, dtype=float32), array(0.879, dtype=float32), array(0.712, dtype=float32), array(0.995, dtype=float32), array(0.717, dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30840 Training loss 0.0376344695687294 Validation loss 0.03947397693991661 Accuracy 0.599399983882904 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.742, dtype=float32), array(0.898, dtype=float32), array(0.892, dtype=float32), array(0.995, dtype=float32), array(0.549, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30850 Training loss 0.03692535310983658 Validation loss 0.03903413191437721 Accuracy 0.6021999716758728 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.804, dtype=float32), array(0.96, dtype=float32), array(0.734, dtype=float32), array(0.99, dtype=float32), array(0.619, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30860 Training loss 0.03857199102640152 Validation loss 0.039575159549713135 Accuracy 0.5964999794960022 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.877, dtype=float32), array(0.916, dtype=float32), array(0.545, dtype=float32), array(0.991, dtype=float32), array(0.7, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30870 Training loss 0.03451699763536453 Validation loss 0.03867149353027344 Accuracy 0.6079000234603882 Accuracies by class [array(0., dtype=float32), array(0.972, dtype=float32), array(0.853, dtype=float32), array(0.875, dtype=float32), array(0.737, dtype=float32), array(0.994, dtype=float32), array(0.676, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30880 Training loss 0.035409145057201385 Validation loss 0.04055352881550789 Accuracy 0.5867999792098999 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.866, dtype=float32), array(0.902, dtype=float32), array(0.443, dtype=float32), array(0.99, dtype=float32), array(0.747, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30890 Training loss 0.034631501883268356 Validation loss 0.038856346160173416 Accuracy 0.6039000153541565 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.736, dtype=float32), array(0.888, dtype=float32), array(0.86, dtype=float32), array(0.983, dtype=float32), array(0.644, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30900 Training loss 0.041478585451841354 Validation loss 0.03898871690034866 Accuracy 0.605400025844574 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.774, dtype=float32), array(0.909, dtype=float32), array(0.837, dtype=float32), array(0.968, dtype=float32), array(0.631, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30910 Training loss 0.0371650867164135 Validation loss 0.03853378817439079 Accuracy 0.6078000068664551 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.806, dtype=float32), array(0.863, dtype=float32), array(0.8, dtype=float32), array(0.981, dtype=float32), array(0.698, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30920 Training loss 0.036005567759275436 Validation loss 0.03871820494532585 Accuracy 0.6054999828338623 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.755, dtype=float32), array(0.847, dtype=float32), array(0.812, dtype=float32), array(0.994, dtype=float32), array(0.725, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30930 Training loss 0.03587358072400093 Validation loss 0.03840307518839836 Accuracy 0.6075999736785889 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.8, dtype=float32), array(0.908, dtype=float32), array(0.689, dtype=float32), array(0.993, dtype=float32), array(0.755, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30940 Training loss 0.03969339653849602 Validation loss 0.03864940628409386 Accuracy 0.6054999828338623 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.713, dtype=float32), array(0.862, dtype=float32), array(0.863, dtype=float32), array(0.993, dtype=float32), array(0.689, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30950 Training loss 0.036946967244148254 Validation loss 0.039215318858623505 Accuracy 0.6007000207901001 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.745, dtype=float32), array(0.947, dtype=float32), array(0.773, dtype=float32), array(0.979, dtype=float32), array(0.63, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30960 Training loss 0.03837953507900238 Validation loss 0.03868561610579491 Accuracy 0.6055999994277954 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.724, dtype=float32), array(0.857, dtype=float32), array(0.834, dtype=float32), array(0.987, dtype=float32), array(0.71, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30970 Training loss 0.037701141089200974 Validation loss 0.03919096663594246 Accuracy 0.6011999845504761 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.695, dtype=float32), array(0.875, dtype=float32), array(0.751, dtype=float32), array(0.994, dtype=float32), array(0.774, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30980 Training loss 0.04040367156267166 Validation loss 0.038632143288850784 Accuracy 0.6075999736785889 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.788, dtype=float32), array(0.885, dtype=float32), array(0.76, dtype=float32), array(0.977, dtype=float32), array(0.737, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 30990 Training loss 0.03602368012070656 Validation loss 0.03868158534169197 Accuracy 0.6055999994277954 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.733, dtype=float32), array(0.871, dtype=float32), array(0.831, dtype=float32), array(0.981, dtype=float32), array(0.717, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31000 Training loss 0.038595933467149734 Validation loss 0.038724109530448914 Accuracy 0.6054999828338623 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.805, dtype=float32), array(0.878, dtype=float32), array(0.825, dtype=float32), array(0.995, dtype=float32), array(0.616, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31010 Training loss 0.03487800061702728 Validation loss 0.038416627794504166 Accuracy 0.6079000234603882 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.791, dtype=float32), array(0.9, dtype=float32), array(0.765, dtype=float32), array(0.987, dtype=float32), array(0.709, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31020 Training loss 0.036198265850543976 Validation loss 0.04001667723059654 Accuracy 0.5949000120162964 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.765, dtype=float32), array(0.897, dtype=float32), array(0.589, dtype=float32), array(0.997, dtype=float32), array(0.817, dtype=float32), array(0., dtype=float32), array(0.927, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31030 Training loss 0.035779621452093124 Validation loss 0.03855879232287407 Accuracy 0.6096000075340271 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.784, dtype=float32), array(0.898, dtype=float32), array(0.74, dtype=float32), array(0.985, dtype=float32), array(0.773, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31040 Training loss 0.033923082053661346 Validation loss 0.038373589515686035 Accuracy 0.6090999841690063 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.806, dtype=float32), array(0.864, dtype=float32), array(0.763, dtype=float32), array(0.986, dtype=float32), array(0.743, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31050 Training loss 0.0327400304377079 Validation loss 0.038200125098228455 Accuracy 0.6104000210762024 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.819, dtype=float32), array(0.903, dtype=float32), array(0.755, dtype=float32), array(0.994, dtype=float32), array(0.708, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31060 Training loss 0.036027390509843826 Validation loss 0.03842811658978462 Accuracy 0.6097000241279602 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.776, dtype=float32), array(0.864, dtype=float32), array(0.826, dtype=float32), array(0.994, dtype=float32), array(0.707, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31070 Training loss 0.03731607645750046 Validation loss 0.038416165858507156 Accuracy 0.6078000068664551 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.849, dtype=float32), array(0.888, dtype=float32), array(0.767, dtype=float32), array(0.992, dtype=float32), array(0.652, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31080 Training loss 0.03844108432531357 Validation loss 0.03862730786204338 Accuracy 0.6050999760627747 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.817, dtype=float32), array(0.826, dtype=float32), array(0.72, dtype=float32), array(0.992, dtype=float32), array(0.765, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31090 Training loss 0.036371756345033646 Validation loss 0.038670822978019714 Accuracy 0.6055999994277954 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.877, dtype=float32), array(0.894, dtype=float32), array(0.727, dtype=float32), array(0.989, dtype=float32), array(0.642, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31100 Training loss 0.03625176474452019 Validation loss 0.03898215666413307 Accuracy 0.6050000190734863 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.846, dtype=float32), array(0.922, dtype=float32), array(0.772, dtype=float32), array(0.995, dtype=float32), array(0.608, dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31110 Training loss 0.039005495607852936 Validation loss 0.03890616446733475 Accuracy 0.6049000024795532 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.835, dtype=float32), array(0.944, dtype=float32), array(0.694, dtype=float32), array(0.995, dtype=float32), array(0.669, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31120 Training loss 0.0340317040681839 Validation loss 0.03839784488081932 Accuracy 0.6092000007629395 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.729, dtype=float32), array(0.889, dtype=float32), array(0.827, dtype=float32), array(0.987, dtype=float32), array(0.724, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31130 Training loss 0.038095153868198395 Validation loss 0.03878138214349747 Accuracy 0.6040999889373779 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.836, dtype=float32), array(0.919, dtype=float32), array(0.814, dtype=float32), array(0.99, dtype=float32), array(0.563, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31140 Training loss 0.0379546582698822 Validation loss 0.03815372660756111 Accuracy 0.6096000075340271 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.834, dtype=float32), array(0.893, dtype=float32), array(0.773, dtype=float32), array(0.991, dtype=float32), array(0.692, dtype=float32), array(0., dtype=float32), array(0.947, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31150 Training loss 0.040511392056941986 Validation loss 0.03873399272561073 Accuracy 0.6039999723434448 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.877, dtype=float32), array(0.915, dtype=float32), array(0.677, dtype=float32), array(0.991, dtype=float32), array(0.658, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31160 Training loss 0.039292819797992706 Validation loss 0.03918229416012764 Accuracy 0.6021999716758728 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.784, dtype=float32), array(0.916, dtype=float32), array(0.716, dtype=float32), array(0.963, dtype=float32), array(0.709, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31170 Training loss 0.04028337076306343 Validation loss 0.03920266032218933 Accuracy 0.6029000282287598 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.656, dtype=float32), array(0.875, dtype=float32), array(0.809, dtype=float32), array(0.994, dtype=float32), array(0.78, dtype=float32), array(0., dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31180 Training loss 0.04029693454504013 Validation loss 0.03883945196866989 Accuracy 0.6046000123023987 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.684, dtype=float32), array(0.883, dtype=float32), array(0.785, dtype=float32), array(0.993, dtype=float32), array(0.785, dtype=float32), array(0., dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31190 Training loss 0.03830718994140625 Validation loss 0.038200438022613525 Accuracy 0.6100000143051147 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.772, dtype=float32), array(0.926, dtype=float32), array(0.784, dtype=float32), array(0.992, dtype=float32), array(0.693, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31200 Training loss 0.0399947352707386 Validation loss 0.03913625329732895 Accuracy 0.6021000146865845 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.774, dtype=float32), array(0.909, dtype=float32), array(0.849, dtype=float32), array(0.986, dtype=float32), array(0.566, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31210 Training loss 0.03902232274413109 Validation loss 0.03834233805537224 Accuracy 0.6086000204086304 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.834, dtype=float32), array(0.883, dtype=float32), array(0.692, dtype=float32), array(0.994, dtype=float32), array(0.751, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31220 Training loss 0.03607848286628723 Validation loss 0.038288481533527374 Accuracy 0.6097999811172485 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.848, dtype=float32), array(0.915, dtype=float32), array(0.748, dtype=float32), array(0.986, dtype=float32), array(0.664, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31230 Training loss 0.036698371171951294 Validation loss 0.03819142282009125 Accuracy 0.61080002784729 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.818, dtype=float32), array(0.885, dtype=float32), array(0.765, dtype=float32), array(0.982, dtype=float32), array(0.722, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31240 Training loss 0.03588111698627472 Validation loss 0.03819349780678749 Accuracy 0.6093999743461609 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.825, dtype=float32), array(0.928, dtype=float32), array(0.744, dtype=float32), array(0.994, dtype=float32), array(0.683, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31250 Training loss 0.03941886126995087 Validation loss 0.03940923884510994 Accuracy 0.5989000201225281 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.629, dtype=float32), array(0.835, dtype=float32), array(0.86, dtype=float32), array(0.974, dtype=float32), array(0.755, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31260 Training loss 0.039071161299943924 Validation loss 0.03889273852109909 Accuracy 0.6032999753952026 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.659, dtype=float32), array(0.921, dtype=float32), array(0.852, dtype=float32), array(0.993, dtype=float32), array(0.67, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31270 Training loss 0.03602031618356705 Validation loss 0.038883551955223083 Accuracy 0.6061000227928162 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.839, dtype=float32), array(0.929, dtype=float32), array(0.701, dtype=float32), array(0.998, dtype=float32), array(0.672, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31280 Training loss 0.038387514650821686 Validation loss 0.03904549777507782 Accuracy 0.6008999943733215 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.819, dtype=float32), array(0.895, dtype=float32), array(0.822, dtype=float32), array(0.99, dtype=float32), array(0.551, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31290 Training loss 0.033355746418237686 Validation loss 0.038384146988391876 Accuracy 0.6093999743461609 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.799, dtype=float32), array(0.875, dtype=float32), array(0.768, dtype=float32), array(0.995, dtype=float32), array(0.724, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31300 Training loss 0.037693366408348083 Validation loss 0.038927383720874786 Accuracy 0.6031000018119812 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.869, dtype=float32), array(0.897, dtype=float32), array(0.708, dtype=float32), array(0.995, dtype=float32), array(0.637, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31310 Training loss 0.040571946650743484 Validation loss 0.03941695764660835 Accuracy 0.5982999801635742 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.686, dtype=float32), array(0.876, dtype=float32), array(0.895, dtype=float32), array(0.981, dtype=float32), array(0.599, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31320 Training loss 0.03467101976275444 Validation loss 0.03857141360640526 Accuracy 0.6053000092506409 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.73, dtype=float32), array(0.913, dtype=float32), array(0.851, dtype=float32), array(0.982, dtype=float32), array(0.645, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31330 Training loss 0.03735515475273132 Validation loss 0.0385592058300972 Accuracy 0.607200026512146 Accuracies by class [array(0., dtype=float32), array(0.975, dtype=float32), array(0.84, dtype=float32), array(0.83, dtype=float32), array(0.775, dtype=float32), array(0.986, dtype=float32), array(0.723, dtype=float32), array(0., dtype=float32), array(0.943, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31340 Training loss 0.0370943509042263 Validation loss 0.03940288722515106 Accuracy 0.597599983215332 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.883, dtype=float32), array(0.892, dtype=float32), array(0.586, dtype=float32), array(0.985, dtype=float32), array(0.72, dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31350 Training loss 0.04074489697813988 Validation loss 0.039016030728816986 Accuracy 0.6019999980926514 Accuracies by class [array(0., dtype=float32), array(0.958, dtype=float32), array(0.896, dtype=float32), array(0.899, dtype=float32), array(0.674, dtype=float32), array(0.993, dtype=float32), array(0.627, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31360 Training loss 0.04021516814827919 Validation loss 0.038524579256772995 Accuracy 0.6071000099182129 Accuracies by class [array(0., dtype=float32), array(0.951, dtype=float32), array(0.676, dtype=float32), array(0.888, dtype=float32), array(0.857, dtype=float32), array(0.991, dtype=float32), array(0.754, dtype=float32), array(0., dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31370 Training loss 0.03770985081791878 Validation loss 0.03805616497993469 Accuracy 0.611299991607666 Accuracies by class [array(0., dtype=float32), array(0.955, dtype=float32), array(0.767, dtype=float32), array(0.896, dtype=float32), array(0.823, dtype=float32), array(0.991, dtype=float32), array(0.711, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31380 Training loss 0.03126833960413933 Validation loss 0.03801392391324043 Accuracy 0.6107000112533569 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.853, dtype=float32), array(0.878, dtype=float32), array(0.739, dtype=float32), array(0.991, dtype=float32), array(0.71, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31390 Training loss 0.0359622947871685 Validation loss 0.03809179738163948 Accuracy 0.6104999780654907 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.819, dtype=float32), array(0.85, dtype=float32), array(0.789, dtype=float32), array(0.989, dtype=float32), array(0.715, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31400 Training loss 0.037890784442424774 Validation loss 0.03849136829376221 Accuracy 0.6061000227928162 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.821, dtype=float32), array(0.912, dtype=float32), array(0.839, dtype=float32), array(0.993, dtype=float32), array(0.567, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31410 Training loss 0.03786132484674454 Validation loss 0.03802725672721863 Accuracy 0.61080002784729 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.857, dtype=float32), array(0.888, dtype=float32), array(0.76, dtype=float32), array(0.994, dtype=float32), array(0.695, dtype=float32), array(0., dtype=float32), array(0.95, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31420 Training loss 0.03825291991233826 Validation loss 0.03849416971206665 Accuracy 0.60589998960495 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.78, dtype=float32), array(0.863, dtype=float32), array(0.863, dtype=float32), array(0.994, dtype=float32), array(0.626, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31430 Training loss 0.04095328599214554 Validation loss 0.03969613462686539 Accuracy 0.5942000150680542 Accuracies by class [array(0., dtype=float32), array(0.954, dtype=float32), array(0.592, dtype=float32), array(0.929, dtype=float32), array(0.893, dtype=float32), array(0.987, dtype=float32), array(0.624, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31440 Training loss 0.03568749129772186 Validation loss 0.03859274834394455 Accuracy 0.6079000234603882 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.795, dtype=float32), array(0.916, dtype=float32), array(0.796, dtype=float32), array(0.969, dtype=float32), array(0.659, dtype=float32), array(0., dtype=float32), array(0.98, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31450 Training loss 0.03632504120469093 Validation loss 0.03800161927938461 Accuracy 0.6132000088691711 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.801, dtype=float32), array(0.905, dtype=float32), array(0.8, dtype=float32), array(0.998, dtype=float32), array(0.696, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31460 Training loss 0.0365864597260952 Validation loss 0.03813427686691284 Accuracy 0.6103000044822693 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.812, dtype=float32), array(0.911, dtype=float32), array(0.751, dtype=float32), array(0.994, dtype=float32), array(0.708, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31470 Training loss 0.03344288468360901 Validation loss 0.03811635822057724 Accuracy 0.6096000075340271 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.805, dtype=float32), array(0.897, dtype=float32), array(0.768, dtype=float32), array(0.994, dtype=float32), array(0.695, dtype=float32), array(0., dtype=float32), array(0.977, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31480 Training loss 0.036195069551467896 Validation loss 0.03869442641735077 Accuracy 0.6028000116348267 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.76, dtype=float32), array(0.815, dtype=float32), array(0.808, dtype=float32), array(0.991, dtype=float32), array(0.715, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31490 Training loss 0.036049723625183105 Validation loss 0.03818626329302788 Accuracy 0.6089000105857849 Accuracies by class [array(0., dtype=float32), array(0.953, dtype=float32), array(0.78, dtype=float32), array(0.906, dtype=float32), array(0.777, dtype=float32), array(0.993, dtype=float32), array(0.707, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31500 Training loss 0.034758299589157104 Validation loss 0.038354046642780304 Accuracy 0.608299970626831 Accuracies by class [array(0., dtype=float32), array(0.956, dtype=float32), array(0.691, dtype=float32), array(0.893, dtype=float32), array(0.833, dtype=float32), array(0.993, dtype=float32), array(0.748, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31510 Training loss 0.036766186356544495 Validation loss 0.03819410130381584 Accuracy 0.6101999878883362 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.842, dtype=float32), array(0.891, dtype=float32), array(0.795, dtype=float32), array(0.99, dtype=float32), array(0.65, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31520 Training loss 0.03689880296587944 Validation loss 0.03837020322680473 Accuracy 0.6061000227928162 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.836, dtype=float32), array(0.897, dtype=float32), array(0.805, dtype=float32), array(0.991, dtype=float32), array(0.592, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31530 Training loss 0.038436539471149445 Validation loss 0.038306910544633865 Accuracy 0.6089000105857849 Accuracies by class [array(0., dtype=float32), array(0.95, dtype=float32), array(0.819, dtype=float32), array(0.907, dtype=float32), array(0.696, dtype=float32), array(0.993, dtype=float32), array(0.753, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31540 Training loss 0.03642069920897484 Validation loss 0.03813248127698898 Accuracy 0.6085000038146973 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.79, dtype=float32), array(0.903, dtype=float32), array(0.841, dtype=float32), array(0.992, dtype=float32), array(0.628, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31550 Training loss 0.036813851445913315 Validation loss 0.03797588497400284 Accuracy 0.6107000112533569 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.817, dtype=float32), array(0.912, dtype=float32), array(0.787, dtype=float32), array(0.992, dtype=float32), array(0.663, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31560 Training loss 0.03744852542877197 Validation loss 0.0379180945456028 Accuracy 0.6104999780654907 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.832, dtype=float32), array(0.904, dtype=float32), array(0.77, dtype=float32), array(0.992, dtype=float32), array(0.676, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31570 Training loss 0.03875560685992241 Validation loss 0.03846968337893486 Accuracy 0.6064000129699707 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.855, dtype=float32), array(0.848, dtype=float32), array(0.705, dtype=float32), array(0.986, dtype=float32), array(0.732, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31580 Training loss 0.041698336601257324 Validation loss 0.038705699145793915 Accuracy 0.6032000184059143 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.669, dtype=float32), array(0.847, dtype=float32), array(0.892, dtype=float32), array(0.99, dtype=float32), array(0.696, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31590 Training loss 0.03771640732884407 Validation loss 0.03809612989425659 Accuracy 0.611299991607666 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.829, dtype=float32), array(0.917, dtype=float32), array(0.791, dtype=float32), array(0.989, dtype=float32), array(0.654, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31600 Training loss 0.036091335117816925 Validation loss 0.03821124508976936 Accuracy 0.6075999736785889 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.836, dtype=float32), array(0.895, dtype=float32), array(0.804, dtype=float32), array(0.988, dtype=float32), array(0.611, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31610 Training loss 0.04171973094344139 Validation loss 0.040189988911151886 Accuracy 0.5861999988555908 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.789, dtype=float32), array(0.889, dtype=float32), array(0.468, dtype=float32), array(0.992, dtype=float32), array(0.809, dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31620 Training loss 0.036457788199186325 Validation loss 0.03803107514977455 Accuracy 0.6090999841690063 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.814, dtype=float32), array(0.896, dtype=float32), array(0.75, dtype=float32), array(0.985, dtype=float32), array(0.706, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31630 Training loss 0.033135466277599335 Validation loss 0.03805045783519745 Accuracy 0.6086000204086304 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.838, dtype=float32), array(0.86, dtype=float32), array(0.779, dtype=float32), array(0.995, dtype=float32), array(0.67, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31640 Training loss 0.03503860533237457 Validation loss 0.03790482506155968 Accuracy 0.6122000217437744 Accuracies by class [array(0., dtype=float32), array(0.962, dtype=float32), array(0.828, dtype=float32), array(0.918, dtype=float32), array(0.775, dtype=float32), array(0.995, dtype=float32), array(0.671, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31650 Training loss 0.03457709774374962 Validation loss 0.03780040144920349 Accuracy 0.6115999817848206 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.809, dtype=float32), array(0.898, dtype=float32), array(0.791, dtype=float32), array(0.994, dtype=float32), array(0.683, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31660 Training loss 0.038365814834833145 Validation loss 0.03859024867415428 Accuracy 0.6065000295639038 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.803, dtype=float32), array(0.921, dtype=float32), array(0.821, dtype=float32), array(0.965, dtype=float32), array(0.614, dtype=float32), array(0., dtype=float32), array(0.978, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31670 Training loss 0.033963192254304886 Validation loss 0.038634225726127625 Accuracy 0.6050999760627747 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.76, dtype=float32), array(0.863, dtype=float32), array(0.871, dtype=float32), array(0.989, dtype=float32), array(0.629, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31680 Training loss 0.03558903560042381 Validation loss 0.037761688232421875 Accuracy 0.61080002784729 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.828, dtype=float32), array(0.917, dtype=float32), array(0.735, dtype=float32), array(0.99, dtype=float32), array(0.704, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31690 Training loss 0.04009325057268143 Validation loss 0.037826213985681534 Accuracy 0.6093000173568726 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.773, dtype=float32), array(0.901, dtype=float32), array(0.748, dtype=float32), array(0.99, dtype=float32), array(0.746, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31700 Training loss 0.03849705681204796 Validation loss 0.03952401503920555 Accuracy 0.5971999764442444 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.78, dtype=float32), array(0.906, dtype=float32), array(0.566, dtype=float32), array(0.996, dtype=float32), array(0.8, dtype=float32), array(0., dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31710 Training loss 0.036317359656095505 Validation loss 0.038539860397577286 Accuracy 0.6019999980926514 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.805, dtype=float32), array(0.888, dtype=float32), array(0.621, dtype=float32), array(0.989, dtype=float32), array(0.797, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31720 Training loss 0.03529118001461029 Validation loss 0.038899291306734085 Accuracy 0.599399983882904 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.892, dtype=float32), array(0.905, dtype=float32), array(0.703, dtype=float32), array(0.99, dtype=float32), array(0.569, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31730 Training loss 0.03693927824497223 Validation loss 0.03795529901981354 Accuracy 0.6093000173568726 Accuracies by class [array(0., dtype=float32), array(0.961, dtype=float32), array(0.777, dtype=float32), array(0.926, dtype=float32), array(0.805, dtype=float32), array(0.996, dtype=float32), array(0.653, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31740 Training loss 0.03772614896297455 Validation loss 0.03754645586013794 Accuracy 0.6122999787330627 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.818, dtype=float32), array(0.9, dtype=float32), array(0.791, dtype=float32), array(0.99, dtype=float32), array(0.684, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31750 Training loss 0.039257243275642395 Validation loss 0.0377366878092289 Accuracy 0.6104000210762024 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.776, dtype=float32), array(0.918, dtype=float32), array(0.791, dtype=float32), array(0.992, dtype=float32), array(0.689, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31760 Training loss 0.03452033922076225 Validation loss 0.037945155054330826 Accuracy 0.6096000075340271 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.79, dtype=float32), array(0.888, dtype=float32), array(0.826, dtype=float32), array(0.99, dtype=float32), array(0.669, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31770 Training loss 0.03810940310359001 Validation loss 0.03833630308508873 Accuracy 0.6046000123023987 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.733, dtype=float32), array(0.905, dtype=float32), array(0.712, dtype=float32), array(0.991, dtype=float32), array(0.771, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31780 Training loss 0.035963863134384155 Validation loss 0.03871804475784302 Accuracy 0.6004999876022339 Accuracies by class [array(0., dtype=float32), array(0.973, dtype=float32), array(0.727, dtype=float32), array(0.862, dtype=float32), array(0.894, dtype=float32), array(0.991, dtype=float32), array(0.594, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31790 Training loss 0.038728244602680206 Validation loss 0.0378975048661232 Accuracy 0.6067000031471252 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.751, dtype=float32), array(0.924, dtype=float32), array(0.846, dtype=float32), array(0.991, dtype=float32), array(0.625, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31800 Training loss 0.03599978983402252 Validation loss 0.03809085860848427 Accuracy 0.6075000166893005 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.793, dtype=float32), array(0.927, dtype=float32), array(0.707, dtype=float32), array(0.994, dtype=float32), array(0.723, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31810 Training loss 0.03336557373404503 Validation loss 0.037744492292404175 Accuracy 0.6093999743461609 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.773, dtype=float32), array(0.889, dtype=float32), array(0.835, dtype=float32), array(0.987, dtype=float32), array(0.675, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31820 Training loss 0.036227136850357056 Validation loss 0.03815961256623268 Accuracy 0.6050999760627747 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.862, dtype=float32), array(0.924, dtype=float32), array(0.652, dtype=float32), array(0.989, dtype=float32), array(0.687, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31830 Training loss 0.03578535094857216 Validation loss 0.03777480870485306 Accuracy 0.6068999767303467 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.686, dtype=float32), array(0.905, dtype=float32), array(0.822, dtype=float32), array(0.99, dtype=float32), array(0.729, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31840 Training loss 0.03443564102053642 Validation loss 0.03807713836431503 Accuracy 0.6061999797821045 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.684, dtype=float32), array(0.916, dtype=float32), array(0.8, dtype=float32), array(0.991, dtype=float32), array(0.742, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31850 Training loss 0.033918172121047974 Validation loss 0.037386324256658554 Accuracy 0.6101999878883362 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.782, dtype=float32), array(0.897, dtype=float32), array(0.761, dtype=float32), array(0.989, dtype=float32), array(0.757, dtype=float32), array(0., dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31860 Training loss 0.038739390671253204 Validation loss 0.0385630801320076 Accuracy 0.6003999710083008 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.83, dtype=float32), array(0.918, dtype=float32), array(0.816, dtype=float32), array(0.986, dtype=float32), array(0.531, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31870 Training loss 0.03742337226867676 Validation loss 0.0380396731197834 Accuracy 0.6092000007629395 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.744, dtype=float32), array(0.903, dtype=float32), array(0.77, dtype=float32), array(0.994, dtype=float32), array(0.75, dtype=float32), array(0., dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31880 Training loss 0.03621157258749008 Validation loss 0.037659235298633575 Accuracy 0.6092000007629395 Accuracies by class [array(0., dtype=float32), array(0.971, dtype=float32), array(0.824, dtype=float32), array(0.843, dtype=float32), array(0.801, dtype=float32), array(0.992, dtype=float32), array(0.687, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31890 Training loss 0.038150638341903687 Validation loss 0.03839564695954323 Accuracy 0.6004999876022339 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.716, dtype=float32), array(0.894, dtype=float32), array(0.883, dtype=float32), array(0.987, dtype=float32), array(0.588, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31900 Training loss 0.038953766226768494 Validation loss 0.03741862624883652 Accuracy 0.6118999719619751 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.716, dtype=float32), array(0.913, dtype=float32), array(0.81, dtype=float32), array(0.978, dtype=float32), array(0.765, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31910 Training loss 0.03640628978610039 Validation loss 0.0385810062289238 Accuracy 0.5990999937057495 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.719, dtype=float32), array(0.879, dtype=float32), array(0.881, dtype=float32), array(0.989, dtype=float32), array(0.613, dtype=float32), array(0., dtype=float32), array(0.94, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31920 Training loss 0.035542506724596024 Validation loss 0.03748665377497673 Accuracy 0.6107000112533569 Accuracies by class [array(0., dtype=float32), array(0.969, dtype=float32), array(0.808, dtype=float32), array(0.883, dtype=float32), array(0.803, dtype=float32), array(0.989, dtype=float32), array(0.698, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31930 Training loss 0.03551299124956131 Validation loss 0.03897877782583237 Accuracy 0.5946000218391418 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.48, dtype=float32), array(0.877, dtype=float32), array(0.885, dtype=float32), array(0.986, dtype=float32), array(0.774, dtype=float32), array(0., dtype=float32), array(0.976, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31940 Training loss 0.035105783492326736 Validation loss 0.037807025015354156 Accuracy 0.6075000166893005 Accuracies by class [array(0., dtype=float32), array(0.966, dtype=float32), array(0.838, dtype=float32), array(0.901, dtype=float32), array(0.803, dtype=float32), array(0.986, dtype=float32), array(0.625, dtype=float32), array(0., dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31950 Training loss 0.03647947311401367 Validation loss 0.037631627172231674 Accuracy 0.6096000075340271 Accuracies by class [array(0., dtype=float32), array(0.957, dtype=float32), array(0.812, dtype=float32), array(0.915, dtype=float32), array(0.752, dtype=float32), array(0.991, dtype=float32), array(0.723, dtype=float32), array(0., dtype=float32), array(0.946, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31960 Training loss 0.03917517513036728 Validation loss 0.038224633783102036 Accuracy 0.6075999736785889 Accuracies by class [array(0., dtype=float32), array(0.959, dtype=float32), array(0.86, dtype=float32), array(0.915, dtype=float32), array(0.698, dtype=float32), array(0.996, dtype=float32), array(0.688, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31970 Training loss 0.034094106405973434 Validation loss 0.03732369840145111 Accuracy 0.6122999787330627 Accuracies by class [array(0., dtype=float32), array(0.96, dtype=float32), array(0.841, dtype=float32), array(0.899, dtype=float32), array(0.778, dtype=float32), array(0.982, dtype=float32), array(0.693, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31980 Training loss 0.04117884859442711 Validation loss 0.038642700761556625 Accuracy 0.5990999937057495 Accuracies by class [array(0., dtype=float32), array(0.967, dtype=float32), array(0.861, dtype=float32), array(0.859, dtype=float32), array(0.825, dtype=float32), array(0.984, dtype=float32), array(0.53, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 31990 Training loss 0.03352876752614975 Validation loss 0.037623241543769836 Accuracy 0.6080999970436096 Accuracies by class [array(0., dtype=float32), array(0.964, dtype=float32), array(0.847, dtype=float32), array(0.887, dtype=float32), array(0.8, dtype=float32), array(0.988, dtype=float32), array(0.629, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32000 Training loss 0.03683006763458252 Validation loss 0.03778143599629402 Accuracy 0.6115999817848206 Accuracies by class [array(0., dtype=float32), array(0.963, dtype=float32), array(0.756, dtype=float32), array(0.914, dtype=float32), array(0.792, dtype=float32), array(0.996, dtype=float32), array(0.735, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32010 Training loss 0.0359337143599987 Validation loss 0.037187088280916214 Accuracy 0.6119999885559082 Accuracies by class [array(0., dtype=float32), array(0.965, dtype=float32), array(0.776, dtype=float32), array(0.881, dtype=float32), array(0.783, dtype=float32), array(0.991, dtype=float32), array(0.769, dtype=float32), array(0., dtype=float32), array(0.955, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32020 Training loss 0.0361030288040638 Validation loss 0.038224030286073685 Accuracy 0.6021000146865845 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.873, dtype=float32), array(0.89, dtype=float32), array(0.627, dtype=float32), array(0.988, dtype=float32), array(0.724, dtype=float32), array(0., dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32030 Training loss 0.033007919788360596 Validation loss 0.03801966831088066 Accuracy 0.60589998960495 Accuracies by class [array(0., dtype=float32), array(0.968, dtype=float32), array(0.809, dtype=float32), array(0.926, dtype=float32), array(0.832, dtype=float32), array(0.98, dtype=float32), array(0.575, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32040 Training loss 0.03947941213846207 Validation loss 0.03854618966579437 Accuracy 0.6014000177383423 Accuracies by class [array(0., dtype=float32), array(0.97, dtype=float32), array(0.846, dtype=float32), array(0.902, dtype=float32), array(0.792, dtype=float32), array(0.995, dtype=float32), array(0.549, dtype=float32), array(0., dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32050 Training loss 0.03744649887084961 Validation loss 0.037493832409381866 Accuracy 0.6079000234603882 Accuracies by class [array(0.002, dtype=float32), array(0.967, dtype=float32), array(0.789, dtype=float32), array(0.907, dtype=float32), array(0.831, dtype=float32), array(0.988, dtype=float32), array(0.636, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32060 Training loss 0.03332648053765297 Validation loss 0.0377790667116642 Accuracy 0.6061999797821045 Accuracies by class [array(0.002, dtype=float32), array(0.961, dtype=float32), array(0.781, dtype=float32), array(0.938, dtype=float32), array(0.67, dtype=float32), array(0.989, dtype=float32), array(0.75, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32070 Training loss 0.03845350071787834 Validation loss 0.03732689842581749 Accuracy 0.6111000180244446 Accuracies by class [array(0.002, dtype=float32), array(0.967, dtype=float32), array(0.804, dtype=float32), array(0.898, dtype=float32), array(0.729, dtype=float32), array(0.981, dtype=float32), array(0.759, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32080 Training loss 0.032807689160108566 Validation loss 0.03714201971888542 Accuracy 0.6110000014305115 Accuracies by class [array(0.001, dtype=float32), array(0.965, dtype=float32), array(0.783, dtype=float32), array(0.919, dtype=float32), array(0.771, dtype=float32), array(0.989, dtype=float32), array(0.716, dtype=float32), array(0., dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32090 Training loss 0.03394098952412605 Validation loss 0.03717706352472305 Accuracy 0.6107000112533569 Accuracies by class [array(0.001, dtype=float32), array(0.967, dtype=float32), array(0.792, dtype=float32), array(0.888, dtype=float32), array(0.814, dtype=float32), array(0.99, dtype=float32), array(0.688, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32100 Training loss 0.03608514741063118 Validation loss 0.03826553747057915 Accuracy 0.6087999939918518 Accuracies by class [array(0.007, dtype=float32), array(0.972, dtype=float32), array(0.866, dtype=float32), array(0.906, dtype=float32), array(0.683, dtype=float32), array(0.995, dtype=float32), array(0.697, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32110 Training loss 0.031131019815802574 Validation loss 0.03492608293890953 Accuracy 0.6308000087738037 Accuracies by class [array(0.393, dtype=float32), array(0.976, dtype=float32), array(0.724, dtype=float32), array(0.871, dtype=float32), array(0.908, dtype=float32), array(0.986, dtype=float32), array(0.476, dtype=float32), array(0., dtype=float32), array(0.974, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32120 Training loss 0.028933171182870865 Validation loss 0.03192157670855522 Accuracy 0.6620000004768372 Accuracies by class [array(0.668, dtype=float32), array(0.962, dtype=float32), array(0.831, dtype=float32), array(0.853, dtype=float32), array(0.78, dtype=float32), array(0.984, dtype=float32), array(0.567, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32130 Training loss 0.030136846005916595 Validation loss 0.03256753459572792 Accuracy 0.6626999974250793 Accuracies by class [array(0.559, dtype=float32), array(0.965, dtype=float32), array(0.772, dtype=float32), array(0.906, dtype=float32), array(0.813, dtype=float32), array(0.996, dtype=float32), array(0.646, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32140 Training loss 0.03336743265390396 Validation loss 0.03428512439131737 Accuracy 0.642799973487854 Accuracies by class [array(0.749, dtype=float32), array(0.965, dtype=float32), array(0.604, dtype=float32), array(0.881, dtype=float32), array(0.915, dtype=float32), array(0.967, dtype=float32), array(0.372, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32150 Training loss 0.02884436585009098 Validation loss 0.03261297196149826 Accuracy 0.6553999781608582 Accuracies by class [array(0.538, dtype=float32), array(0.962, dtype=float32), array(0.824, dtype=float32), array(0.902, dtype=float32), array(0.831, dtype=float32), array(0.986, dtype=float32), array(0.538, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32160 Training loss 0.030732717365026474 Validation loss 0.03306533768773079 Accuracy 0.6579999923706055 Accuracies by class [array(0.543, dtype=float32), array(0.971, dtype=float32), array(0.807, dtype=float32), array(0.886, dtype=float32), array(0.823, dtype=float32), array(0.998, dtype=float32), array(0.594, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32170 Training loss 0.031245306134223938 Validation loss 0.031475383788347244 Accuracy 0.6654999852180481 Accuracies by class [array(0.656, dtype=float32), array(0.961, dtype=float32), array(0.807, dtype=float32), array(0.902, dtype=float32), array(0.806, dtype=float32), array(0.986, dtype=float32), array(0.572, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32180 Training loss 0.027531340718269348 Validation loss 0.031711071729660034 Accuracy 0.6629999876022339 Accuracies by class [array(0.711, dtype=float32), array(0.971, dtype=float32), array(0.732, dtype=float32), array(0.832, dtype=float32), array(0.8, dtype=float32), array(0.985, dtype=float32), array(0.631, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32190 Training loss 0.02901935577392578 Validation loss 0.031910091638565063 Accuracy 0.6654999852180481 Accuracies by class [array(0.728, dtype=float32), array(0.963, dtype=float32), array(0.829, dtype=float32), array(0.906, dtype=float32), array(0.698, dtype=float32), array(0.992, dtype=float32), array(0.574, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32200 Training loss 0.02794848568737507 Validation loss 0.03237367048859596 Accuracy 0.6567000150680542 Accuracies by class [array(0.607, dtype=float32), array(0.961, dtype=float32), array(0.855, dtype=float32), array(0.907, dtype=float32), array(0.637, dtype=float32), array(0.974, dtype=float32), array(0.661, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32210 Training loss 0.03362646326422691 Validation loss 0.032731793820858 Accuracy 0.6621999740600586 Accuracies by class [array(0.767, dtype=float32), array(0.959, dtype=float32), array(0.748, dtype=float32), array(0.89, dtype=float32), array(0.77, dtype=float32), array(0.958, dtype=float32), array(0.562, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32220 Training loss 0.03086765483021736 Validation loss 0.031980808824300766 Accuracy 0.6647999882698059 Accuracies by class [array(0.736, dtype=float32), array(0.97, dtype=float32), array(0.734, dtype=float32), array(0.854, dtype=float32), array(0.884, dtype=float32), array(0.995, dtype=float32), array(0.511, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32230 Training loss 0.03251885995268822 Validation loss 0.032163720577955246 Accuracy 0.65829998254776 Accuracies by class [array(0.602, dtype=float32), array(0.958, dtype=float32), array(0.87, dtype=float32), array(0.896, dtype=float32), array(0.768, dtype=float32), array(0.988, dtype=float32), array(0.543, dtype=float32), array(0., dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32240 Training loss 0.0308060385286808 Validation loss 0.03140120953321457 Accuracy 0.6632999777793884 Accuracies by class [array(0.771, dtype=float32), array(0.959, dtype=float32), array(0.73, dtype=float32), array(0.898, dtype=float32), array(0.848, dtype=float32), array(0.989, dtype=float32), array(0.469, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32250 Training loss 0.03165507689118385 Validation loss 0.03103719651699066 Accuracy 0.6676999926567078 Accuracies by class [array(0.694, dtype=float32), array(0.958, dtype=float32), array(0.782, dtype=float32), array(0.923, dtype=float32), array(0.801, dtype=float32), array(0.991, dtype=float32), array(0.563, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32260 Training loss 0.030961085110902786 Validation loss 0.03301655128598213 Accuracy 0.6578999757766724 Accuracies by class [array(0.505, dtype=float32), array(0.963, dtype=float32), array(0.73, dtype=float32), array(0.929, dtype=float32), array(0.783, dtype=float32), array(0.996, dtype=float32), array(0.716, dtype=float32), array(0., dtype=float32), array(0.957, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32270 Training loss 0.03322877362370491 Validation loss 0.034309569746255875 Accuracy 0.6478000283241272 Accuracies by class [array(0.591, dtype=float32), array(0.967, dtype=float32), array(0.762, dtype=float32), array(0.913, dtype=float32), array(0.571, dtype=float32), array(0.949, dtype=float32), array(0.76, dtype=float32), array(0., dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32280 Training loss 0.0291462205350399 Validation loss 0.03114302083849907 Accuracy 0.6704999804496765 Accuracies by class [array(0.764, dtype=float32), array(0.96, dtype=float32), array(0.773, dtype=float32), array(0.876, dtype=float32), array(0.8, dtype=float32), array(0.971, dtype=float32), array(0.582, dtype=float32), array(0., dtype=float32), array(0.979, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32290 Training loss 0.02537328563630581 Validation loss 0.030816538259387016 Accuracy 0.6704000234603882 Accuracies by class [array(0.764, dtype=float32), array(0.965, dtype=float32), array(0.81, dtype=float32), array(0.849, dtype=float32), array(0.795, dtype=float32), array(0.988, dtype=float32), array(0.564, dtype=float32), array(0., dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32300 Training loss 0.028831953182816505 Validation loss 0.031731098890304565 Accuracy 0.6664999723434448 Accuracies by class [array(0.778, dtype=float32), array(0.954, dtype=float32), array(0.795, dtype=float32), array(0.901, dtype=float32), array(0.807, dtype=float32), array(0.969, dtype=float32), array(0.493, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32310 Training loss 0.02817399613559246 Validation loss 0.031243622303009033 Accuracy 0.6636999845504761 Accuracies by class [array(0.837, dtype=float32), array(0.97, dtype=float32), array(0.839, dtype=float32), array(0.902, dtype=float32), array(0.67, dtype=float32), array(0.989, dtype=float32), array(0.463, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32320 Training loss 0.033237095922231674 Validation loss 0.033079229295253754 Accuracy 0.6467000246047974 Accuracies by class [array(0.477, dtype=float32), array(0.97, dtype=float32), array(0.714, dtype=float32), array(0.772, dtype=float32), array(0.839, dtype=float32), array(0.973, dtype=float32), array(0.758, dtype=float32), array(0., dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32330 Training loss 0.029942210763692856 Validation loss 0.03157365322113037 Accuracy 0.6657000184059143 Accuracies by class [array(0.701, dtype=float32), array(0.974, dtype=float32), array(0.784, dtype=float32), array(0.861, dtype=float32), array(0.853, dtype=float32), array(0.985, dtype=float32), array(0.528, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32340 Training loss 0.03036394715309143 Validation loss 0.03135005012154579 Accuracy 0.6690000295639038 Accuracies by class [array(0.76, dtype=float32), array(0.966, dtype=float32), array(0.764, dtype=float32), array(0.893, dtype=float32), array(0.803, dtype=float32), array(0.962, dtype=float32), array(0.567, dtype=float32), array(0., dtype=float32), array(0.975, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32350 Training loss 0.025701124221086502 Validation loss 0.03159444406628609 Accuracy 0.6635000109672546 Accuracies by class [array(0.701, dtype=float32), array(0.968, dtype=float32), array(0.677, dtype=float32), array(0.86, dtype=float32), array(0.856, dtype=float32), array(0.96, dtype=float32), array(0.643, dtype=float32), array(0., dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32360 Training loss 0.025402143597602844 Validation loss 0.030418720096349716 Accuracy 0.6700000166893005 Accuracies by class [array(0.753, dtype=float32), array(0.969, dtype=float32), array(0.807, dtype=float32), array(0.857, dtype=float32), array(0.802, dtype=float32), array(0.986, dtype=float32), array(0.555, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32370 Training loss 0.026890933513641357 Validation loss 0.030809316784143448 Accuracy 0.6682999730110168 Accuracies by class [array(0.773, dtype=float32), array(0.968, dtype=float32), array(0.693, dtype=float32), array(0.838, dtype=float32), array(0.854, dtype=float32), array(0.983, dtype=float32), array(0.603, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32380 Training loss 0.03003961220383644 Validation loss 0.030692221596837044 Accuracy 0.6658999919891357 Accuracies by class [array(0.751, dtype=float32), array(0.971, dtype=float32), array(0.802, dtype=float32), array(0.865, dtype=float32), array(0.845, dtype=float32), array(0.979, dtype=float32), array(0.483, dtype=float32), array(0., dtype=float32), array(0.963, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32390 Training loss 0.029330912977457047 Validation loss 0.03236059099435806 Accuracy 0.6658999919891357 Accuracies by class [array(0.877, dtype=float32), array(0.956, dtype=float32), array(0.782, dtype=float32), array(0.86, dtype=float32), array(0.818, dtype=float32), array(0.994, dtype=float32), array(0.413, dtype=float32), array(0., dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32400 Training loss 0.028272196650505066 Validation loss 0.030702892690896988 Accuracy 0.666700005531311 Accuracies by class [array(0.862, dtype=float32), array(0.965, dtype=float32), array(0.773, dtype=float32), array(0.789, dtype=float32), array(0.838, dtype=float32), array(0.987, dtype=float32), array(0.482, dtype=float32), array(0., dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32410 Training loss 0.03112041763961315 Validation loss 0.031241126358509064 Accuracy 0.6675000190734863 Accuracies by class [array(0.733, dtype=float32), array(0.964, dtype=float32), array(0.825, dtype=float32), array(0.917, dtype=float32), array(0.777, dtype=float32), array(0.988, dtype=float32), array(0.504, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32420 Training loss 0.027190467342734337 Validation loss 0.030936431139707565 Accuracy 0.6664000153541565 Accuracies by class [array(0.754, dtype=float32), array(0.964, dtype=float32), array(0.852, dtype=float32), array(0.916, dtype=float32), array(0.687, dtype=float32), array(0.986, dtype=float32), array(0.538, dtype=float32), array(0., dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32430 Training loss 0.02827705442905426 Validation loss 0.030318357050418854 Accuracy 0.6690000295639038 Accuracies by class [array(0.769, dtype=float32), array(0.976, dtype=float32), array(0.703, dtype=float32), array(0.811, dtype=float32), array(0.845, dtype=float32), array(0.984, dtype=float32), array(0.64, dtype=float32), array(0., dtype=float32), array(0.962, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32440 Training loss 0.028956761583685875 Validation loss 0.03082544356584549 Accuracy 0.6643000245094299 Accuracies by class [array(0.625, dtype=float32), array(0.963, dtype=float32), array(0.775, dtype=float32), array(0.913, dtype=float32), array(0.814, dtype=float32), array(0.968, dtype=float32), array(0.617, dtype=float32), array(0., dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32450 Training loss 0.026690127328038216 Validation loss 0.02976142056286335 Accuracy 0.6725999712944031 Accuracies by class [array(0.773, dtype=float32), array(0.964, dtype=float32), array(0.787, dtype=float32), array(0.891, dtype=float32), array(0.777, dtype=float32), array(0.971, dtype=float32), array(0.59, dtype=float32), array(0., dtype=float32), array(0.973, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32460 Training loss 0.029216675087809563 Validation loss 0.030732490122318268 Accuracy 0.670799970626831 Accuracies by class [array(0.753, dtype=float32), array(0.967, dtype=float32), array(0.784, dtype=float32), array(0.87, dtype=float32), array(0.853, dtype=float32), array(0.988, dtype=float32), array(0.534, dtype=float32), array(0.001, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32470 Training loss 0.027638887986540794 Validation loss 0.030769774690270424 Accuracy 0.6628000140190125 Accuracies by class [array(0.753, dtype=float32), array(0.962, dtype=float32), array(0.7, dtype=float32), array(0.863, dtype=float32), array(0.892, dtype=float32), array(0.973, dtype=float32), array(0.532, dtype=float32), array(0.001, dtype=float32), array(0.952, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32480 Training loss 0.030951084569096565 Validation loss 0.030263490974903107 Accuracy 0.6692000031471252 Accuracies by class [array(0.846, dtype=float32), array(0.958, dtype=float32), array(0.781, dtype=float32), array(0.872, dtype=float32), array(0.805, dtype=float32), array(0.987, dtype=float32), array(0.472, dtype=float32), array(0.001, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32490 Training loss 0.030206594616174698 Validation loss 0.030201701447367668 Accuracy 0.6687999963760376 Accuracies by class [array(0.768, dtype=float32), array(0.968, dtype=float32), array(0.763, dtype=float32), array(0.882, dtype=float32), array(0.851, dtype=float32), array(0.974, dtype=float32), array(0.51, dtype=float32), array(0.001, dtype=float32), array(0.971, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32500 Training loss 0.02832491509616375 Validation loss 0.02998577617108822 Accuracy 0.6693999767303467 Accuracies by class [array(0.848, dtype=float32), array(0.965, dtype=float32), array(0.765, dtype=float32), array(0.862, dtype=float32), array(0.759, dtype=float32), array(0.987, dtype=float32), array(0.537, dtype=float32), array(0.002, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32510 Training loss 0.027283217757940292 Validation loss 0.029813144356012344 Accuracy 0.6704000234603882 Accuracies by class [array(0.805, dtype=float32), array(0.971, dtype=float32), array(0.733, dtype=float32), array(0.852, dtype=float32), array(0.807, dtype=float32), array(0.963, dtype=float32), array(0.599, dtype=float32), array(0.002, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32520 Training loss 0.027388963848352432 Validation loss 0.029702631756663322 Accuracy 0.6747999787330627 Accuracies by class [array(0.788, dtype=float32), array(0.957, dtype=float32), array(0.752, dtype=float32), array(0.89, dtype=float32), array(0.823, dtype=float32), array(0.966, dtype=float32), array(0.598, dtype=float32), array(0.006, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32530 Training loss 0.02914212830364704 Validation loss 0.030860934406518936 Accuracy 0.6579999923706055 Accuracies by class [array(0.516, dtype=float32), array(0.955, dtype=float32), array(0.763, dtype=float32), array(0.868, dtype=float32), array(0.781, dtype=float32), array(0.975, dtype=float32), array(0.749, dtype=float32), array(0.004, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32540 Training loss 0.028554212301969528 Validation loss 0.02967783249914646 Accuracy 0.6680999994277954 Accuracies by class [array(0.756, dtype=float32), array(0.96, dtype=float32), array(0.802, dtype=float32), array(0.879, dtype=float32), array(0.851, dtype=float32), array(0.982, dtype=float32), array(0.484, dtype=float32), array(0.006, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32550 Training loss 0.02637915313243866 Validation loss 0.029352910816669464 Accuracy 0.6712999939918518 Accuracies by class [array(0.734, dtype=float32), array(0.967, dtype=float32), array(0.819, dtype=float32), array(0.882, dtype=float32), array(0.693, dtype=float32), array(0.976, dtype=float32), array(0.668, dtype=float32), array(0.008, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32560 Training loss 0.01870771311223507 Validation loss 0.024295996874570847 Accuracy 0.7365000247955322 Accuracies by class [array(0.742, dtype=float32), array(0.972, dtype=float32), array(0.799, dtype=float32), array(0.852, dtype=float32), array(0.825, dtype=float32), array(0.9, dtype=float32), array(0.569, dtype=float32), array(0.771, dtype=float32), array(0.935, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32570 Training loss 0.019855540245771408 Validation loss 0.023360708728432655 Accuracy 0.7441999912261963 Accuracies by class [array(0.826, dtype=float32), array(0.952, dtype=float32), array(0.736, dtype=float32), array(0.887, dtype=float32), array(0.824, dtype=float32), array(0.923, dtype=float32), array(0.586, dtype=float32), array(0.749, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32580 Training loss 0.020694393664598465 Validation loss 0.02438466250896454 Accuracy 0.7426000237464905 Accuracies by class [array(0.657, dtype=float32), array(0.958, dtype=float32), array(0.761, dtype=float32), array(0.883, dtype=float32), array(0.766, dtype=float32), array(0.822, dtype=float32), array(0.708, dtype=float32), array(0.94, dtype=float32), array(0.931, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32590 Training loss 0.021751565858721733 Validation loss 0.02332061529159546 Accuracy 0.746399998664856 Accuracies by class [array(0.825, dtype=float32), array(0.968, dtype=float32), array(0.845, dtype=float32), array(0.841, dtype=float32), array(0.693, dtype=float32), array(0.913, dtype=float32), array(0.552, dtype=float32), array(0.863, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32600 Training loss 0.02418053150177002 Validation loss 0.02287430688738823 Accuracy 0.7513999938964844 Accuracies by class [array(0.803, dtype=float32), array(0.966, dtype=float32), array(0.747, dtype=float32), array(0.82, dtype=float32), array(0.838, dtype=float32), array(0.917, dtype=float32), array(0.581, dtype=float32), array(0.873, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32610 Training loss 0.02051333151757717 Validation loss 0.022975439205765724 Accuracy 0.7516999840736389 Accuracies by class [array(0.713, dtype=float32), array(0.955, dtype=float32), array(0.779, dtype=float32), array(0.933, dtype=float32), array(0.728, dtype=float32), array(0.935, dtype=float32), array(0.649, dtype=float32), array(0.869, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32620 Training loss 0.0212183129042387 Validation loss 0.022760234773159027 Accuracy 0.7595999836921692 Accuracies by class [array(0.792, dtype=float32), array(0.965, dtype=float32), array(0.808, dtype=float32), array(0.872, dtype=float32), array(0.781, dtype=float32), array(0.92, dtype=float32), array(0.581, dtype=float32), array(0.924, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32630 Training loss 0.018082302063703537 Validation loss 0.023347441107034683 Accuracy 0.7493000030517578 Accuracies by class [array(0.618, dtype=float32), array(0.966, dtype=float32), array(0.744, dtype=float32), array(0.875, dtype=float32), array(0.837, dtype=float32), array(0.939, dtype=float32), array(0.677, dtype=float32), array(0.881, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32640 Training loss 0.021597202867269516 Validation loss 0.022821905091404915 Accuracy 0.7541999816894531 Accuracies by class [array(0.736, dtype=float32), array(0.958, dtype=float32), array(0.836, dtype=float32), array(0.917, dtype=float32), array(0.711, dtype=float32), array(0.942, dtype=float32), array(0.592, dtype=float32), array(0.894, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32650 Training loss 0.02100379765033722 Validation loss 0.02394603006541729 Accuracy 0.7419999837875366 Accuracies by class [array(0.878, dtype=float32), array(0.962, dtype=float32), array(0.818, dtype=float32), array(0.801, dtype=float32), array(0.782, dtype=float32), array(0.939, dtype=float32), array(0.382, dtype=float32), array(0.889, dtype=float32), array(0.969, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32660 Training loss 0.021468844264745712 Validation loss 0.024027487263083458 Accuracy 0.7422999739646912 Accuracies by class [array(0.709, dtype=float32), array(0.965, dtype=float32), array(0.726, dtype=float32), array(0.859, dtype=float32), array(0.845, dtype=float32), array(0.972, dtype=float32), array(0.636, dtype=float32), array(0.747, dtype=float32), array(0.964, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32670 Training loss 0.01948390156030655 Validation loss 0.022692350670695305 Accuracy 0.7529000043869019 Accuracies by class [array(0.779, dtype=float32), array(0.963, dtype=float32), array(0.671, dtype=float32), array(0.884, dtype=float32), array(0.861, dtype=float32), array(0.917, dtype=float32), array(0.604, dtype=float32), array(0.911, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32680 Training loss 0.01998426392674446 Validation loss 0.022555366158485413 Accuracy 0.753000020980835 Accuracies by class [array(0.848, dtype=float32), array(0.971, dtype=float32), array(0.734, dtype=float32), array(0.877, dtype=float32), array(0.811, dtype=float32), array(0.963, dtype=float32), array(0.523, dtype=float32), array(0.847, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32690 Training loss 0.019606616348028183 Validation loss 0.023378662765026093 Accuracy 0.7443000078201294 Accuracies by class [array(0.855, dtype=float32), array(0.969, dtype=float32), array(0.734, dtype=float32), array(0.866, dtype=float32), array(0.653, dtype=float32), array(0.946, dtype=float32), array(0.583, dtype=float32), array(0.888, dtype=float32), array(0.949, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32700 Training loss 0.020234055817127228 Validation loss 0.022683853283524513 Accuracy 0.7566999793052673 Accuracies by class [array(0.829, dtype=float32), array(0.964, dtype=float32), array(0.763, dtype=float32), array(0.871, dtype=float32), array(0.816, dtype=float32), array(0.966, dtype=float32), array(0.541, dtype=float32), array(0.858, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32710 Training loss 0.022450683638453484 Validation loss 0.02359868586063385 Accuracy 0.7480000257492065 Accuracies by class [array(0.695, dtype=float32), array(0.96, dtype=float32), array(0.781, dtype=float32), array(0.911, dtype=float32), array(0.788, dtype=float32), array(0.948, dtype=float32), array(0.619, dtype=float32), array(0.811, dtype=float32), array(0.967, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32720 Training loss 0.017443614080548286 Validation loss 0.02222135290503502 Accuracy 0.7578999996185303 Accuracies by class [array(0.714, dtype=float32), array(0.963, dtype=float32), array(0.781, dtype=float32), array(0.911, dtype=float32), array(0.8, dtype=float32), array(0.95, dtype=float32), array(0.596, dtype=float32), array(0.899, dtype=float32), array(0.965, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32730 Training loss 0.01832641288638115 Validation loss 0.022006884217262268 Accuracy 0.7598000168800354 Accuracies by class [array(0.739, dtype=float32), array(0.963, dtype=float32), array(0.811, dtype=float32), array(0.877, dtype=float32), array(0.765, dtype=float32), array(0.935, dtype=float32), array(0.636, dtype=float32), array(0.914, dtype=float32), array(0.958, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32740 Training loss 0.019926486536860466 Validation loss 0.02286653034389019 Accuracy 0.7493000030517578 Accuracies by class [array(0.679, dtype=float32), array(0.962, dtype=float32), array(0.798, dtype=float32), array(0.903, dtype=float32), array(0.659, dtype=float32), array(0.936, dtype=float32), array(0.697, dtype=float32), array(0.903, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32750 Training loss 0.020920803770422935 Validation loss 0.022212153300642967 Accuracy 0.7576000094413757 Accuracies by class [array(0.668, dtype=float32), array(0.962, dtype=float32), array(0.815, dtype=float32), array(0.896, dtype=float32), array(0.777, dtype=float32), array(0.933, dtype=float32), array(0.635, dtype=float32), array(0.924, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32760 Training loss 0.019673116505146027 Validation loss 0.02255069836974144 Accuracy 0.7548999786376953 Accuracies by class [array(0.727, dtype=float32), array(0.961, dtype=float32), array(0.781, dtype=float32), array(0.912, dtype=float32), array(0.809, dtype=float32), array(0.957, dtype=float32), array(0.57, dtype=float32), array(0.862, dtype=float32), array(0.97, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32770 Training loss 0.0187593512237072 Validation loss 0.022826503962278366 Accuracy 0.7498000264167786 Accuracies by class [array(0.712, dtype=float32), array(0.961, dtype=float32), array(0.632, dtype=float32), array(0.883, dtype=float32), array(0.838, dtype=float32), array(0.946, dtype=float32), array(0.682, dtype=float32), array(0.885, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32780 Training loss 0.023572595790028572 Validation loss 0.023559048771858215 Accuracy 0.7462999820709229 Accuracies by class [array(0.81, dtype=float32), array(0.962, dtype=float32), array(0.897, dtype=float32), array(0.889, dtype=float32), array(0.623, dtype=float32), array(0.928, dtype=float32), array(0.487, dtype=float32), array(0.928, dtype=float32), array(0.939, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32790 Training loss 0.02082033082842827 Validation loss 0.02224137634038925 Accuracy 0.7556999921798706 Accuracies by class [array(0.791, dtype=float32), array(0.96, dtype=float32), array(0.754, dtype=float32), array(0.925, dtype=float32), array(0.729, dtype=float32), array(0.937, dtype=float32), array(0.616, dtype=float32), array(0.901, dtype=float32), array(0.944, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32800 Training loss 0.02151363156735897 Validation loss 0.022886643186211586 Accuracy 0.754800021648407 Accuracies by class [array(0.739, dtype=float32), array(0.962, dtype=float32), array(0.816, dtype=float32), array(0.945, dtype=float32), array(0.744, dtype=float32), array(0.902, dtype=float32), array(0.531, dtype=float32), array(0.958, dtype=float32), array(0.951, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32810 Training loss 0.02142604999244213 Validation loss 0.021943608298897743 Accuracy 0.7599999904632568 Accuracies by class [array(0.832, dtype=float32), array(0.965, dtype=float32), array(0.711, dtype=float32), array(0.864, dtype=float32), array(0.852, dtype=float32), array(0.951, dtype=float32), array(0.541, dtype=float32), array(0.918, dtype=float32), array(0.966, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32820 Training loss 0.021196862682700157 Validation loss 0.022756032645702362 Accuracy 0.7497000098228455 Accuracies by class [array(0.872, dtype=float32), array(0.961, dtype=float32), array(0.841, dtype=float32), array(0.879, dtype=float32), array(0.751, dtype=float32), array(0.941, dtype=float32), array(0.382, dtype=float32), array(0.898, dtype=float32), array(0.972, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32830 Training loss 0.0184071883559227 Validation loss 0.02229997143149376 Accuracy 0.7603999972343445 Accuracies by class [array(0.842, dtype=float32), array(0.961, dtype=float32), array(0.711, dtype=float32), array(0.868, dtype=float32), array(0.823, dtype=float32), array(0.906, dtype=float32), array(0.577, dtype=float32), array(0.963, dtype=float32), array(0.953, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32840 Training loss 0.02252241224050522 Validation loss 0.02208077721297741 Accuracy 0.7599999904632568 Accuracies by class [array(0.845, dtype=float32), array(0.969, dtype=float32), array(0.762, dtype=float32), array(0.893, dtype=float32), array(0.817, dtype=float32), array(0.921, dtype=float32), array(0.487, dtype=float32), array(0.947, dtype=float32), array(0.959, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32850 Training loss 0.02129482850432396 Validation loss 0.021876294165849686 Accuracy 0.7583000063896179 Accuracies by class [array(0.764, dtype=float32), array(0.968, dtype=float32), array(0.713, dtype=float32), array(0.911, dtype=float32), array(0.783, dtype=float32), array(0.925, dtype=float32), array(0.634, dtype=float32), array(0.929, dtype=float32), array(0.956, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32860 Training loss 0.018232740461826324 Validation loss 0.022316765040159225 Accuracy 0.7573000192642212 Accuracies by class [array(0.855, dtype=float32), array(0.965, dtype=float32), array(0.758, dtype=float32), array(0.907, dtype=float32), array(0.828, dtype=float32), array(0.936, dtype=float32), array(0.433, dtype=float32), array(0.921, dtype=float32), array(0.969, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32870 Training loss 0.021166253834962845 Validation loss 0.02248268760740757 Accuracy 0.7537999749183655 Accuracies by class [array(0.84, dtype=float32), array(0.969, dtype=float32), array(0.86, dtype=float32), array(0.862, dtype=float32), array(0.64, dtype=float32), array(0.935, dtype=float32), array(0.54, dtype=float32), array(0.934, dtype=float32), array(0.957, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32880 Training loss 0.024675119668245316 Validation loss 0.022709231823682785 Accuracy 0.7537999749183655 Accuracies by class [array(0.888, dtype=float32), array(0.966, dtype=float32), array(0.747, dtype=float32), array(0.807, dtype=float32), array(0.759, dtype=float32), array(0.937, dtype=float32), array(0.56, dtype=float32), array(0.914, dtype=float32), array(0.959, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32890 Training loss 0.02065734937787056 Validation loss 0.022344468161463737 Accuracy 0.7549999952316284 Accuracies by class [array(0.709, dtype=float32), array(0.965, dtype=float32), array(0.77, dtype=float32), array(0.906, dtype=float32), array(0.676, dtype=float32), array(0.919, dtype=float32), array(0.693, dtype=float32), array(0.943, dtype=float32), array(0.968, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32900 Training loss 0.017981696873903275 Validation loss 0.02242753840982914 Accuracy 0.7542999982833862 Accuracies by class [array(0.769, dtype=float32), array(0.966, dtype=float32), array(0.771, dtype=float32), array(0.84, dtype=float32), array(0.863, dtype=float32), array(0.911, dtype=float32), array(0.512, dtype=float32), array(0.944, dtype=float32), array(0.966, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32910 Training loss 0.01757097989320755 Validation loss 0.02169262431561947 Accuracy 0.7612000107765198 Accuracies by class [array(0.769, dtype=float32), array(0.963, dtype=float32), array(0.788, dtype=float32), array(0.893, dtype=float32), array(0.789, dtype=float32), array(0.931, dtype=float32), array(0.573, dtype=float32), array(0.938, dtype=float32), array(0.967, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32920 Training loss 0.01829591579735279 Validation loss 0.021520288661122322 Accuracy 0.7630000114440918 Accuracies by class [array(0.819, dtype=float32), array(0.959, dtype=float32), array(0.722, dtype=float32), array(0.89, dtype=float32), array(0.794, dtype=float32), array(0.919, dtype=float32), array(0.611, dtype=float32), array(0.948, dtype=float32), array(0.968, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32930 Training loss 0.019845454022288322 Validation loss 0.021796055138111115 Accuracy 0.757099986076355 Accuracies by class [array(0.739, dtype=float32), array(0.958, dtype=float32), array(0.806, dtype=float32), array(0.889, dtype=float32), array(0.791, dtype=float32), array(0.95, dtype=float32), array(0.588, dtype=float32), array(0.886, dtype=float32), array(0.963, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32940 Training loss 0.020582959055900574 Validation loss 0.02474977821111679 Accuracy 0.7322999835014343 Accuracies by class [array(0.494, dtype=float32), array(0.955, dtype=float32), array(0.683, dtype=float32), array(0.893, dtype=float32), array(0.701, dtype=float32), array(0.93, dtype=float32), array(0.78, dtype=float32), array(0.933, dtype=float32), array(0.954, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32950 Training loss 0.021876055747270584 Validation loss 0.021733734756708145 Accuracy 0.7583000063896179 Accuracies by class [array(0.735, dtype=float32), array(0.963, dtype=float32), array(0.749, dtype=float32), array(0.871, dtype=float32), array(0.827, dtype=float32), array(0.942, dtype=float32), array(0.619, dtype=float32), array(0.907, dtype=float32), array(0.969, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32960 Training loss 0.01870517060160637 Validation loss 0.022161943838000298 Accuracy 0.7551000118255615 Accuracies by class [array(0.844, dtype=float32), array(0.965, dtype=float32), array(0.803, dtype=float32), array(0.91, dtype=float32), array(0.8, dtype=float32), array(0.934, dtype=float32), array(0.421, dtype=float32), array(0.907, dtype=float32), array(0.966, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 32970 Training loss 0.021260304376482964 Validation loss 0.022116083651781082 Accuracy 0.7549999952316284 Accuracies by class [array(0.758, dtype=float32), array(0.963, dtype=float32), array(0.655, dtype=float32), array(0.887, dtype=float32), array(0.845, dtype=float32), array(0.926, dtype=float32), array(0.621, dtype=float32), array(0.934, dtype=float32), array(0.961, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 32980 Training loss 0.021247487515211105 Validation loss 0.021686583757400513 Accuracy 0.7591999769210815 Accuracies by class [array(0.834, dtype=float32), array(0.963, dtype=float32), array(0.799, dtype=float32), array(0.92, dtype=float32), array(0.76, dtype=float32), array(0.937, dtype=float32), array(0.494, dtype=float32), array(0.915, dtype=float32), array(0.968, dtype=float32), array(0.002, dtype=float32)]\n",
      "Iteration 32990 Training loss 0.020419562235474586 Validation loss 0.021472854539752007 Accuracy 0.760699987411499 Accuracies by class [array(0.772, dtype=float32), array(0.967, dtype=float32), array(0.745, dtype=float32), array(0.876, dtype=float32), array(0.766, dtype=float32), array(0.924, dtype=float32), array(0.662, dtype=float32), array(0.935, dtype=float32), array(0.96, dtype=float32), array(0., dtype=float32)]\n",
      "Iteration 33000 Training loss 0.021435152739286423 Validation loss 0.021689843386411667 Accuracy 0.7599999904632568 Accuracies by class [array(0.823, dtype=float32), array(0.968, dtype=float32), array(0.803, dtype=float32), array(0.824, dtype=float32), array(0.824, dtype=float32), array(0.933, dtype=float32), array(0.531, dtype=float32), array(0.929, dtype=float32), array(0.964, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 33010 Training loss 0.018154755234718323 Validation loss 0.02139117196202278 Accuracy 0.7635999917984009 Accuracies by class [array(0.792, dtype=float32), array(0.967, dtype=float32), array(0.815, dtype=float32), array(0.887, dtype=float32), array(0.752, dtype=float32), array(0.946, dtype=float32), array(0.588, dtype=float32), array(0.922, dtype=float32), array(0.966, dtype=float32), array(0.001, dtype=float32)]\n",
      "Iteration 33020 Training loss 0.01550333946943283 Validation loss 0.021525420248508453 Accuracy 0.758899986743927 Accuracies by class [array(0.815, dtype=float32), array(0.97, dtype=float32), array(0.849, dtype=float32), array(0.833, dtype=float32), array(0.75, dtype=float32), array(0.947, dtype=float32), array(0.562, dtype=float32), array(0.904, dtype=float32), array(0.956, dtype=float32), array(0.003, dtype=float32)]\n",
      "Iteration 33030 Training loss 0.015834448859095573 Validation loss 0.01919066347181797 Accuracy 0.7958999872207642 Accuracies by class [array(0.752, dtype=float32), array(0.97, dtype=float32), array(0.814, dtype=float32), array(0.884, dtype=float32), array(0.82, dtype=float32), array(0.877, dtype=float32), array(0.534, dtype=float32), array(0.835, dtype=float32), array(0.825, dtype=float32), array(0.648, dtype=float32)]\n",
      "Iteration 33040 Training loss 0.014846197329461575 Validation loss 0.01606758125126362 Accuracy 0.8307999968528748 Accuracies by class [array(0.832, dtype=float32), array(0.965, dtype=float32), array(0.745, dtype=float32), array(0.89, dtype=float32), array(0.811, dtype=float32), array(0.913, dtype=float32), array(0.591, dtype=float32), array(0.91, dtype=float32), array(0.942, dtype=float32), array(0.709, dtype=float32)]\n",
      "Iteration 33050 Training loss 0.012600596062839031 Validation loss 0.015786299481987953 Accuracy 0.8342999815940857 Accuracies by class [array(0.761, dtype=float32), array(0.965, dtype=float32), array(0.823, dtype=float32), array(0.903, dtype=float32), array(0.706, dtype=float32), array(0.928, dtype=float32), array(0.637, dtype=float32), array(0.873, dtype=float32), array(0.937, dtype=float32), array(0.81, dtype=float32)]\n",
      "Iteration 33060 Training loss 0.010520062409341335 Validation loss 0.015080838464200497 Accuracy 0.8428999781608582 Accuracies by class [array(0.794, dtype=float32), array(0.966, dtype=float32), array(0.814, dtype=float32), array(0.909, dtype=float32), array(0.757, dtype=float32), array(0.921, dtype=float32), array(0.586, dtype=float32), array(0.907, dtype=float32), array(0.948, dtype=float32), array(0.827, dtype=float32)]\n",
      "Iteration 33070 Training loss 0.011474880389869213 Validation loss 0.01545413862913847 Accuracy 0.838100016117096 Accuracies by class [array(0.836, dtype=float32), array(0.969, dtype=float32), array(0.831, dtype=float32), array(0.845, dtype=float32), array(0.768, dtype=float32), array(0.889, dtype=float32), array(0.512, dtype=float32), array(0.873, dtype=float32), array(0.94, dtype=float32), array(0.918, dtype=float32)]\n",
      "Iteration 33080 Training loss 0.010420527309179306 Validation loss 0.015183158218860626 Accuracy 0.8413000106811523 Accuracies by class [array(0.85, dtype=float32), array(0.966, dtype=float32), array(0.837, dtype=float32), array(0.849, dtype=float32), array(0.699, dtype=float32), array(0.897, dtype=float32), array(0.557, dtype=float32), array(0.911, dtype=float32), array(0.947, dtype=float32), array(0.9, dtype=float32)]\n",
      "Iteration 33090 Training loss 0.012895246036350727 Validation loss 0.01540413312613964 Accuracy 0.8396000266075134 Accuracies by class [array(0.741, dtype=float32), array(0.96, dtype=float32), array(0.711, dtype=float32), array(0.91, dtype=float32), array(0.806, dtype=float32), array(0.942, dtype=float32), array(0.648, dtype=float32), array(0.803, dtype=float32), array(0.956, dtype=float32), array(0.919, dtype=float32)]\n",
      "Iteration 33100 Training loss 0.014406882226467133 Validation loss 0.015096421353518963 Accuracy 0.8424000144004822 Accuracies by class [array(0.812, dtype=float32), array(0.965, dtype=float32), array(0.825, dtype=float32), array(0.874, dtype=float32), array(0.818, dtype=float32), array(0.918, dtype=float32), array(0.431, dtype=float32), array(0.907, dtype=float32), array(0.965, dtype=float32), array(0.909, dtype=float32)]\n",
      "Iteration 33110 Training loss 0.01321177463978529 Validation loss 0.014891369268298149 Accuracy 0.8446999788284302 Accuracies by class [array(0.759, dtype=float32), array(0.961, dtype=float32), array(0.835, dtype=float32), array(0.86, dtype=float32), array(0.727, dtype=float32), array(0.913, dtype=float32), array(0.616, dtype=float32), array(0.891, dtype=float32), array(0.953, dtype=float32), array(0.932, dtype=float32)]\n",
      "Iteration 33120 Training loss 0.011643024161458015 Validation loss 0.015236835926771164 Accuracy 0.8424000144004822 Accuracies by class [array(0.778, dtype=float32), array(0.963, dtype=float32), array(0.622, dtype=float32), array(0.88, dtype=float32), array(0.784, dtype=float32), array(0.908, dtype=float32), array(0.704, dtype=float32), array(0.917, dtype=float32), array(0.959, dtype=float32), array(0.909, dtype=float32)]\n",
      "Iteration 33130 Training loss 0.011898520402610302 Validation loss 0.014895517379045486 Accuracy 0.8450000286102295 Accuracies by class [array(0.778, dtype=float32), array(0.962, dtype=float32), array(0.759, dtype=float32), array(0.865, dtype=float32), array(0.827, dtype=float32), array(0.876, dtype=float32), array(0.6, dtype=float32), array(0.933, dtype=float32), array(0.943, dtype=float32), array(0.907, dtype=float32)]\n",
      "Iteration 33140 Training loss 0.012389766983687878 Validation loss 0.01484617218375206 Accuracy 0.84579998254776 Accuracies by class [array(0.838, dtype=float32), array(0.965, dtype=float32), array(0.8, dtype=float32), array(0.834, dtype=float32), array(0.84, dtype=float32), array(0.882, dtype=float32), array(0.503, dtype=float32), array(0.938, dtype=float32), array(0.95, dtype=float32), array(0.908, dtype=float32)]\n",
      "Iteration 33150 Training loss 0.014086912386119366 Validation loss 0.016159849241375923 Accuracy 0.8317000269889832 Accuracies by class [array(0.637, dtype=float32), array(0.961, dtype=float32), array(0.817, dtype=float32), array(0.877, dtype=float32), array(0.828, dtype=float32), array(0.907, dtype=float32), array(0.545, dtype=float32), array(0.938, dtype=float32), array(0.946, dtype=float32), array(0.861, dtype=float32)]\n",
      "Iteration 33160 Training loss 0.011628082022070885 Validation loss 0.014219592325389385 Accuracy 0.8517000079154968 Accuracies by class [array(0.801, dtype=float32), array(0.962, dtype=float32), array(0.805, dtype=float32), array(0.903, dtype=float32), array(0.803, dtype=float32), array(0.911, dtype=float32), array(0.545, dtype=float32), array(0.911, dtype=float32), array(0.959, dtype=float32), array(0.917, dtype=float32)]\n",
      "Iteration 33170 Training loss 0.01243263017386198 Validation loss 0.014860016293823719 Accuracy 0.845300018787384 Accuracies by class [array(0.794, dtype=float32), array(0.954, dtype=float32), array(0.821, dtype=float32), array(0.928, dtype=float32), array(0.714, dtype=float32), array(0.883, dtype=float32), array(0.544, dtype=float32), array(0.926, dtype=float32), array(0.965, dtype=float32), array(0.924, dtype=float32)]\n",
      "Iteration 33180 Training loss 0.011343628168106079 Validation loss 0.01412784680724144 Accuracy 0.8529999852180481 Accuracies by class [array(0.777, dtype=float32), array(0.963, dtype=float32), array(0.766, dtype=float32), array(0.886, dtype=float32), array(0.784, dtype=float32), array(0.918, dtype=float32), array(0.637, dtype=float32), array(0.932, dtype=float32), array(0.971, dtype=float32), array(0.896, dtype=float32)]\n",
      "Iteration 33190 Training loss 0.012114710174500942 Validation loss 0.015345688909292221 Accuracy 0.8406000137329102 Accuracies by class [array(0.833, dtype=float32), array(0.963, dtype=float32), array(0.861, dtype=float32), array(0.922, dtype=float32), array(0.68, dtype=float32), array(0.904, dtype=float32), array(0.474, dtype=float32), array(0.952, dtype=float32), array(0.965, dtype=float32), array(0.852, dtype=float32)]\n",
      "Iteration 33200 Training loss 0.012673264369368553 Validation loss 0.014269618317484856 Accuracy 0.8510000109672546 Accuracies by class [array(0.793, dtype=float32), array(0.965, dtype=float32), array(0.73, dtype=float32), array(0.911, dtype=float32), array(0.82, dtype=float32), array(0.918, dtype=float32), array(0.59, dtype=float32), array(0.89, dtype=float32), array(0.959, dtype=float32), array(0.934, dtype=float32)]\n",
      "Iteration 33210 Training loss 0.012620164081454277 Validation loss 0.014477364718914032 Accuracy 0.8503999710083008 Accuracies by class [array(0.765, dtype=float32), array(0.965, dtype=float32), array(0.714, dtype=float32), array(0.873, dtype=float32), array(0.833, dtype=float32), array(0.89, dtype=float32), array(0.656, dtype=float32), array(0.95, dtype=float32), array(0.96, dtype=float32), array(0.898, dtype=float32)]\n",
      "Iteration 33220 Training loss 0.013128277845680714 Validation loss 0.014745145104825497 Accuracy 0.8468999862670898 Accuracies by class [array(0.859, dtype=float32), array(0.961, dtype=float32), array(0.788, dtype=float32), array(0.87, dtype=float32), array(0.826, dtype=float32), array(0.945, dtype=float32), array(0.493, dtype=float32), array(0.828, dtype=float32), array(0.967, dtype=float32), array(0.932, dtype=float32)]\n",
      "Iteration 33230 Training loss 0.011846707202494144 Validation loss 0.014407876878976822 Accuracy 0.850600004196167 Accuracies by class [array(0.857, dtype=float32), array(0.968, dtype=float32), array(0.803, dtype=float32), array(0.831, dtype=float32), array(0.779, dtype=float32), array(0.906, dtype=float32), array(0.569, dtype=float32), array(0.936, dtype=float32), array(0.955, dtype=float32), array(0.902, dtype=float32)]\n",
      "Iteration 33240 Training loss 0.012411934323608875 Validation loss 0.014928375370800495 Accuracy 0.8450000286102295 Accuracies by class [array(0.808, dtype=float32), array(0.957, dtype=float32), array(0.832, dtype=float32), array(0.928, dtype=float32), array(0.679, dtype=float32), array(0.915, dtype=float32), array(0.543, dtype=float32), array(0.908, dtype=float32), array(0.951, dtype=float32), array(0.929, dtype=float32)]\n",
      "Iteration 33250 Training loss 0.010630911216139793 Validation loss 0.014550126157701015 Accuracy 0.8489000201225281 Accuracies by class [array(0.783, dtype=float32), array(0.962, dtype=float32), array(0.755, dtype=float32), array(0.881, dtype=float32), array(0.72, dtype=float32), array(0.91, dtype=float32), array(0.682, dtype=float32), array(0.937, dtype=float32), array(0.949, dtype=float32), array(0.91, dtype=float32)]\n",
      "Iteration 33260 Training loss 0.012410253286361694 Validation loss 0.014084702357649803 Accuracy 0.853600025177002 Accuracies by class [array(0.803, dtype=float32), array(0.961, dtype=float32), array(0.737, dtype=float32), array(0.905, dtype=float32), array(0.846, dtype=float32), array(0.923, dtype=float32), array(0.565, dtype=float32), array(0.929, dtype=float32), array(0.959, dtype=float32), array(0.908, dtype=float32)]\n",
      "Iteration 33270 Training loss 0.015179709531366825 Validation loss 0.014438196085393429 Accuracy 0.8504999876022339 Accuracies by class [array(0.834, dtype=float32), array(0.967, dtype=float32), array(0.824, dtype=float32), array(0.88, dtype=float32), array(0.809, dtype=float32), array(0.931, dtype=float32), array(0.492, dtype=float32), array(0.875, dtype=float32), array(0.954, dtype=float32), array(0.939, dtype=float32)]\n",
      "Iteration 33280 Training loss 0.010858632624149323 Validation loss 0.014260286465287209 Accuracy 0.8517000079154968 Accuracies by class [array(0.821, dtype=float32), array(0.964, dtype=float32), array(0.787, dtype=float32), array(0.847, dtype=float32), array(0.838, dtype=float32), array(0.926, dtype=float32), array(0.545, dtype=float32), array(0.926, dtype=float32), array(0.964, dtype=float32), array(0.899, dtype=float32)]\n",
      "Iteration 33290 Training loss 0.013299096375703812 Validation loss 0.014266625046730042 Accuracy 0.8518000245094299 Accuracies by class [array(0.81, dtype=float32), array(0.961, dtype=float32), array(0.756, dtype=float32), array(0.899, dtype=float32), array(0.834, dtype=float32), array(0.952, dtype=float32), array(0.545, dtype=float32), array(0.863, dtype=float32), array(0.971, dtype=float32), array(0.927, dtype=float32)]\n",
      "Iteration 33300 Training loss 0.010901781730353832 Validation loss 0.014782804064452648 Accuracy 0.8475000262260437 Accuracies by class [array(0.847, dtype=float32), array(0.963, dtype=float32), array(0.78, dtype=float32), array(0.846, dtype=float32), array(0.869, dtype=float32), array(0.901, dtype=float32), array(0.45, dtype=float32), array(0.917, dtype=float32), array(0.964, dtype=float32), array(0.938, dtype=float32)]\n",
      "Iteration 33310 Training loss 0.012645985931158066 Validation loss 0.013983930461108685 Accuracy 0.8550000190734863 Accuracies by class [array(0.847, dtype=float32), array(0.957, dtype=float32), array(0.772, dtype=float32), array(0.875, dtype=float32), array(0.84, dtype=float32), array(0.92, dtype=float32), array(0.529, dtype=float32), array(0.925, dtype=float32), array(0.969, dtype=float32), array(0.916, dtype=float32)]\n",
      "Iteration 33320 Training loss 0.011911052279174328 Validation loss 0.01465196069329977 Accuracy 0.847599983215332 Accuracies by class [array(0.831, dtype=float32), array(0.962, dtype=float32), array(0.662, dtype=float32), array(0.911, dtype=float32), array(0.873, dtype=float32), array(0.913, dtype=float32), array(0.52, dtype=float32), array(0.927, dtype=float32), array(0.963, dtype=float32), array(0.914, dtype=float32)]\n",
      "Iteration 33330 Training loss 0.012371878139674664 Validation loss 0.014216093346476555 Accuracy 0.8522999882698059 Accuracies by class [array(0.805, dtype=float32), array(0.966, dtype=float32), array(0.677, dtype=float32), array(0.892, dtype=float32), array(0.827, dtype=float32), array(0.94, dtype=float32), array(0.634, dtype=float32), array(0.924, dtype=float32), array(0.965, dtype=float32), array(0.893, dtype=float32)]\n",
      "Iteration 33340 Training loss 0.010954349301755428 Validation loss 0.014775129035115242 Accuracy 0.8458999991416931 Accuracies by class [array(0.69, dtype=float32), array(0.967, dtype=float32), array(0.737, dtype=float32), array(0.897, dtype=float32), array(0.743, dtype=float32), array(0.924, dtype=float32), array(0.708, dtype=float32), array(0.919, dtype=float32), array(0.955, dtype=float32), array(0.919, dtype=float32)]\n",
      "Iteration 33350 Training loss 0.013029292225837708 Validation loss 0.01403878815472126 Accuracy 0.8533999919891357 Accuracies by class [array(0.774, dtype=float32), array(0.965, dtype=float32), array(0.772, dtype=float32), array(0.918, dtype=float32), array(0.796, dtype=float32), array(0.923, dtype=float32), array(0.582, dtype=float32), array(0.899, dtype=float32), array(0.971, dtype=float32), array(0.934, dtype=float32)]\n",
      "Iteration 33360 Training loss 0.012791414745151997 Validation loss 0.014380230568349361 Accuracy 0.850600004196167 Accuracies by class [array(0.76, dtype=float32), array(0.966, dtype=float32), array(0.758, dtype=float32), array(0.908, dtype=float32), array(0.731, dtype=float32), array(0.921, dtype=float32), array(0.66, dtype=float32), array(0.915, dtype=float32), array(0.96, dtype=float32), array(0.927, dtype=float32)]\n",
      "Iteration 33370 Training loss 0.011221710592508316 Validation loss 0.014038866385817528 Accuracy 0.8537999987602234 Accuracies by class [array(0.855, dtype=float32), array(0.967, dtype=float32), array(0.739, dtype=float32), array(0.899, dtype=float32), array(0.796, dtype=float32), array(0.924, dtype=float32), array(0.561, dtype=float32), array(0.913, dtype=float32), array(0.962, dtype=float32), array(0.922, dtype=float32)]\n",
      "Iteration 33380 Training loss 0.014006117358803749 Validation loss 0.015033392235636711 Accuracy 0.8442999720573425 Accuracies by class [array(0.677, dtype=float32), array(0.961, dtype=float32), array(0.802, dtype=float32), array(0.928, dtype=float32), array(0.759, dtype=float32), array(0.951, dtype=float32), array(0.603, dtype=float32), array(0.893, dtype=float32), array(0.956, dtype=float32), array(0.913, dtype=float32)]\n",
      "Iteration 33390 Training loss 0.009638217277824879 Validation loss 0.0156253594905138 Accuracy 0.8371999859809875 Accuracies by class [array(0.838, dtype=float32), array(0.956, dtype=float32), array(0.829, dtype=float32), array(0.88, dtype=float32), array(0.626, dtype=float32), array(0.93, dtype=float32), array(0.601, dtype=float32), array(0.949, dtype=float32), array(0.956, dtype=float32), array(0.807, dtype=float32)]\n",
      "Iteration 33400 Training loss 0.01467114221304655 Validation loss 0.01512385718524456 Accuracy 0.8439000248908997 Accuracies by class [array(0.798, dtype=float32), array(0.964, dtype=float32), array(0.813, dtype=float32), array(0.889, dtype=float32), array(0.84, dtype=float32), array(0.932, dtype=float32), array(0.41, dtype=float32), array(0.913, dtype=float32), array(0.967, dtype=float32), array(0.913, dtype=float32)]\n",
      "Iteration 33410 Training loss 0.011313281953334808 Validation loss 0.015277148224413395 Accuracy 0.8407999873161316 Accuracies by class [array(0.856, dtype=float32), array(0.963, dtype=float32), array(0.733, dtype=float32), array(0.82, dtype=float32), array(0.692, dtype=float32), array(0.931, dtype=float32), array(0.661, dtype=float32), array(0.934, dtype=float32), array(0.942, dtype=float32), array(0.876, dtype=float32)]\n",
      "Iteration 33420 Training loss 0.012800655327737331 Validation loss 0.01447910163551569 Accuracy 0.8503000140190125 Accuracies by class [array(0.867, dtype=float32), array(0.963, dtype=float32), array(0.738, dtype=float32), array(0.829, dtype=float32), array(0.853, dtype=float32), array(0.927, dtype=float32), array(0.531, dtype=float32), array(0.921, dtype=float32), array(0.964, dtype=float32), array(0.91, dtype=float32)]\n",
      "Iteration 33430 Training loss 0.013344889506697655 Validation loss 0.014678757637739182 Accuracy 0.847100019454956 Accuracies by class [array(0.829, dtype=float32), array(0.966, dtype=float32), array(0.83, dtype=float32), array(0.908, dtype=float32), array(0.689, dtype=float32), array(0.924, dtype=float32), array(0.56, dtype=float32), array(0.944, dtype=float32), array(0.955, dtype=float32), array(0.866, dtype=float32)]\n",
      "Iteration 33440 Training loss 0.0124184750020504 Validation loss 0.014646817930042744 Accuracy 0.8476999998092651 Accuracies by class [array(0.845, dtype=float32), array(0.966, dtype=float32), array(0.842, dtype=float32), array(0.879, dtype=float32), array(0.686, dtype=float32), array(0.935, dtype=float32), array(0.544, dtype=float32), array(0.918, dtype=float32), array(0.956, dtype=float32), array(0.906, dtype=float32)]\n",
      "Iteration 33450 Training loss 0.01049527246505022 Validation loss 0.01407952606678009 Accuracy 0.8531000018119812 Accuracies by class [array(0.796, dtype=float32), array(0.949, dtype=float32), array(0.786, dtype=float32), array(0.914, dtype=float32), array(0.78, dtype=float32), array(0.937, dtype=float32), array(0.595, dtype=float32), array(0.898, dtype=float32), array(0.961, dtype=float32), array(0.915, dtype=float32)]\n",
      "Iteration 33460 Training loss 0.011221474967896938 Validation loss 0.01542964018881321 Accuracy 0.840399980545044 Accuracies by class [array(0.747, dtype=float32), array(0.963, dtype=float32), array(0.682, dtype=float32), array(0.881, dtype=float32), array(0.902, dtype=float32), array(0.912, dtype=float32), array(0.509, dtype=float32), array(0.941, dtype=float32), array(0.962, dtype=float32), array(0.905, dtype=float32)]\n",
      "Iteration 33470 Training loss 0.011087121441960335 Validation loss 0.013976242393255234 Accuracy 0.8543999791145325 Accuracies by class [array(0.807, dtype=float32), array(0.968, dtype=float32), array(0.805, dtype=float32), array(0.83, dtype=float32), array(0.826, dtype=float32), array(0.943, dtype=float32), array(0.59, dtype=float32), array(0.91, dtype=float32), array(0.965, dtype=float32), array(0.9, dtype=float32)]\n",
      "Iteration 33480 Training loss 0.010445981286466122 Validation loss 0.013938771560788155 Accuracy 0.8549000024795532 Accuracies by class [array(0.834, dtype=float32), array(0.967, dtype=float32), array(0.84, dtype=float32), array(0.883, dtype=float32), array(0.742, dtype=float32), array(0.94, dtype=float32), array(0.556, dtype=float32), array(0.901, dtype=float32), array(0.96, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 33490 Training loss 0.0115586519241333 Validation loss 0.013754956424236298 Accuracy 0.8575000166893005 Accuracies by class [array(0.819, dtype=float32), array(0.962, dtype=float32), array(0.744, dtype=float32), array(0.877, dtype=float32), array(0.792, dtype=float32), array(0.929, dtype=float32), array(0.644, dtype=float32), array(0.919, dtype=float32), array(0.969, dtype=float32), array(0.92, dtype=float32)]\n",
      "Iteration 33500 Training loss 0.012417281046509743 Validation loss 0.014018157497048378 Accuracy 0.8540999889373779 Accuracies by class [array(0.866, dtype=float32), array(0.965, dtype=float32), array(0.82, dtype=float32), array(0.875, dtype=float32), array(0.786, dtype=float32), array(0.933, dtype=float32), array(0.492, dtype=float32), array(0.889, dtype=float32), array(0.964, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 33510 Training loss 0.011268464848399162 Validation loss 0.01419750228524208 Accuracy 0.852400004863739 Accuracies by class [array(0.804, dtype=float32), array(0.963, dtype=float32), array(0.729, dtype=float32), array(0.918, dtype=float32), array(0.73, dtype=float32), array(0.92, dtype=float32), array(0.668, dtype=float32), array(0.876, dtype=float32), array(0.958, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 33520 Training loss 0.012361682020127773 Validation loss 0.014412296935915947 Accuracy 0.8500999808311462 Accuracies by class [array(0.863, dtype=float32), array(0.969, dtype=float32), array(0.831, dtype=float32), array(0.836, dtype=float32), array(0.694, dtype=float32), array(0.919, dtype=float32), array(0.575, dtype=float32), array(0.913, dtype=float32), array(0.956, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 33530 Training loss 0.012020427733659744 Validation loss 0.013994306325912476 Accuracy 0.853600025177002 Accuracies by class [array(0.839, dtype=float32), array(0.97, dtype=float32), array(0.794, dtype=float32), array(0.832, dtype=float32), array(0.834, dtype=float32), array(0.926, dtype=float32), array(0.534, dtype=float32), array(0.923, dtype=float32), array(0.964, dtype=float32), array(0.92, dtype=float32)]\n",
      "Iteration 33540 Training loss 0.009468026459217072 Validation loss 0.013914070092141628 Accuracy 0.8550000190734863 Accuracies by class [array(0.837, dtype=float32), array(0.97, dtype=float32), array(0.785, dtype=float32), array(0.85, dtype=float32), array(0.781, dtype=float32), array(0.914, dtype=float32), array(0.598, dtype=float32), array(0.944, dtype=float32), array(0.97, dtype=float32), array(0.901, dtype=float32)]\n",
      "Iteration 33550 Training loss 0.010852878913283348 Validation loss 0.013908273540437222 Accuracy 0.8555999994277954 Accuracies by class [array(0.793, dtype=float32), array(0.971, dtype=float32), array(0.743, dtype=float32), array(0.854, dtype=float32), array(0.857, dtype=float32), array(0.921, dtype=float32), array(0.605, dtype=float32), array(0.911, dtype=float32), array(0.961, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 33560 Training loss 0.01248557772487402 Validation loss 0.014255676418542862 Accuracy 0.8517000079154968 Accuracies by class [array(0.837, dtype=float32), array(0.962, dtype=float32), array(0.827, dtype=float32), array(0.891, dtype=float32), array(0.659, dtype=float32), array(0.912, dtype=float32), array(0.6, dtype=float32), array(0.93, dtype=float32), array(0.962, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 33570 Training loss 0.01361836213618517 Validation loss 0.014487084932625294 Accuracy 0.8497999906539917 Accuracies by class [array(0.878, dtype=float32), array(0.97, dtype=float32), array(0.764, dtype=float32), array(0.845, dtype=float32), array(0.847, dtype=float32), array(0.923, dtype=float32), array(0.466, dtype=float32), array(0.934, dtype=float32), array(0.959, dtype=float32), array(0.912, dtype=float32)]\n",
      "Iteration 33580 Training loss 0.01374067086726427 Validation loss 0.014363398775458336 Accuracy 0.8504999876022339 Accuracies by class [array(0.814, dtype=float32), array(0.958, dtype=float32), array(0.644, dtype=float32), array(0.919, dtype=float32), array(0.78, dtype=float32), array(0.921, dtype=float32), array(0.662, dtype=float32), array(0.901, dtype=float32), array(0.962, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 33590 Training loss 0.010591743513941765 Validation loss 0.013975080102682114 Accuracy 0.8533999919891357 Accuracies by class [array(0.713, dtype=float32), array(0.964, dtype=float32), array(0.786, dtype=float32), array(0.901, dtype=float32), array(0.801, dtype=float32), array(0.921, dtype=float32), array(0.638, dtype=float32), array(0.92, dtype=float32), array(0.962, dtype=float32), array(0.928, dtype=float32)]\n",
      "Iteration 33600 Training loss 0.011468312703073025 Validation loss 0.014245066791772842 Accuracy 0.8507999777793884 Accuracies by class [array(0.751, dtype=float32), array(0.967, dtype=float32), array(0.763, dtype=float32), array(0.882, dtype=float32), array(0.747, dtype=float32), array(0.902, dtype=float32), array(0.694, dtype=float32), array(0.945, dtype=float32), array(0.939, dtype=float32), array(0.918, dtype=float32)]\n",
      "Iteration 33610 Training loss 0.01020255871117115 Validation loss 0.013691812753677368 Accuracy 0.8582000136375427 Accuracies by class [array(0.808, dtype=float32), array(0.967, dtype=float32), array(0.784, dtype=float32), array(0.874, dtype=float32), array(0.808, dtype=float32), array(0.92, dtype=float32), array(0.613, dtype=float32), array(0.941, dtype=float32), array(0.952, dtype=float32), array(0.915, dtype=float32)]\n",
      "Iteration 33620 Training loss 0.012673509307205677 Validation loss 0.014025094918906689 Accuracy 0.8535000085830688 Accuracies by class [array(0.775, dtype=float32), array(0.967, dtype=float32), array(0.754, dtype=float32), array(0.884, dtype=float32), array(0.857, dtype=float32), array(0.931, dtype=float32), array(0.565, dtype=float32), array(0.933, dtype=float32), array(0.966, dtype=float32), array(0.903, dtype=float32)]\n",
      "Iteration 33630 Training loss 0.010819726623594761 Validation loss 0.013600096106529236 Accuracy 0.8582000136375427 Accuracies by class [array(0.825, dtype=float32), array(0.967, dtype=float32), array(0.757, dtype=float32), array(0.859, dtype=float32), array(0.842, dtype=float32), array(0.924, dtype=float32), array(0.596, dtype=float32), array(0.927, dtype=float32), array(0.962, dtype=float32), array(0.923, dtype=float32)]\n",
      "Iteration 33640 Training loss 0.009476479142904282 Validation loss 0.013398793525993824 Accuracy 0.8611000180244446 Accuracies by class [array(0.782, dtype=float32), array(0.967, dtype=float32), array(0.807, dtype=float32), array(0.892, dtype=float32), array(0.786, dtype=float32), array(0.931, dtype=float32), array(0.631, dtype=float32), array(0.898, dtype=float32), array(0.967, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 33650 Training loss 0.012011113576591015 Validation loss 0.013944827020168304 Accuracy 0.8550000190734863 Accuracies by class [array(0.813, dtype=float32), array(0.964, dtype=float32), array(0.774, dtype=float32), array(0.897, dtype=float32), array(0.726, dtype=float32), array(0.926, dtype=float32), array(0.659, dtype=float32), array(0.947, dtype=float32), array(0.945, dtype=float32), array(0.899, dtype=float32)]\n",
      "Iteration 33660 Training loss 0.011598952114582062 Validation loss 0.013731880113482475 Accuracy 0.8561999797821045 Accuracies by class [array(0.83, dtype=float32), array(0.966, dtype=float32), array(0.822, dtype=float32), array(0.911, dtype=float32), array(0.757, dtype=float32), array(0.934, dtype=float32), array(0.531, dtype=float32), array(0.914, dtype=float32), array(0.96, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 33670 Training loss 0.009694568812847137 Validation loss 0.014122082851827145 Accuracy 0.8535000085830688 Accuracies by class [array(0.798, dtype=float32), array(0.959, dtype=float32), array(0.77, dtype=float32), array(0.917, dtype=float32), array(0.746, dtype=float32), array(0.926, dtype=float32), array(0.626, dtype=float32), array(0.957, dtype=float32), array(0.967, dtype=float32), array(0.869, dtype=float32)]\n",
      "Iteration 33680 Training loss 0.009818307124078274 Validation loss 0.01434948481619358 Accuracy 0.8500999808311462 Accuracies by class [array(0.735, dtype=float32), array(0.963, dtype=float32), array(0.769, dtype=float32), array(0.857, dtype=float32), array(0.858, dtype=float32), array(0.948, dtype=float32), array(0.578, dtype=float32), array(0.895, dtype=float32), array(0.962, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 33690 Training loss 0.011725442484021187 Validation loss 0.01412727776914835 Accuracy 0.8532000184059143 Accuracies by class [array(0.749, dtype=float32), array(0.961, dtype=float32), array(0.819, dtype=float32), array(0.923, dtype=float32), array(0.785, dtype=float32), array(0.922, dtype=float32), array(0.555, dtype=float32), array(0.929, dtype=float32), array(0.96, dtype=float32), array(0.929, dtype=float32)]\n",
      "Iteration 33700 Training loss 0.009602992795407772 Validation loss 0.0137721486389637 Accuracy 0.8574000000953674 Accuracies by class [array(0.761, dtype=float32), array(0.967, dtype=float32), array(0.7, dtype=float32), array(0.886, dtype=float32), array(0.845, dtype=float32), array(0.925, dtype=float32), array(0.67, dtype=float32), array(0.942, dtype=float32), array(0.969, dtype=float32), array(0.909, dtype=float32)]\n",
      "Iteration 33710 Training loss 0.011052139103412628 Validation loss 0.013562490232288837 Accuracy 0.8582000136375427 Accuracies by class [array(0.824, dtype=float32), array(0.965, dtype=float32), array(0.819, dtype=float32), array(0.856, dtype=float32), array(0.773, dtype=float32), array(0.92, dtype=float32), array(0.621, dtype=float32), array(0.911, dtype=float32), array(0.951, dtype=float32), array(0.942, dtype=float32)]\n",
      "Iteration 33720 Training loss 0.010526671074330807 Validation loss 0.013807559385895729 Accuracy 0.8555999994277954 Accuracies by class [array(0.823, dtype=float32), array(0.962, dtype=float32), array(0.744, dtype=float32), array(0.829, dtype=float32), array(0.845, dtype=float32), array(0.92, dtype=float32), array(0.61, dtype=float32), array(0.937, dtype=float32), array(0.962, dtype=float32), array(0.924, dtype=float32)]\n",
      "Iteration 33730 Training loss 0.010508058592677116 Validation loss 0.014358284883201122 Accuracy 0.8507999777793884 Accuracies by class [array(0.721, dtype=float32), array(0.963, dtype=float32), array(0.741, dtype=float32), array(0.865, dtype=float32), array(0.877, dtype=float32), array(0.936, dtype=float32), array(0.599, dtype=float32), array(0.913, dtype=float32), array(0.965, dtype=float32), array(0.928, dtype=float32)]\n",
      "Iteration 33740 Training loss 0.012340730056166649 Validation loss 0.01368949469178915 Accuracy 0.8574000000953674 Accuracies by class [array(0.76, dtype=float32), array(0.963, dtype=float32), array(0.827, dtype=float32), array(0.899, dtype=float32), array(0.756, dtype=float32), array(0.921, dtype=float32), array(0.629, dtype=float32), array(0.921, dtype=float32), array(0.959, dtype=float32), array(0.939, dtype=float32)]\n",
      "Iteration 33750 Training loss 0.01289889682084322 Validation loss 0.014319763518869877 Accuracy 0.850600004196167 Accuracies by class [array(0.852, dtype=float32), array(0.968, dtype=float32), array(0.795, dtype=float32), array(0.879, dtype=float32), array(0.845, dtype=float32), array(0.921, dtype=float32), array(0.432, dtype=float32), array(0.949, dtype=float32), array(0.958, dtype=float32), array(0.907, dtype=float32)]\n",
      "Iteration 33760 Training loss 0.00914471223950386 Validation loss 0.013475715182721615 Accuracy 0.8598999977111816 Accuracies by class [array(0.785, dtype=float32), array(0.963, dtype=float32), array(0.81, dtype=float32), array(0.888, dtype=float32), array(0.739, dtype=float32), array(0.948, dtype=float32), array(0.678, dtype=float32), array(0.939, dtype=float32), array(0.949, dtype=float32), array(0.9, dtype=float32)]\n",
      "Iteration 33770 Training loss 0.011921879835426807 Validation loss 0.013891594484448433 Accuracy 0.8543000221252441 Accuracies by class [array(0.806, dtype=float32), array(0.972, dtype=float32), array(0.855, dtype=float32), array(0.852, dtype=float32), array(0.762, dtype=float32), array(0.944, dtype=float32), array(0.555, dtype=float32), array(0.909, dtype=float32), array(0.959, dtype=float32), array(0.929, dtype=float32)]\n",
      "Iteration 33780 Training loss 0.009949932806193829 Validation loss 0.01415130216628313 Accuracy 0.8529999852180481 Accuracies by class [array(0.838, dtype=float32), array(0.973, dtype=float32), array(0.742, dtype=float32), array(0.832, dtype=float32), array(0.879, dtype=float32), array(0.944, dtype=float32), array(0.514, dtype=float32), array(0.899, dtype=float32), array(0.967, dtype=float32), array(0.942, dtype=float32)]\n",
      "Iteration 33790 Training loss 0.011151864193379879 Validation loss 0.013626349158585072 Accuracy 0.859000027179718 Accuracies by class [array(0.764, dtype=float32), array(0.964, dtype=float32), array(0.749, dtype=float32), array(0.894, dtype=float32), array(0.84, dtype=float32), array(0.945, dtype=float32), array(0.624, dtype=float32), array(0.908, dtype=float32), array(0.963, dtype=float32), array(0.939, dtype=float32)]\n",
      "Iteration 33800 Training loss 0.011140760965645313 Validation loss 0.013754589483141899 Accuracy 0.8568999767303467 Accuracies by class [array(0.805, dtype=float32), array(0.965, dtype=float32), array(0.702, dtype=float32), array(0.881, dtype=float32), array(0.792, dtype=float32), array(0.943, dtype=float32), array(0.68, dtype=float32), array(0.906, dtype=float32), array(0.96, dtype=float32), array(0.935, dtype=float32)]\n",
      "Iteration 33810 Training loss 0.013084986247122288 Validation loss 0.014534004963934422 Accuracy 0.8489000201225281 Accuracies by class [array(0.862, dtype=float32), array(0.969, dtype=float32), array(0.787, dtype=float32), array(0.832, dtype=float32), array(0.845, dtype=float32), array(0.951, dtype=float32), array(0.45, dtype=float32), array(0.888, dtype=float32), array(0.971, dtype=float32), array(0.934, dtype=float32)]\n",
      "Iteration 33820 Training loss 0.010966363362967968 Validation loss 0.014218869619071484 Accuracy 0.8529999852180481 Accuracies by class [array(0.769, dtype=float32), array(0.965, dtype=float32), array(0.797, dtype=float32), array(0.87, dtype=float32), array(0.852, dtype=float32), array(0.931, dtype=float32), array(0.554, dtype=float32), array(0.868, dtype=float32), array(0.962, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 33830 Training loss 0.010849671438336372 Validation loss 0.013720043003559113 Accuracy 0.8574000000953674 Accuracies by class [array(0.846, dtype=float32), array(0.967, dtype=float32), array(0.771, dtype=float32), array(0.868, dtype=float32), array(0.844, dtype=float32), array(0.937, dtype=float32), array(0.524, dtype=float32), array(0.896, dtype=float32), array(0.972, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 33840 Training loss 0.011689321137964725 Validation loss 0.013953541405498981 Accuracy 0.8557999730110168 Accuracies by class [array(0.829, dtype=float32), array(0.968, dtype=float32), array(0.819, dtype=float32), array(0.889, dtype=float32), array(0.818, dtype=float32), array(0.922, dtype=float32), array(0.484, dtype=float32), array(0.953, dtype=float32), array(0.969, dtype=float32), array(0.907, dtype=float32)]\n",
      "Iteration 33850 Training loss 0.010118654929101467 Validation loss 0.013595332391560078 Accuracy 0.8586999773979187 Accuracies by class [array(0.831, dtype=float32), array(0.964, dtype=float32), array(0.815, dtype=float32), array(0.889, dtype=float32), array(0.819, dtype=float32), array(0.944, dtype=float32), array(0.507, dtype=float32), array(0.929, dtype=float32), array(0.964, dtype=float32), array(0.925, dtype=float32)]\n",
      "Iteration 33860 Training loss 0.009264389984309673 Validation loss 0.013614816591143608 Accuracy 0.8579000234603882 Accuracies by class [array(0.758, dtype=float32), array(0.964, dtype=float32), array(0.757, dtype=float32), array(0.871, dtype=float32), array(0.816, dtype=float32), array(0.918, dtype=float32), array(0.671, dtype=float32), array(0.928, dtype=float32), array(0.955, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 33870 Training loss 0.011653761379420757 Validation loss 0.014960609376430511 Accuracy 0.8453999757766724 Accuracies by class [array(0.577, dtype=float32), array(0.962, dtype=float32), array(0.788, dtype=float32), array(0.864, dtype=float32), array(0.808, dtype=float32), array(0.943, dtype=float32), array(0.709, dtype=float32), array(0.899, dtype=float32), array(0.96, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 33880 Training loss 0.012720456346869469 Validation loss 0.014263536781072617 Accuracy 0.8515999913215637 Accuracies by class [array(0.757, dtype=float32), array(0.964, dtype=float32), array(0.659, dtype=float32), array(0.898, dtype=float32), array(0.884, dtype=float32), array(0.924, dtype=float32), array(0.602, dtype=float32), array(0.921, dtype=float32), array(0.964, dtype=float32), array(0.943, dtype=float32)]\n",
      "Iteration 33890 Training loss 0.011296303942799568 Validation loss 0.014935772866010666 Accuracy 0.8456000089645386 Accuracies by class [array(0.603, dtype=float32), array(0.962, dtype=float32), array(0.728, dtype=float32), array(0.872, dtype=float32), array(0.834, dtype=float32), array(0.926, dtype=float32), array(0.714, dtype=float32), array(0.892, dtype=float32), array(0.965, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 33900 Training loss 0.010668609291315079 Validation loss 0.014128393493592739 Accuracy 0.8525999784469604 Accuracies by class [array(0.811, dtype=float32), array(0.964, dtype=float32), array(0.71, dtype=float32), array(0.905, dtype=float32), array(0.872, dtype=float32), array(0.942, dtype=float32), array(0.511, dtype=float32), array(0.89, dtype=float32), array(0.97, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 33910 Training loss 0.010830074548721313 Validation loss 0.014174983836710453 Accuracy 0.8522999882698059 Accuracies by class [array(0.717, dtype=float32), array(0.966, dtype=float32), array(0.855, dtype=float32), array(0.906, dtype=float32), array(0.78, dtype=float32), array(0.929, dtype=float32), array(0.553, dtype=float32), array(0.945, dtype=float32), array(0.964, dtype=float32), array(0.908, dtype=float32)]\n",
      "Iteration 33920 Training loss 0.009818444959819317 Validation loss 0.013478776440024376 Accuracy 0.8598999977111816 Accuracies by class [array(0.844, dtype=float32), array(0.972, dtype=float32), array(0.829, dtype=float32), array(0.86, dtype=float32), array(0.771, dtype=float32), array(0.932, dtype=float32), array(0.551, dtype=float32), array(0.937, dtype=float32), array(0.971, dtype=float32), array(0.932, dtype=float32)]\n",
      "Iteration 33930 Training loss 0.011918293312191963 Validation loss 0.014438620768487453 Accuracy 0.8501999974250793 Accuracies by class [array(0.661, dtype=float32), array(0.966, dtype=float32), array(0.774, dtype=float32), array(0.83, dtype=float32), array(0.768, dtype=float32), array(0.936, dtype=float32), array(0.75, dtype=float32), array(0.931, dtype=float32), array(0.949, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 33940 Training loss 0.014166469685733318 Validation loss 0.01439165323972702 Accuracy 0.8503000140190125 Accuracies by class [array(0.869, dtype=float32), array(0.972, dtype=float32), array(0.797, dtype=float32), array(0.818, dtype=float32), array(0.864, dtype=float32), array(0.929, dtype=float32), array(0.422, dtype=float32), array(0.92, dtype=float32), array(0.962, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 33950 Training loss 0.010228845290839672 Validation loss 0.013618849217891693 Accuracy 0.858299970626831 Accuracies by class [array(0.842, dtype=float32), array(0.967, dtype=float32), array(0.797, dtype=float32), array(0.877, dtype=float32), array(0.808, dtype=float32), array(0.926, dtype=float32), array(0.539, dtype=float32), array(0.944, dtype=float32), array(0.961, dtype=float32), array(0.922, dtype=float32)]\n",
      "Iteration 33960 Training loss 0.010939031839370728 Validation loss 0.013846301473677158 Accuracy 0.85589998960495 Accuracies by class [array(0.847, dtype=float32), array(0.962, dtype=float32), array(0.745, dtype=float32), array(0.88, dtype=float32), array(0.859, dtype=float32), array(0.919, dtype=float32), array(0.519, dtype=float32), array(0.942, dtype=float32), array(0.954, dtype=float32), array(0.932, dtype=float32)]\n",
      "Iteration 33970 Training loss 0.010547812096774578 Validation loss 0.013676718808710575 Accuracy 0.8575000166893005 Accuracies by class [array(0.863, dtype=float32), array(0.964, dtype=float32), array(0.804, dtype=float32), array(0.863, dtype=float32), array(0.843, dtype=float32), array(0.935, dtype=float32), array(0.498, dtype=float32), array(0.893, dtype=float32), array(0.957, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 33980 Training loss 0.010882336646318436 Validation loss 0.013674040324985981 Accuracy 0.857699990272522 Accuracies by class [array(0.871, dtype=float32), array(0.962, dtype=float32), array(0.803, dtype=float32), array(0.844, dtype=float32), array(0.829, dtype=float32), array(0.94, dtype=float32), array(0.527, dtype=float32), array(0.929, dtype=float32), array(0.947, dtype=float32), array(0.925, dtype=float32)]\n",
      "Iteration 33990 Training loss 0.011125958524644375 Validation loss 0.013831817544996738 Accuracy 0.85589998960495 Accuracies by class [array(0.821, dtype=float32), array(0.969, dtype=float32), array(0.743, dtype=float32), array(0.854, dtype=float32), array(0.735, dtype=float32), array(0.939, dtype=float32), array(0.669, dtype=float32), array(0.907, dtype=float32), array(0.972, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 34000 Training loss 0.009767614305019379 Validation loss 0.013879065401852131 Accuracy 0.8553000092506409 Accuracies by class [array(0.842, dtype=float32), array(0.955, dtype=float32), array(0.697, dtype=float32), array(0.897, dtype=float32), array(0.849, dtype=float32), array(0.934, dtype=float32), array(0.572, dtype=float32), array(0.938, dtype=float32), array(0.965, dtype=float32), array(0.904, dtype=float32)]\n",
      "Iteration 34010 Training loss 0.011344308033585548 Validation loss 0.014409789815545082 Accuracy 0.8503999710083008 Accuracies by class [array(0.77, dtype=float32), array(0.964, dtype=float32), array(0.715, dtype=float32), array(0.881, dtype=float32), array(0.878, dtype=float32), array(0.936, dtype=float32), array(0.554, dtype=float32), array(0.935, dtype=float32), array(0.956, dtype=float32), array(0.915, dtype=float32)]\n",
      "Iteration 34020 Training loss 0.011934074573218822 Validation loss 0.013620319776237011 Accuracy 0.8587999939918518 Accuracies by class [array(0.82, dtype=float32), array(0.956, dtype=float32), array(0.721, dtype=float32), array(0.912, dtype=float32), array(0.804, dtype=float32), array(0.943, dtype=float32), array(0.617, dtype=float32), array(0.926, dtype=float32), array(0.963, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 34030 Training loss 0.010769763961434364 Validation loss 0.014387419447302818 Accuracy 0.8496000170707703 Accuracies by class [array(0.812, dtype=float32), array(0.969, dtype=float32), array(0.668, dtype=float32), array(0.821, dtype=float32), array(0.813, dtype=float32), array(0.931, dtype=float32), array(0.681, dtype=float32), array(0.946, dtype=float32), array(0.94, dtype=float32), array(0.915, dtype=float32)]\n",
      "Iteration 34040 Training loss 0.013090415857732296 Validation loss 0.014003033749759197 Accuracy 0.8539999723434448 Accuracies by class [array(0.812, dtype=float32), array(0.965, dtype=float32), array(0.856, dtype=float32), array(0.9, dtype=float32), array(0.739, dtype=float32), array(0.947, dtype=float32), array(0.516, dtype=float32), array(0.92, dtype=float32), array(0.956, dtype=float32), array(0.929, dtype=float32)]\n",
      "Iteration 34050 Training loss 0.009080859832465649 Validation loss 0.013827555812895298 Accuracy 0.8557999730110168 Accuracies by class [array(0.819, dtype=float32), array(0.965, dtype=float32), array(0.804, dtype=float32), array(0.883, dtype=float32), array(0.725, dtype=float32), array(0.934, dtype=float32), array(0.613, dtype=float32), array(0.94, dtype=float32), array(0.95, dtype=float32), array(0.925, dtype=float32)]\n",
      "Iteration 34060 Training loss 0.012430677190423012 Validation loss 0.01509800273925066 Accuracy 0.8439000248908997 Accuracies by class [array(0.878, dtype=float32), array(0.968, dtype=float32), array(0.889, dtype=float32), array(0.826, dtype=float32), array(0.701, dtype=float32), array(0.932, dtype=float32), array(0.418, dtype=float32), array(0.916, dtype=float32), array(0.96, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 34070 Training loss 0.011737612076103687 Validation loss 0.013452276587486267 Accuracy 0.8596000075340271 Accuracies by class [array(0.818, dtype=float32), array(0.967, dtype=float32), array(0.745, dtype=float32), array(0.902, dtype=float32), array(0.814, dtype=float32), array(0.949, dtype=float32), array(0.585, dtype=float32), array(0.921, dtype=float32), array(0.957, dtype=float32), array(0.938, dtype=float32)]\n",
      "Iteration 34080 Training loss 0.011204039677977562 Validation loss 0.014451172202825546 Accuracy 0.848800003528595 Accuracies by class [array(0.684, dtype=float32), array(0.97, dtype=float32), array(0.723, dtype=float32), array(0.875, dtype=float32), array(0.838, dtype=float32), array(0.931, dtype=float32), array(0.647, dtype=float32), array(0.946, dtype=float32), array(0.97, dtype=float32), array(0.904, dtype=float32)]\n",
      "Iteration 34090 Training loss 0.010209704749286175 Validation loss 0.013868171721696854 Accuracy 0.8550999760627747 Accuracies by class [array(0.743, dtype=float32), array(0.971, dtype=float32), array(0.835, dtype=float32), array(0.878, dtype=float32), array(0.76, dtype=float32), array(0.939, dtype=float32), array(0.612, dtype=float32), array(0.885, dtype=float32), array(0.97, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 34100 Training loss 0.011686816811561584 Validation loss 0.013713095337152481 Accuracy 0.8567000031471252 Accuracies by class [array(0.818, dtype=float32), array(0.965, dtype=float32), array(0.84, dtype=float32), array(0.897, dtype=float32), array(0.804, dtype=float32), array(0.935, dtype=float32), array(0.488, dtype=float32), array(0.91, dtype=float32), array(0.962, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 34110 Training loss 0.010424123145639896 Validation loss 0.01418925542384386 Accuracy 0.852400004863739 Accuracies by class [array(0.735, dtype=float32), array(0.966, dtype=float32), array(0.749, dtype=float32), array(0.922, dtype=float32), array(0.844, dtype=float32), array(0.949, dtype=float32), array(0.572, dtype=float32), array(0.879, dtype=float32), array(0.959, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 34120 Training loss 0.010981092229485512 Validation loss 0.014556274749338627 Accuracy 0.8478000164031982 Accuracies by class [array(0.739, dtype=float32), array(0.961, dtype=float32), array(0.603, dtype=float32), array(0.878, dtype=float32), array(0.881, dtype=float32), array(0.938, dtype=float32), array(0.66, dtype=float32), array(0.907, dtype=float32), array(0.966, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 34130 Training loss 0.014321397058665752 Validation loss 0.014879057183861732 Accuracy 0.8442999720573425 Accuracies by class [array(0.689, dtype=float32), array(0.963, dtype=float32), array(0.726, dtype=float32), array(0.844, dtype=float32), array(0.723, dtype=float32), array(0.948, dtype=float32), array(0.766, dtype=float32), array(0.899, dtype=float32), array(0.955, dtype=float32), array(0.93, dtype=float32)]\n",
      "Iteration 34140 Training loss 0.011068485677242279 Validation loss 0.013367662206292152 Accuracy 0.8600999712944031 Accuracies by class [array(0.811, dtype=float32), array(0.96, dtype=float32), array(0.743, dtype=float32), array(0.875, dtype=float32), array(0.832, dtype=float32), array(0.935, dtype=float32), array(0.626, dtype=float32), array(0.933, dtype=float32), array(0.96, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 34150 Training loss 0.012326091527938843 Validation loss 0.01345233153551817 Accuracy 0.8597999811172485 Accuracies by class [array(0.848, dtype=float32), array(0.965, dtype=float32), array(0.793, dtype=float32), array(0.852, dtype=float32), array(0.814, dtype=float32), array(0.941, dtype=float32), array(0.564, dtype=float32), array(0.922, dtype=float32), array(0.959, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 34160 Training loss 0.009445570409297943 Validation loss 0.013858186081051826 Accuracy 0.85589998960495 Accuracies by class [array(0.856, dtype=float32), array(0.96, dtype=float32), array(0.783, dtype=float32), array(0.859, dtype=float32), array(0.854, dtype=float32), array(0.944, dtype=float32), array(0.476, dtype=float32), array(0.929, dtype=float32), array(0.961, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 34170 Training loss 0.010802390985190868 Validation loss 0.013625373132526875 Accuracy 0.8586999773979187 Accuracies by class [array(0.719, dtype=float32), array(0.96, dtype=float32), array(0.816, dtype=float32), array(0.88, dtype=float32), array(0.821, dtype=float32), array(0.938, dtype=float32), array(0.644, dtype=float32), array(0.899, dtype=float32), array(0.957, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 34180 Training loss 0.010983867570757866 Validation loss 0.013311785645782948 Accuracy 0.8611000180244446 Accuracies by class [array(0.793, dtype=float32), array(0.96, dtype=float32), array(0.819, dtype=float32), array(0.876, dtype=float32), array(0.805, dtype=float32), array(0.932, dtype=float32), array(0.596, dtype=float32), array(0.944, dtype=float32), array(0.957, dtype=float32), array(0.929, dtype=float32)]\n",
      "Iteration 34190 Training loss 0.010710150003433228 Validation loss 0.014189627952873707 Accuracy 0.8521999716758728 Accuracies by class [array(0.786, dtype=float32), array(0.961, dtype=float32), array(0.827, dtype=float32), array(0.905, dtype=float32), array(0.836, dtype=float32), array(0.924, dtype=float32), array(0.456, dtype=float32), array(0.951, dtype=float32), array(0.953, dtype=float32), array(0.923, dtype=float32)]\n",
      "Iteration 34200 Training loss 0.009408134035766125 Validation loss 0.013513916172087193 Accuracy 0.8597000241279602 Accuracies by class [array(0.736, dtype=float32), array(0.957, dtype=float32), array(0.786, dtype=float32), array(0.897, dtype=float32), array(0.754, dtype=float32), array(0.935, dtype=float32), array(0.705, dtype=float32), array(0.924, dtype=float32), array(0.958, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 34210 Training loss 0.009669926948845387 Validation loss 0.013137055560946465 Accuracy 0.8633999824523926 Accuracies by class [array(0.823, dtype=float32), array(0.963, dtype=float32), array(0.802, dtype=float32), array(0.88, dtype=float32), array(0.781, dtype=float32), array(0.942, dtype=float32), array(0.621, dtype=float32), array(0.909, dtype=float32), array(0.966, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 34220 Training loss 0.009483524598181248 Validation loss 0.013732219114899635 Accuracy 0.8572999835014343 Accuracies by class [array(0.743, dtype=float32), array(0.965, dtype=float32), array(0.852, dtype=float32), array(0.879, dtype=float32), array(0.727, dtype=float32), array(0.943, dtype=float32), array(0.643, dtype=float32), array(0.909, dtype=float32), array(0.965, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 34230 Training loss 0.013650740496814251 Validation loss 0.014855578541755676 Accuracy 0.8447999954223633 Accuracies by class [array(0.582, dtype=float32), array(0.967, dtype=float32), array(0.724, dtype=float32), array(0.87, dtype=float32), array(0.851, dtype=float32), array(0.944, dtype=float32), array(0.69, dtype=float32), array(0.921, dtype=float32), array(0.966, dtype=float32), array(0.933, dtype=float32)]\n",
      "Iteration 34240 Training loss 0.01007937267422676 Validation loss 0.013492551632225513 Accuracy 0.8596000075340271 Accuracies by class [array(0.743, dtype=float32), array(0.965, dtype=float32), array(0.715, dtype=float32), array(0.867, dtype=float32), array(0.823, dtype=float32), array(0.934, dtype=float32), array(0.716, dtype=float32), array(0.931, dtype=float32), array(0.959, dtype=float32), array(0.943, dtype=float32)]\n",
      "Iteration 34250 Training loss 0.010760885663330555 Validation loss 0.013595672324299812 Accuracy 0.858299970626831 Accuracies by class [array(0.845, dtype=float32), array(0.965, dtype=float32), array(0.816, dtype=float32), array(0.862, dtype=float32), array(0.821, dtype=float32), array(0.928, dtype=float32), array(0.51, dtype=float32), array(0.961, dtype=float32), array(0.963, dtype=float32), array(0.912, dtype=float32)]\n",
      "Iteration 34260 Training loss 0.011436361819505692 Validation loss 0.013693037442862988 Accuracy 0.8565999865531921 Accuracies by class [array(0.748, dtype=float32), array(0.961, dtype=float32), array(0.811, dtype=float32), array(0.893, dtype=float32), array(0.747, dtype=float32), array(0.926, dtype=float32), array(0.675, dtype=float32), array(0.969, dtype=float32), array(0.957, dtype=float32), array(0.879, dtype=float32)]\n",
      "Iteration 34270 Training loss 0.012053482234477997 Validation loss 0.013654688373208046 Accuracy 0.8572999835014343 Accuracies by class [array(0.847, dtype=float32), array(0.961, dtype=float32), array(0.825, dtype=float32), array(0.889, dtype=float32), array(0.725, dtype=float32), array(0.923, dtype=float32), array(0.594, dtype=float32), array(0.896, dtype=float32), array(0.949, dtype=float32), array(0.964, dtype=float32)]\n",
      "Iteration 34280 Training loss 0.01125195063650608 Validation loss 0.0135364830493927 Accuracy 0.8586000204086304 Accuracies by class [array(0.776, dtype=float32), array(0.965, dtype=float32), array(0.824, dtype=float32), array(0.914, dtype=float32), array(0.714, dtype=float32), array(0.937, dtype=float32), array(0.627, dtype=float32), array(0.94, dtype=float32), array(0.958, dtype=float32), array(0.931, dtype=float32)]\n",
      "Iteration 34290 Training loss 0.009843411855399609 Validation loss 0.013969172723591328 Accuracy 0.8544999957084656 Accuracies by class [array(0.787, dtype=float32), array(0.971, dtype=float32), array(0.718, dtype=float32), array(0.863, dtype=float32), array(0.883, dtype=float32), array(0.94, dtype=float32), array(0.56, dtype=float32), array(0.926, dtype=float32), array(0.962, dtype=float32), array(0.935, dtype=float32)]\n",
      "Iteration 34300 Training loss 0.009220646694302559 Validation loss 0.013432632200419903 Accuracy 0.8603000044822693 Accuracies by class [array(0.861, dtype=float32), array(0.966, dtype=float32), array(0.802, dtype=float32), array(0.899, dtype=float32), array(0.824, dtype=float32), array(0.931, dtype=float32), array(0.482, dtype=float32), array(0.941, dtype=float32), array(0.961, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 34310 Training loss 0.010628663003444672 Validation loss 0.013226506300270557 Accuracy 0.8628000020980835 Accuracies by class [array(0.824, dtype=float32), array(0.968, dtype=float32), array(0.729, dtype=float32), array(0.901, dtype=float32), array(0.797, dtype=float32), array(0.936, dtype=float32), array(0.635, dtype=float32), array(0.922, dtype=float32), array(0.963, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 34320 Training loss 0.011238436214625835 Validation loss 0.013218149542808533 Accuracy 0.8614000082015991 Accuracies by class [array(0.817, dtype=float32), array(0.966, dtype=float32), array(0.788, dtype=float32), array(0.887, dtype=float32), array(0.844, dtype=float32), array(0.942, dtype=float32), array(0.554, dtype=float32), array(0.914, dtype=float32), array(0.968, dtype=float32), array(0.934, dtype=float32)]\n",
      "Iteration 34330 Training loss 0.012670283205807209 Validation loss 0.01380391139537096 Accuracy 0.8555999994277954 Accuracies by class [array(0.856, dtype=float32), array(0.964, dtype=float32), array(0.761, dtype=float32), array(0.841, dtype=float32), array(0.852, dtype=float32), array(0.945, dtype=float32), array(0.529, dtype=float32), array(0.94, dtype=float32), array(0.959, dtype=float32), array(0.909, dtype=float32)]\n",
      "Iteration 34340 Training loss 0.011761929839849472 Validation loss 0.013057378120720387 Accuracy 0.8639000058174133 Accuracies by class [array(0.812, dtype=float32), array(0.966, dtype=float32), array(0.773, dtype=float32), array(0.871, dtype=float32), array(0.802, dtype=float32), array(0.936, dtype=float32), array(0.639, dtype=float32), array(0.938, dtype=float32), array(0.965, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 34350 Training loss 0.010646789334714413 Validation loss 0.013290664181113243 Accuracy 0.86080002784729 Accuracies by class [array(0.81, dtype=float32), array(0.969, dtype=float32), array(0.821, dtype=float32), array(0.847, dtype=float32), array(0.775, dtype=float32), array(0.939, dtype=float32), array(0.61, dtype=float32), array(0.939, dtype=float32), array(0.972, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 34360 Training loss 0.010780957527458668 Validation loss 0.014497575350105762 Accuracy 0.848800003528595 Accuracies by class [array(0.76, dtype=float32), array(0.962, dtype=float32), array(0.613, dtype=float32), array(0.868, dtype=float32), array(0.842, dtype=float32), array(0.932, dtype=float32), array(0.695, dtype=float32), array(0.964, dtype=float32), array(0.969, dtype=float32), array(0.883, dtype=float32)]\n",
      "Iteration 34370 Training loss 0.011475369334220886 Validation loss 0.013461650349199772 Accuracy 0.8608999848365784 Accuracies by class [array(0.767, dtype=float32), array(0.957, dtype=float32), array(0.838, dtype=float32), array(0.895, dtype=float32), array(0.774, dtype=float32), array(0.913, dtype=float32), array(0.605, dtype=float32), array(0.934, dtype=float32), array(0.968, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 34380 Training loss 0.012054666876792908 Validation loss 0.013325337320566177 Accuracy 0.8605999946594238 Accuracies by class [array(0.798, dtype=float32), array(0.96, dtype=float32), array(0.835, dtype=float32), array(0.899, dtype=float32), array(0.787, dtype=float32), array(0.93, dtype=float32), array(0.557, dtype=float32), array(0.931, dtype=float32), array(0.963, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 34390 Training loss 0.011911897920072079 Validation loss 0.013797527179121971 Accuracy 0.8574000000953674 Accuracies by class [array(0.84, dtype=float32), array(0.963, dtype=float32), array(0.639, dtype=float32), array(0.861, dtype=float32), array(0.856, dtype=float32), array(0.943, dtype=float32), array(0.637, dtype=float32), array(0.912, dtype=float32), array(0.969, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 34400 Training loss 0.011727077886462212 Validation loss 0.013036406598985195 Accuracy 0.8640999794006348 Accuracies by class [array(0.816, dtype=float32), array(0.963, dtype=float32), array(0.79, dtype=float32), array(0.897, dtype=float32), array(0.799, dtype=float32), array(0.937, dtype=float32), array(0.601, dtype=float32), array(0.938, dtype=float32), array(0.97, dtype=float32), array(0.93, dtype=float32)]\n",
      "Iteration 34410 Training loss 0.00988898053765297 Validation loss 0.014494703151285648 Accuracy 0.8500000238418579 Accuracies by class [array(0.875, dtype=float32), array(0.972, dtype=float32), array(0.807, dtype=float32), array(0.848, dtype=float32), array(0.84, dtype=float32), array(0.924, dtype=float32), array(0.404, dtype=float32), array(0.96, dtype=float32), array(0.965, dtype=float32), array(0.905, dtype=float32)]\n",
      "Iteration 34420 Training loss 0.008898690342903137 Validation loss 0.013598723337054253 Accuracy 0.8585000038146973 Accuracies by class [array(0.759, dtype=float32), array(0.964, dtype=float32), array(0.742, dtype=float32), array(0.921, dtype=float32), array(0.824, dtype=float32), array(0.94, dtype=float32), array(0.619, dtype=float32), array(0.897, dtype=float32), array(0.962, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 34430 Training loss 0.00880797952413559 Validation loss 0.013602370396256447 Accuracy 0.8582000136375427 Accuracies by class [array(0.788, dtype=float32), array(0.972, dtype=float32), array(0.697, dtype=float32), array(0.862, dtype=float32), array(0.863, dtype=float32), array(0.923, dtype=float32), array(0.643, dtype=float32), array(0.93, dtype=float32), array(0.951, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 34440 Training loss 0.009986023418605328 Validation loss 0.013593298383057117 Accuracy 0.8585000038146973 Accuracies by class [array(0.763, dtype=float32), array(0.968, dtype=float32), array(0.718, dtype=float32), array(0.878, dtype=float32), array(0.824, dtype=float32), array(0.939, dtype=float32), array(0.676, dtype=float32), array(0.922, dtype=float32), array(0.951, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 34450 Training loss 0.013195730745792389 Validation loss 0.013654189184308052 Accuracy 0.8564000129699707 Accuracies by class [array(0.749, dtype=float32), array(0.969, dtype=float32), array(0.742, dtype=float32), array(0.879, dtype=float32), array(0.862, dtype=float32), array(0.944, dtype=float32), array(0.601, dtype=float32), array(0.894, dtype=float32), array(0.969, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 34460 Training loss 0.012298096902668476 Validation loss 0.013527174480259418 Accuracy 0.8593999743461609 Accuracies by class [array(0.82, dtype=float32), array(0.965, dtype=float32), array(0.794, dtype=float32), array(0.921, dtype=float32), array(0.832, dtype=float32), array(0.932, dtype=float32), array(0.494, dtype=float32), array(0.917, dtype=float32), array(0.964, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 34470 Training loss 0.009216608479619026 Validation loss 0.01346534676849842 Accuracy 0.8597000241279602 Accuracies by class [array(0.726, dtype=float32), array(0.966, dtype=float32), array(0.783, dtype=float32), array(0.861, dtype=float32), array(0.795, dtype=float32), array(0.937, dtype=float32), array(0.716, dtype=float32), array(0.916, dtype=float32), array(0.953, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 34480 Training loss 0.00853163842111826 Validation loss 0.013283930718898773 Accuracy 0.8608999848365784 Accuracies by class [array(0.827, dtype=float32), array(0.966, dtype=float32), array(0.717, dtype=float32), array(0.857, dtype=float32), array(0.864, dtype=float32), array(0.938, dtype=float32), array(0.599, dtype=float32), array(0.936, dtype=float32), array(0.973, dtype=float32), array(0.932, dtype=float32)]\n",
      "Iteration 34490 Training loss 0.009943338111042976 Validation loss 0.013290910981595516 Accuracy 0.8622999787330627 Accuracies by class [array(0.8, dtype=float32), array(0.966, dtype=float32), array(0.813, dtype=float32), array(0.897, dtype=float32), array(0.819, dtype=float32), array(0.94, dtype=float32), array(0.549, dtype=float32), array(0.919, dtype=float32), array(0.971, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 34500 Training loss 0.010648270137608051 Validation loss 0.013658316805958748 Accuracy 0.8579999804496765 Accuracies by class [array(0.851, dtype=float32), array(0.965, dtype=float32), array(0.837, dtype=float32), array(0.88, dtype=float32), array(0.793, dtype=float32), array(0.938, dtype=float32), array(0.467, dtype=float32), array(0.916, dtype=float32), array(0.974, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 34510 Training loss 0.009869878180325031 Validation loss 0.013115599751472473 Accuracy 0.8633000254631042 Accuracies by class [array(0.843, dtype=float32), array(0.965, dtype=float32), array(0.822, dtype=float32), array(0.85, dtype=float32), array(0.767, dtype=float32), array(0.917, dtype=float32), array(0.634, dtype=float32), array(0.962, dtype=float32), array(0.965, dtype=float32), array(0.908, dtype=float32)]\n",
      "Iteration 34520 Training loss 0.01292468886822462 Validation loss 0.013423220254480839 Accuracy 0.8597000241279602 Accuracies by class [array(0.72, dtype=float32), array(0.957, dtype=float32), array(0.778, dtype=float32), array(0.869, dtype=float32), array(0.775, dtype=float32), array(0.942, dtype=float32), array(0.723, dtype=float32), array(0.931, dtype=float32), array(0.966, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 34530 Training loss 0.013311815448105335 Validation loss 0.014222966507077217 Accuracy 0.8526999950408936 Accuracies by class [array(0.808, dtype=float32), array(0.964, dtype=float32), array(0.744, dtype=float32), array(0.827, dtype=float32), array(0.898, dtype=float32), array(0.94, dtype=float32), array(0.512, dtype=float32), array(0.927, dtype=float32), array(0.967, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 34540 Training loss 0.00948205403983593 Validation loss 0.013197631575167179 Accuracy 0.8618999719619751 Accuracies by class [array(0.855, dtype=float32), array(0.963, dtype=float32), array(0.847, dtype=float32), array(0.886, dtype=float32), array(0.783, dtype=float32), array(0.937, dtype=float32), array(0.517, dtype=float32), array(0.923, dtype=float32), array(0.961, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 34550 Training loss 0.010317079722881317 Validation loss 0.013370116241276264 Accuracy 0.8605999946594238 Accuracies by class [array(0.858, dtype=float32), array(0.958, dtype=float32), array(0.745, dtype=float32), array(0.843, dtype=float32), array(0.85, dtype=float32), array(0.946, dtype=float32), array(0.589, dtype=float32), array(0.897, dtype=float32), array(0.966, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 34560 Training loss 0.011466622352600098 Validation loss 0.014027198776602745 Accuracy 0.8540999889373779 Accuracies by class [array(0.663, dtype=float32), array(0.966, dtype=float32), array(0.763, dtype=float32), array(0.879, dtype=float32), array(0.82, dtype=float32), array(0.939, dtype=float32), array(0.712, dtype=float32), array(0.95, dtype=float32), array(0.941, dtype=float32), array(0.908, dtype=float32)]\n",
      "Iteration 34570 Training loss 0.009936398826539516 Validation loss 0.013007262721657753 Accuracy 0.8640999794006348 Accuracies by class [array(0.822, dtype=float32), array(0.965, dtype=float32), array(0.754, dtype=float32), array(0.874, dtype=float32), array(0.853, dtype=float32), array(0.916, dtype=float32), array(0.612, dtype=float32), array(0.928, dtype=float32), array(0.958, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 34580 Training loss 0.011356774717569351 Validation loss 0.013128549791872501 Accuracy 0.8626000285148621 Accuracies by class [array(0.8, dtype=float32), array(0.967, dtype=float32), array(0.812, dtype=float32), array(0.88, dtype=float32), array(0.826, dtype=float32), array(0.923, dtype=float32), array(0.587, dtype=float32), array(0.923, dtype=float32), array(0.946, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 34590 Training loss 0.009957702830433846 Validation loss 0.013410004787147045 Accuracy 0.8600000143051147 Accuracies by class [array(0.784, dtype=float32), array(0.965, dtype=float32), array(0.676, dtype=float32), array(0.89, dtype=float32), array(0.829, dtype=float32), array(0.936, dtype=float32), array(0.698, dtype=float32), array(0.953, dtype=float32), array(0.963, dtype=float32), array(0.906, dtype=float32)]\n",
      "Iteration 34600 Training loss 0.009137831628322601 Validation loss 0.01343479473143816 Accuracy 0.8586999773979187 Accuracies by class [array(0.778, dtype=float32), array(0.969, dtype=float32), array(0.745, dtype=float32), array(0.866, dtype=float32), array(0.842, dtype=float32), array(0.924, dtype=float32), array(0.633, dtype=float32), array(0.962, dtype=float32), array(0.966, dtype=float32), array(0.902, dtype=float32)]\n",
      "Iteration 34610 Training loss 0.011142897419631481 Validation loss 0.013328073546290398 Accuracy 0.8619999885559082 Accuracies by class [array(0.732, dtype=float32), array(0.965, dtype=float32), array(0.782, dtype=float32), array(0.905, dtype=float32), array(0.838, dtype=float32), array(0.934, dtype=float32), array(0.619, dtype=float32), array(0.944, dtype=float32), array(0.969, dtype=float32), array(0.932, dtype=float32)]\n",
      "Iteration 34620 Training loss 0.010155715048313141 Validation loss 0.013688242062926292 Accuracy 0.8575000166893005 Accuracies by class [array(0.772, dtype=float32), array(0.967, dtype=float32), array(0.779, dtype=float32), array(0.912, dtype=float32), array(0.845, dtype=float32), array(0.938, dtype=float32), array(0.529, dtype=float32), array(0.925, dtype=float32), array(0.961, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 34630 Training loss 0.00984345842152834 Validation loss 0.013278058730065823 Accuracy 0.8611999750137329 Accuracies by class [array(0.765, dtype=float32), array(0.963, dtype=float32), array(0.748, dtype=float32), array(0.896, dtype=float32), array(0.84, dtype=float32), array(0.94, dtype=float32), array(0.642, dtype=float32), array(0.903, dtype=float32), array(0.965, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 34640 Training loss 0.010706517845392227 Validation loss 0.013179803267121315 Accuracy 0.8621000051498413 Accuracies by class [array(0.858, dtype=float32), array(0.968, dtype=float32), array(0.776, dtype=float32), array(0.87, dtype=float32), array(0.814, dtype=float32), array(0.943, dtype=float32), array(0.556, dtype=float32), array(0.92, dtype=float32), array(0.971, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 34650 Training loss 0.010122721083462238 Validation loss 0.014953415840864182 Accuracy 0.8442000150680542 Accuracies by class [array(0.739, dtype=float32), array(0.968, dtype=float32), array(0.728, dtype=float32), array(0.875, dtype=float32), array(0.61, dtype=float32), array(0.942, dtype=float32), array(0.758, dtype=float32), array(0.919, dtype=float32), array(0.964, dtype=float32), array(0.939, dtype=float32)]\n",
      "Iteration 34660 Training loss 0.008858286775648594 Validation loss 0.013270796276628971 Accuracy 0.8605999946594238 Accuracies by class [array(0.771, dtype=float32), array(0.973, dtype=float32), array(0.829, dtype=float32), array(0.878, dtype=float32), array(0.772, dtype=float32), array(0.948, dtype=float32), array(0.625, dtype=float32), array(0.941, dtype=float32), array(0.969, dtype=float32), array(0.9, dtype=float32)]\n",
      "Iteration 34670 Training loss 0.008246826939284801 Validation loss 0.013357525691390038 Accuracy 0.8601999878883362 Accuracies by class [array(0.782, dtype=float32), array(0.972, dtype=float32), array(0.779, dtype=float32), array(0.87, dtype=float32), array(0.851, dtype=float32), array(0.954, dtype=float32), array(0.581, dtype=float32), array(0.9, dtype=float32), array(0.971, dtype=float32), array(0.942, dtype=float32)]\n",
      "Iteration 34680 Training loss 0.012050514109432697 Validation loss 0.013374934904277325 Accuracy 0.8600000143051147 Accuracies by class [array(0.856, dtype=float32), array(0.968, dtype=float32), array(0.76, dtype=float32), array(0.875, dtype=float32), array(0.848, dtype=float32), array(0.94, dtype=float32), array(0.513, dtype=float32), array(0.939, dtype=float32), array(0.971, dtype=float32), array(0.93, dtype=float32)]\n",
      "Iteration 34690 Training loss 0.011632843874394894 Validation loss 0.013478732667863369 Accuracy 0.8583999872207642 Accuracies by class [array(0.802, dtype=float32), array(0.97, dtype=float32), array(0.822, dtype=float32), array(0.88, dtype=float32), array(0.83, dtype=float32), array(0.934, dtype=float32), array(0.516, dtype=float32), array(0.958, dtype=float32), array(0.968, dtype=float32), array(0.904, dtype=float32)]\n",
      "Iteration 34700 Training loss 0.010344129987061024 Validation loss 0.013211098499596119 Accuracy 0.8623999953269958 Accuracies by class [array(0.818, dtype=float32), array(0.971, dtype=float32), array(0.775, dtype=float32), array(0.883, dtype=float32), array(0.856, dtype=float32), array(0.938, dtype=float32), array(0.536, dtype=float32), array(0.943, dtype=float32), array(0.969, dtype=float32), array(0.935, dtype=float32)]\n",
      "Iteration 34710 Training loss 0.01033723820000887 Validation loss 0.013120174407958984 Accuracy 0.8636999726295471 Accuracies by class [array(0.772, dtype=float32), array(0.966, dtype=float32), array(0.838, dtype=float32), array(0.9, dtype=float32), array(0.789, dtype=float32), array(0.939, dtype=float32), array(0.596, dtype=float32), array(0.933, dtype=float32), array(0.962, dtype=float32), array(0.942, dtype=float32)]\n",
      "Iteration 34720 Training loss 0.010132146999239922 Validation loss 0.012926295399665833 Accuracy 0.8644000291824341 Accuracies by class [array(0.789, dtype=float32), array(0.966, dtype=float32), array(0.81, dtype=float32), array(0.895, dtype=float32), array(0.822, dtype=float32), array(0.935, dtype=float32), array(0.595, dtype=float32), array(0.925, dtype=float32), array(0.958, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 34730 Training loss 0.010253717191517353 Validation loss 0.014025800861418247 Accuracy 0.8533999919891357 Accuracies by class [array(0.664, dtype=float32), array(0.958, dtype=float32), array(0.718, dtype=float32), array(0.913, dtype=float32), array(0.781, dtype=float32), array(0.922, dtype=float32), array(0.742, dtype=float32), array(0.959, dtype=float32), array(0.958, dtype=float32), array(0.919, dtype=float32)]\n",
      "Iteration 34740 Training loss 0.00915206503123045 Validation loss 0.013756628148257732 Accuracy 0.85589998960495 Accuracies by class [array(0.679, dtype=float32), array(0.963, dtype=float32), array(0.77, dtype=float32), array(0.907, dtype=float32), array(0.818, dtype=float32), array(0.948, dtype=float32), array(0.665, dtype=float32), array(0.886, dtype=float32), array(0.964, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 34750 Training loss 0.01087869517505169 Validation loss 0.013129404745995998 Accuracy 0.8618999719619751 Accuracies by class [array(0.814, dtype=float32), array(0.964, dtype=float32), array(0.752, dtype=float32), array(0.882, dtype=float32), array(0.831, dtype=float32), array(0.937, dtype=float32), array(0.6, dtype=float32), array(0.941, dtype=float32), array(0.97, dtype=float32), array(0.928, dtype=float32)]\n",
      "Iteration 34760 Training loss 0.010948090814054012 Validation loss 0.012982930056750774 Accuracy 0.8644000291824341 Accuracies by class [array(0.851, dtype=float32), array(0.963, dtype=float32), array(0.818, dtype=float32), array(0.884, dtype=float32), array(0.809, dtype=float32), array(0.937, dtype=float32), array(0.536, dtype=float32), array(0.946, dtype=float32), array(0.968, dtype=float32), array(0.932, dtype=float32)]\n",
      "Iteration 34770 Training loss 0.011329707689583302 Validation loss 0.013112514279782772 Accuracy 0.8634999990463257 Accuracies by class [array(0.82, dtype=float32), array(0.963, dtype=float32), array(0.765, dtype=float32), array(0.902, dtype=float32), array(0.769, dtype=float32), array(0.933, dtype=float32), array(0.637, dtype=float32), array(0.939, dtype=float32), array(0.966, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 34780 Training loss 0.010621219873428345 Validation loss 0.0132100535556674 Accuracy 0.8628000020980835 Accuracies by class [array(0.866, dtype=float32), array(0.966, dtype=float32), array(0.825, dtype=float32), array(0.885, dtype=float32), array(0.757, dtype=float32), array(0.938, dtype=float32), array(0.561, dtype=float32), array(0.943, dtype=float32), array(0.96, dtype=float32), array(0.927, dtype=float32)]\n",
      "Iteration 34790 Training loss 0.010032550431787968 Validation loss 0.01356806792318821 Accuracy 0.8586999773979187 Accuracies by class [array(0.838, dtype=float32), array(0.966, dtype=float32), array(0.778, dtype=float32), array(0.908, dtype=float32), array(0.815, dtype=float32), array(0.94, dtype=float32), array(0.534, dtype=float32), array(0.914, dtype=float32), array(0.937, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 34800 Training loss 0.011594247072935104 Validation loss 0.01426756102591753 Accuracy 0.8513000011444092 Accuracies by class [array(0.661, dtype=float32), array(0.96, dtype=float32), array(0.837, dtype=float32), array(0.919, dtype=float32), array(0.694, dtype=float32), array(0.939, dtype=float32), array(0.681, dtype=float32), array(0.953, dtype=float32), array(0.954, dtype=float32), array(0.915, dtype=float32)]\n",
      "Iteration 34810 Training loss 0.010661346837878227 Validation loss 0.013682623393833637 Accuracy 0.8568999767303467 Accuracies by class [array(0.749, dtype=float32), array(0.967, dtype=float32), array(0.832, dtype=float32), array(0.901, dtype=float32), array(0.743, dtype=float32), array(0.943, dtype=float32), array(0.617, dtype=float32), array(0.889, dtype=float32), array(0.965, dtype=float32), array(0.963, dtype=float32)]\n",
      "Iteration 34820 Training loss 0.013559202663600445 Validation loss 0.013508935458958149 Accuracy 0.858299970626831 Accuracies by class [array(0.812, dtype=float32), array(0.969, dtype=float32), array(0.793, dtype=float32), array(0.889, dtype=float32), array(0.71, dtype=float32), array(0.941, dtype=float32), array(0.662, dtype=float32), array(0.957, dtype=float32), array(0.968, dtype=float32), array(0.882, dtype=float32)]\n",
      "Iteration 34830 Training loss 0.009587558917701244 Validation loss 0.013235039077699184 Accuracy 0.8618000149726868 Accuracies by class [array(0.851, dtype=float32), array(0.97, dtype=float32), array(0.761, dtype=float32), array(0.903, dtype=float32), array(0.814, dtype=float32), array(0.95, dtype=float32), array(0.555, dtype=float32), array(0.898, dtype=float32), array(0.968, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 34840 Training loss 0.0106055224314332 Validation loss 0.01360376738011837 Accuracy 0.8585000038146973 Accuracies by class [array(0.79, dtype=float32), array(0.967, dtype=float32), array(0.705, dtype=float32), array(0.899, dtype=float32), array(0.742, dtype=float32), array(0.932, dtype=float32), array(0.713, dtype=float32), array(0.923, dtype=float32), array(0.962, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 34850 Training loss 0.010136795230209827 Validation loss 0.0130775710567832 Accuracy 0.8632000088691711 Accuracies by class [array(0.79, dtype=float32), array(0.966, dtype=float32), array(0.738, dtype=float32), array(0.877, dtype=float32), array(0.772, dtype=float32), array(0.937, dtype=float32), array(0.717, dtype=float32), array(0.929, dtype=float32), array(0.965, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 34860 Training loss 0.007682774215936661 Validation loss 0.013055426999926567 Accuracy 0.8628000020980835 Accuracies by class [array(0.816, dtype=float32), array(0.969, dtype=float32), array(0.794, dtype=float32), array(0.876, dtype=float32), array(0.834, dtype=float32), array(0.93, dtype=float32), array(0.567, dtype=float32), array(0.929, dtype=float32), array(0.964, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 34870 Training loss 0.010060968808829784 Validation loss 0.013826252892613411 Accuracy 0.8553000092506409 Accuracies by class [array(0.743, dtype=float32), array(0.968, dtype=float32), array(0.822, dtype=float32), array(0.908, dtype=float32), array(0.813, dtype=float32), array(0.947, dtype=float32), array(0.543, dtype=float32), array(0.908, dtype=float32), array(0.95, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 34880 Training loss 0.010130754671990871 Validation loss 0.013094216585159302 Accuracy 0.8621000051498413 Accuracies by class [array(0.777, dtype=float32), array(0.967, dtype=float32), array(0.799, dtype=float32), array(0.898, dtype=float32), array(0.809, dtype=float32), array(0.948, dtype=float32), array(0.604, dtype=float32), array(0.915, dtype=float32), array(0.959, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 34890 Training loss 0.011419892311096191 Validation loss 0.013373402878642082 Accuracy 0.8603000044822693 Accuracies by class [array(0.858, dtype=float32), array(0.966, dtype=float32), array(0.817, dtype=float32), array(0.891, dtype=float32), array(0.833, dtype=float32), array(0.931, dtype=float32), array(0.475, dtype=float32), array(0.951, dtype=float32), array(0.959, dtype=float32), array(0.922, dtype=float32)]\n",
      "Iteration 34900 Training loss 0.011878862977027893 Validation loss 0.013244608417153358 Accuracy 0.8618000149726868 Accuracies by class [array(0.778, dtype=float32), array(0.964, dtype=float32), array(0.811, dtype=float32), array(0.918, dtype=float32), array(0.744, dtype=float32), array(0.926, dtype=float32), array(0.634, dtype=float32), array(0.948, dtype=float32), array(0.96, dtype=float32), array(0.935, dtype=float32)]\n",
      "Iteration 34910 Training loss 0.007776498328894377 Validation loss 0.012943457812070847 Accuracy 0.8636999726295471 Accuracies by class [array(0.812, dtype=float32), array(0.963, dtype=float32), array(0.767, dtype=float32), array(0.898, dtype=float32), array(0.811, dtype=float32), array(0.93, dtype=float32), array(0.621, dtype=float32), array(0.94, dtype=float32), array(0.959, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 34920 Training loss 0.012876882217824459 Validation loss 0.013336178846657276 Accuracy 0.8610000014305115 Accuracies by class [array(0.861, dtype=float32), array(0.967, dtype=float32), array(0.835, dtype=float32), array(0.875, dtype=float32), array(0.774, dtype=float32), array(0.951, dtype=float32), array(0.542, dtype=float32), array(0.868, dtype=float32), array(0.971, dtype=float32), array(0.966, dtype=float32)]\n",
      "Iteration 34930 Training loss 0.00849955528974533 Validation loss 0.0129973404109478 Accuracy 0.8640999794006348 Accuracies by class [array(0.819, dtype=float32), array(0.966, dtype=float32), array(0.797, dtype=float32), array(0.878, dtype=float32), array(0.786, dtype=float32), array(0.931, dtype=float32), array(0.623, dtype=float32), array(0.922, dtype=float32), array(0.968, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 34940 Training loss 0.010606305673718452 Validation loss 0.013528247363865376 Accuracy 0.8593000173568726 Accuracies by class [array(0.78, dtype=float32), array(0.962, dtype=float32), array(0.768, dtype=float32), array(0.927, dtype=float32), array(0.838, dtype=float32), array(0.939, dtype=float32), array(0.55, dtype=float32), array(0.902, dtype=float32), array(0.971, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 34950 Training loss 0.009390355087816715 Validation loss 0.013315556570887566 Accuracy 0.8611000180244446 Accuracies by class [array(0.811, dtype=float32), array(0.965, dtype=float32), array(0.781, dtype=float32), array(0.898, dtype=float32), array(0.748, dtype=float32), array(0.937, dtype=float32), array(0.648, dtype=float32), array(0.898, dtype=float32), array(0.965, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 34960 Training loss 0.011743194423615932 Validation loss 0.01424630731344223 Accuracy 0.8521000146865845 Accuracies by class [array(0.804, dtype=float32), array(0.969, dtype=float32), array(0.812, dtype=float32), array(0.872, dtype=float32), array(0.86, dtype=float32), array(0.93, dtype=float32), array(0.434, dtype=float32), array(0.915, dtype=float32), array(0.964, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 34970 Training loss 0.010221999138593674 Validation loss 0.013475030660629272 Accuracy 0.8600000143051147 Accuracies by class [array(0.757, dtype=float32), array(0.965, dtype=float32), array(0.787, dtype=float32), array(0.915, dtype=float32), array(0.853, dtype=float32), array(0.937, dtype=float32), array(0.555, dtype=float32), array(0.917, dtype=float32), array(0.963, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 34980 Training loss 0.009678248316049576 Validation loss 0.01312608364969492 Accuracy 0.8618000149726868 Accuracies by class [array(0.88, dtype=float32), array(0.968, dtype=float32), array(0.794, dtype=float32), array(0.887, dtype=float32), array(0.788, dtype=float32), array(0.945, dtype=float32), array(0.526, dtype=float32), array(0.93, dtype=float32), array(0.966, dtype=float32), array(0.934, dtype=float32)]\n",
      "Iteration 34990 Training loss 0.010281465947628021 Validation loss 0.013492265716195107 Accuracy 0.859000027179718 Accuracies by class [array(0.814, dtype=float32), array(0.965, dtype=float32), array(0.747, dtype=float32), array(0.917, dtype=float32), array(0.757, dtype=float32), array(0.939, dtype=float32), array(0.619, dtype=float32), array(0.9, dtype=float32), array(0.974, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 35000 Training loss 0.01048719696700573 Validation loss 0.013023805804550648 Accuracy 0.8633000254631042 Accuracies by class [array(0.81, dtype=float32), array(0.965, dtype=float32), array(0.753, dtype=float32), array(0.904, dtype=float32), array(0.847, dtype=float32), array(0.936, dtype=float32), array(0.577, dtype=float32), array(0.939, dtype=float32), array(0.968, dtype=float32), array(0.934, dtype=float32)]\n",
      "Iteration 35010 Training loss 0.009883789345622063 Validation loss 0.013461846858263016 Accuracy 0.859000027179718 Accuracies by class [array(0.825, dtype=float32), array(0.971, dtype=float32), array(0.779, dtype=float32), array(0.853, dtype=float32), array(0.806, dtype=float32), array(0.939, dtype=float32), array(0.611, dtype=float32), array(0.888, dtype=float32), array(0.96, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 35020 Training loss 0.010652052238583565 Validation loss 0.013293218798935413 Accuracy 0.8605999946594238 Accuracies by class [array(0.831, dtype=float32), array(0.97, dtype=float32), array(0.805, dtype=float32), array(0.875, dtype=float32), array(0.811, dtype=float32), array(0.938, dtype=float32), array(0.556, dtype=float32), array(0.89, dtype=float32), array(0.969, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 35030 Training loss 0.00856917817145586 Validation loss 0.01355770230293274 Accuracy 0.8585000038146973 Accuracies by class [array(0.765, dtype=float32), array(0.96, dtype=float32), array(0.786, dtype=float32), array(0.915, dtype=float32), array(0.827, dtype=float32), array(0.926, dtype=float32), array(0.558, dtype=float32), array(0.92, dtype=float32), array(0.969, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 35040 Training loss 0.009690281935036182 Validation loss 0.014053422957658768 Accuracy 0.8529999852180481 Accuracies by class [array(0.618, dtype=float32), array(0.967, dtype=float32), array(0.785, dtype=float32), array(0.914, dtype=float32), array(0.817, dtype=float32), array(0.934, dtype=float32), array(0.657, dtype=float32), array(0.924, dtype=float32), array(0.968, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 35050 Training loss 0.010498148389160633 Validation loss 0.014081527478992939 Accuracy 0.8539999723434448 Accuracies by class [array(0.663, dtype=float32), array(0.967, dtype=float32), array(0.727, dtype=float32), array(0.886, dtype=float32), array(0.852, dtype=float32), array(0.922, dtype=float32), array(0.678, dtype=float32), array(0.948, dtype=float32), array(0.96, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 35060 Training loss 0.007756290957331657 Validation loss 0.01314905658364296 Accuracy 0.862500011920929 Accuracies by class [array(0.743, dtype=float32), array(0.968, dtype=float32), array(0.805, dtype=float32), array(0.885, dtype=float32), array(0.812, dtype=float32), array(0.939, dtype=float32), array(0.633, dtype=float32), array(0.931, dtype=float32), array(0.966, dtype=float32), array(0.943, dtype=float32)]\n",
      "Iteration 35070 Training loss 0.008757449686527252 Validation loss 0.01336945965886116 Accuracy 0.8607000112533569 Accuracies by class [array(0.758, dtype=float32), array(0.97, dtype=float32), array(0.743, dtype=float32), array(0.863, dtype=float32), array(0.871, dtype=float32), array(0.945, dtype=float32), array(0.622, dtype=float32), array(0.924, dtype=float32), array(0.967, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 35080 Training loss 0.009576620534062386 Validation loss 0.01293853297829628 Accuracy 0.864799976348877 Accuracies by class [array(0.828, dtype=float32), array(0.97, dtype=float32), array(0.784, dtype=float32), array(0.859, dtype=float32), array(0.851, dtype=float32), array(0.927, dtype=float32), array(0.572, dtype=float32), array(0.933, dtype=float32), array(0.971, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 35090 Training loss 0.013377061113715172 Validation loss 0.01395946741104126 Accuracy 0.8546000123023987 Accuracies by class [array(0.706, dtype=float32), array(0.97, dtype=float32), array(0.715, dtype=float32), array(0.846, dtype=float32), array(0.857, dtype=float32), array(0.948, dtype=float32), array(0.675, dtype=float32), array(0.939, dtype=float32), array(0.967, dtype=float32), array(0.923, dtype=float32)]\n",
      "Iteration 35100 Training loss 0.010236457921564579 Validation loss 0.012929210439324379 Accuracy 0.8651000261306763 Accuracies by class [array(0.815, dtype=float32), array(0.97, dtype=float32), array(0.806, dtype=float32), array(0.882, dtype=float32), array(0.802, dtype=float32), array(0.937, dtype=float32), array(0.588, dtype=float32), array(0.935, dtype=float32), array(0.97, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 35110 Training loss 0.010353018529713154 Validation loss 0.013053087517619133 Accuracy 0.8636000156402588 Accuracies by class [array(0.77, dtype=float32), array(0.97, dtype=float32), array(0.789, dtype=float32), array(0.903, dtype=float32), array(0.801, dtype=float32), array(0.941, dtype=float32), array(0.628, dtype=float32), array(0.93, dtype=float32), array(0.963, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 35120 Training loss 0.008552799001336098 Validation loss 0.012584581039845943 Accuracy 0.8683000206947327 Accuracies by class [array(0.84, dtype=float32), array(0.964, dtype=float32), array(0.773, dtype=float32), array(0.907, dtype=float32), array(0.814, dtype=float32), array(0.938, dtype=float32), array(0.601, dtype=float32), array(0.953, dtype=float32), array(0.967, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 35130 Training loss 0.011236208491027355 Validation loss 0.012868208810687065 Accuracy 0.8651000261306763 Accuracies by class [array(0.767, dtype=float32), array(0.968, dtype=float32), array(0.76, dtype=float32), array(0.896, dtype=float32), array(0.753, dtype=float32), array(0.94, dtype=float32), array(0.712, dtype=float32), array(0.955, dtype=float32), array(0.964, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 35140 Training loss 0.012830330990254879 Validation loss 0.013499020598828793 Accuracy 0.8590999841690063 Accuracies by class [array(0.672, dtype=float32), array(0.971, dtype=float32), array(0.766, dtype=float32), array(0.876, dtype=float32), array(0.817, dtype=float32), array(0.943, dtype=float32), array(0.702, dtype=float32), array(0.931, dtype=float32), array(0.969, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 35150 Training loss 0.009261924773454666 Validation loss 0.013286574743688107 Accuracy 0.8604999780654907 Accuracies by class [array(0.735, dtype=float32), array(0.971, dtype=float32), array(0.782, dtype=float32), array(0.846, dtype=float32), array(0.767, dtype=float32), array(0.926, dtype=float32), array(0.714, dtype=float32), array(0.955, dtype=float32), array(0.965, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 35160 Training loss 0.01327141560614109 Validation loss 0.01290274690836668 Accuracy 0.8658999800682068 Accuracies by class [array(0.837, dtype=float32), array(0.969, dtype=float32), array(0.744, dtype=float32), array(0.893, dtype=float32), array(0.812, dtype=float32), array(0.929, dtype=float32), array(0.62, dtype=float32), array(0.927, dtype=float32), array(0.971, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 35170 Training loss 0.008717299439013004 Validation loss 0.012916948646306992 Accuracy 0.8644000291824341 Accuracies by class [array(0.817, dtype=float32), array(0.97, dtype=float32), array(0.782, dtype=float32), array(0.87, dtype=float32), array(0.83, dtype=float32), array(0.934, dtype=float32), array(0.595, dtype=float32), array(0.925, dtype=float32), array(0.967, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 35180 Training loss 0.012762045487761497 Validation loss 0.01304510049521923 Accuracy 0.8633999824523926 Accuracies by class [array(0.828, dtype=float32), array(0.968, dtype=float32), array(0.753, dtype=float32), array(0.869, dtype=float32), array(0.764, dtype=float32), array(0.941, dtype=float32), array(0.671, dtype=float32), array(0.951, dtype=float32), array(0.968, dtype=float32), array(0.921, dtype=float32)]\n",
      "Iteration 35190 Training loss 0.009333092719316483 Validation loss 0.013067356310784817 Accuracy 0.8640000224113464 Accuracies by class [array(0.847, dtype=float32), array(0.966, dtype=float32), array(0.79, dtype=float32), array(0.875, dtype=float32), array(0.772, dtype=float32), array(0.935, dtype=float32), array(0.638, dtype=float32), array(0.966, dtype=float32), array(0.963, dtype=float32), array(0.888, dtype=float32)]\n",
      "Iteration 35200 Training loss 0.007986804470419884 Validation loss 0.013351507484912872 Accuracy 0.8611000180244446 Accuracies by class [array(0.818, dtype=float32), array(0.967, dtype=float32), array(0.839, dtype=float32), array(0.889, dtype=float32), array(0.725, dtype=float32), array(0.929, dtype=float32), array(0.628, dtype=float32), array(0.97, dtype=float32), array(0.955, dtype=float32), array(0.891, dtype=float32)]\n",
      "Iteration 35210 Training loss 0.010356036946177483 Validation loss 0.013124472461640835 Accuracy 0.8636000156402588 Accuracies by class [array(0.769, dtype=float32), array(0.971, dtype=float32), array(0.786, dtype=float32), array(0.896, dtype=float32), array(0.843, dtype=float32), array(0.941, dtype=float32), array(0.593, dtype=float32), array(0.918, dtype=float32), array(0.962, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 35220 Training loss 0.011833840981125832 Validation loss 0.013304323889315128 Accuracy 0.8611999750137329 Accuracies by class [array(0.789, dtype=float32), array(0.97, dtype=float32), array(0.78, dtype=float32), array(0.9, dtype=float32), array(0.695, dtype=float32), array(0.938, dtype=float32), array(0.688, dtype=float32), array(0.948, dtype=float32), array(0.965, dtype=float32), array(0.939, dtype=float32)]\n",
      "Iteration 35230 Training loss 0.010373150929808617 Validation loss 0.012996546924114227 Accuracy 0.8636999726295471 Accuracies by class [array(0.801, dtype=float32), array(0.969, dtype=float32), array(0.84, dtype=float32), array(0.906, dtype=float32), array(0.78, dtype=float32), array(0.943, dtype=float32), array(0.558, dtype=float32), array(0.927, dtype=float32), array(0.967, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 35240 Training loss 0.012194108217954636 Validation loss 0.013662442564964294 Accuracy 0.857699990272522 Accuracies by class [array(0.807, dtype=float32), array(0.967, dtype=float32), array(0.856, dtype=float32), array(0.915, dtype=float32), array(0.63, dtype=float32), array(0.942, dtype=float32), array(0.63, dtype=float32), array(0.904, dtype=float32), array(0.965, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 35250 Training loss 0.008113591931760311 Validation loss 0.012943355366587639 Accuracy 0.8644000291824341 Accuracies by class [array(0.818, dtype=float32), array(0.97, dtype=float32), array(0.743, dtype=float32), array(0.873, dtype=float32), array(0.791, dtype=float32), array(0.934, dtype=float32), array(0.663, dtype=float32), array(0.934, dtype=float32), array(0.965, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 35260 Training loss 0.01181277446448803 Validation loss 0.013546370901167393 Accuracy 0.8598999977111816 Accuracies by class [array(0.861, dtype=float32), array(0.971, dtype=float32), array(0.814, dtype=float32), array(0.89, dtype=float32), array(0.841, dtype=float32), array(0.946, dtype=float32), array(0.436, dtype=float32), array(0.925, dtype=float32), array(0.96, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 35270 Training loss 0.010333274491131306 Validation loss 0.013434665277600288 Accuracy 0.8590999841690063 Accuracies by class [array(0.814, dtype=float32), array(0.975, dtype=float32), array(0.861, dtype=float32), array(0.849, dtype=float32), array(0.785, dtype=float32), array(0.94, dtype=float32), array(0.532, dtype=float32), array(0.917, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 35280 Training loss 0.011197023093700409 Validation loss 0.01345842145383358 Accuracy 0.8598999977111816 Accuracies by class [array(0.87, dtype=float32), array(0.96, dtype=float32), array(0.701, dtype=float32), array(0.892, dtype=float32), array(0.879, dtype=float32), array(0.914, dtype=float32), array(0.519, dtype=float32), array(0.966, dtype=float32), array(0.968, dtype=float32), array(0.93, dtype=float32)]\n",
      "Iteration 35290 Training loss 0.010837992653250694 Validation loss 0.013715146109461784 Accuracy 0.8568000197410583 Accuracies by class [array(0.801, dtype=float32), array(0.96, dtype=float32), array(0.709, dtype=float32), array(0.881, dtype=float32), array(0.891, dtype=float32), array(0.939, dtype=float32), array(0.544, dtype=float32), array(0.932, dtype=float32), array(0.97, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 35300 Training loss 0.008460870943963528 Validation loss 0.012964066118001938 Accuracy 0.8637999892234802 Accuracies by class [array(0.808, dtype=float32), array(0.957, dtype=float32), array(0.747, dtype=float32), array(0.893, dtype=float32), array(0.845, dtype=float32), array(0.941, dtype=float32), array(0.615, dtype=float32), array(0.918, dtype=float32), array(0.965, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 35310 Training loss 0.009576666168868542 Validation loss 0.01288398914039135 Accuracy 0.8652999997138977 Accuracies by class [array(0.848, dtype=float32), array(0.96, dtype=float32), array(0.727, dtype=float32), array(0.912, dtype=float32), array(0.84, dtype=float32), array(0.941, dtype=float32), array(0.592, dtype=float32), array(0.921, dtype=float32), array(0.961, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 35320 Training loss 0.009012826718389988 Validation loss 0.013434895314276218 Accuracy 0.8597999811172485 Accuracies by class [array(0.817, dtype=float32), array(0.963, dtype=float32), array(0.876, dtype=float32), array(0.879, dtype=float32), array(0.768, dtype=float32), array(0.94, dtype=float32), array(0.55, dtype=float32), array(0.914, dtype=float32), array(0.932, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 35330 Training loss 0.011956468224525452 Validation loss 0.013595213182270527 Accuracy 0.8568000197410583 Accuracies by class [array(0.756, dtype=float32), array(0.963, dtype=float32), array(0.856, dtype=float32), array(0.884, dtype=float32), array(0.654, dtype=float32), array(0.947, dtype=float32), array(0.686, dtype=float32), array(0.935, dtype=float32), array(0.957, dtype=float32), array(0.93, dtype=float32)]\n",
      "Iteration 35340 Training loss 0.008478565141558647 Validation loss 0.013217161409556866 Accuracy 0.861299991607666 Accuracies by class [array(0.824, dtype=float32), array(0.965, dtype=float32), array(0.832, dtype=float32), array(0.899, dtype=float32), array(0.739, dtype=float32), array(0.941, dtype=float32), array(0.606, dtype=float32), array(0.889, dtype=float32), array(0.958, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 35350 Training loss 0.011494306847453117 Validation loss 0.013201845809817314 Accuracy 0.862500011920929 Accuracies by class [array(0.818, dtype=float32), array(0.971, dtype=float32), array(0.766, dtype=float32), array(0.862, dtype=float32), array(0.854, dtype=float32), array(0.935, dtype=float32), array(0.581, dtype=float32), array(0.914, dtype=float32), array(0.962, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 35360 Training loss 0.009990451857447624 Validation loss 0.013116611167788506 Accuracy 0.8632000088691711 Accuracies by class [array(0.83, dtype=float32), array(0.969, dtype=float32), array(0.787, dtype=float32), array(0.89, dtype=float32), array(0.839, dtype=float32), array(0.939, dtype=float32), array(0.555, dtype=float32), array(0.899, dtype=float32), array(0.962, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 35370 Training loss 0.010799067094922066 Validation loss 0.013084699399769306 Accuracy 0.8632000088691711 Accuracies by class [array(0.834, dtype=float32), array(0.968, dtype=float32), array(0.748, dtype=float32), array(0.907, dtype=float32), array(0.78, dtype=float32), array(0.936, dtype=float32), array(0.628, dtype=float32), array(0.949, dtype=float32), array(0.947, dtype=float32), array(0.935, dtype=float32)]\n",
      "Iteration 35380 Training loss 0.010312794707715511 Validation loss 0.013601278886198997 Accuracy 0.8586999773979187 Accuracies by class [array(0.871, dtype=float32), array(0.968, dtype=float32), array(0.74, dtype=float32), array(0.842, dtype=float32), array(0.865, dtype=float32), array(0.938, dtype=float32), array(0.519, dtype=float32), array(0.952, dtype=float32), array(0.961, dtype=float32), array(0.931, dtype=float32)]\n",
      "Iteration 35390 Training loss 0.010730447247624397 Validation loss 0.013781377114355564 Accuracy 0.8565000295639038 Accuracies by class [array(0.708, dtype=float32), array(0.963, dtype=float32), array(0.7, dtype=float32), array(0.912, dtype=float32), array(0.854, dtype=float32), array(0.942, dtype=float32), array(0.651, dtype=float32), array(0.913, dtype=float32), array(0.967, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 35400 Training loss 0.011020257137715816 Validation loss 0.013005065731704235 Accuracy 0.8639000058174133 Accuracies by class [array(0.865, dtype=float32), array(0.963, dtype=float32), array(0.833, dtype=float32), array(0.901, dtype=float32), array(0.689, dtype=float32), array(0.942, dtype=float32), array(0.599, dtype=float32), array(0.937, dtype=float32), array(0.965, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 35410 Training loss 0.008867894299328327 Validation loss 0.013533312827348709 Accuracy 0.858299970626831 Accuracies by class [array(0.677, dtype=float32), array(0.97, dtype=float32), array(0.777, dtype=float32), array(0.856, dtype=float32), array(0.786, dtype=float32), array(0.935, dtype=float32), array(0.738, dtype=float32), array(0.959, dtype=float32), array(0.963, dtype=float32), array(0.922, dtype=float32)]\n",
      "Iteration 35420 Training loss 0.009736688807606697 Validation loss 0.012831888161599636 Accuracy 0.8664000034332275 Accuracies by class [array(0.841, dtype=float32), array(0.961, dtype=float32), array(0.773, dtype=float32), array(0.89, dtype=float32), array(0.813, dtype=float32), array(0.931, dtype=float32), array(0.611, dtype=float32), array(0.966, dtype=float32), array(0.967, dtype=float32), array(0.911, dtype=float32)]\n",
      "Iteration 35430 Training loss 0.010621034540235996 Validation loss 0.013155507855117321 Accuracy 0.8622000217437744 Accuracies by class [array(0.758, dtype=float32), array(0.96, dtype=float32), array(0.791, dtype=float32), array(0.895, dtype=float32), array(0.724, dtype=float32), array(0.939, dtype=float32), array(0.72, dtype=float32), array(0.941, dtype=float32), array(0.962, dtype=float32), array(0.932, dtype=float32)]\n",
      "Iteration 35440 Training loss 0.0112603222951293 Validation loss 0.01293399091809988 Accuracy 0.8639000058174133 Accuracies by class [array(0.862, dtype=float32), array(0.962, dtype=float32), array(0.808, dtype=float32), array(0.886, dtype=float32), array(0.796, dtype=float32), array(0.939, dtype=float32), array(0.54, dtype=float32), array(0.935, dtype=float32), array(0.968, dtype=float32), array(0.943, dtype=float32)]\n",
      "Iteration 35450 Training loss 0.008119098842144012 Validation loss 0.012958087958395481 Accuracy 0.8634999990463257 Accuracies by class [array(0.8, dtype=float32), array(0.964, dtype=float32), array(0.833, dtype=float32), array(0.889, dtype=float32), array(0.798, dtype=float32), array(0.949, dtype=float32), array(0.572, dtype=float32), array(0.908, dtype=float32), array(0.966, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 35460 Training loss 0.008575443178415298 Validation loss 0.012957978062331676 Accuracy 0.8646000027656555 Accuracies by class [array(0.752, dtype=float32), array(0.963, dtype=float32), array(0.787, dtype=float32), array(0.885, dtype=float32), array(0.807, dtype=float32), array(0.951, dtype=float32), array(0.678, dtype=float32), array(0.919, dtype=float32), array(0.96, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 35470 Training loss 0.008496878668665886 Validation loss 0.013179340399801731 Accuracy 0.8626000285148621 Accuracies by class [array(0.857, dtype=float32), array(0.968, dtype=float32), array(0.774, dtype=float32), array(0.873, dtype=float32), array(0.847, dtype=float32), array(0.937, dtype=float32), array(0.512, dtype=float32), array(0.933, dtype=float32), array(0.971, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 35480 Training loss 0.008240031078457832 Validation loss 0.013418466784060001 Accuracy 0.8590999841690063 Accuracies by class [array(0.802, dtype=float32), array(0.964, dtype=float32), array(0.809, dtype=float32), array(0.873, dtype=float32), array(0.705, dtype=float32), array(0.945, dtype=float32), array(0.654, dtype=float32), array(0.928, dtype=float32), array(0.969, dtype=float32), array(0.942, dtype=float32)]\n",
      "Iteration 35490 Training loss 0.010551851242780685 Validation loss 0.013179261237382889 Accuracy 0.863099992275238 Accuracies by class [array(0.842, dtype=float32), array(0.962, dtype=float32), array(0.831, dtype=float32), array(0.919, dtype=float32), array(0.747, dtype=float32), array(0.948, dtype=float32), array(0.554, dtype=float32), array(0.9, dtype=float32), array(0.969, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 35500 Training loss 0.010797061026096344 Validation loss 0.013070475310087204 Accuracy 0.8628000020980835 Accuracies by class [array(0.795, dtype=float32), array(0.96, dtype=float32), array(0.698, dtype=float32), array(0.887, dtype=float32), array(0.825, dtype=float32), array(0.946, dtype=float32), array(0.683, dtype=float32), array(0.926, dtype=float32), array(0.962, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 35510 Training loss 0.008499362505972385 Validation loss 0.01309211179614067 Accuracy 0.8636999726295471 Accuracies by class [array(0.872, dtype=float32), array(0.968, dtype=float32), array(0.826, dtype=float32), array(0.869, dtype=float32), array(0.821, dtype=float32), array(0.944, dtype=float32), array(0.497, dtype=float32), array(0.918, dtype=float32), array(0.963, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 35520 Training loss 0.008372848853468895 Validation loss 0.013417882844805717 Accuracy 0.8596000075340271 Accuracies by class [array(0.809, dtype=float32), array(0.961, dtype=float32), array(0.797, dtype=float32), array(0.888, dtype=float32), array(0.688, dtype=float32), array(0.936, dtype=float32), array(0.693, dtype=float32), array(0.967, dtype=float32), array(0.952, dtype=float32), array(0.905, dtype=float32)]\n",
      "Iteration 35530 Training loss 0.009204610250890255 Validation loss 0.01279391348361969 Accuracy 0.8654999732971191 Accuracies by class [array(0.777, dtype=float32), array(0.963, dtype=float32), array(0.763, dtype=float32), array(0.885, dtype=float32), array(0.811, dtype=float32), array(0.94, dtype=float32), array(0.683, dtype=float32), array(0.92, dtype=float32), array(0.957, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 35540 Training loss 0.009286164306104183 Validation loss 0.013590996153652668 Accuracy 0.8578000068664551 Accuracies by class [array(0.762, dtype=float32), array(0.961, dtype=float32), array(0.866, dtype=float32), array(0.919, dtype=float32), array(0.692, dtype=float32), array(0.938, dtype=float32), array(0.614, dtype=float32), array(0.913, dtype=float32), array(0.953, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 35550 Training loss 0.00888841599225998 Validation loss 0.013099021278321743 Accuracy 0.8632000088691711 Accuracies by class [array(0.808, dtype=float32), array(0.969, dtype=float32), array(0.742, dtype=float32), array(0.886, dtype=float32), array(0.864, dtype=float32), array(0.95, dtype=float32), array(0.591, dtype=float32), array(0.912, dtype=float32), array(0.963, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 35560 Training loss 0.008122497238218784 Validation loss 0.013143054209649563 Accuracy 0.8634999990463257 Accuracies by class [array(0.843, dtype=float32), array(0.968, dtype=float32), array(0.789, dtype=float32), array(0.899, dtype=float32), array(0.82, dtype=float32), array(0.939, dtype=float32), array(0.562, dtype=float32), array(0.967, dtype=float32), array(0.951, dtype=float32), array(0.897, dtype=float32)]\n",
      "Iteration 35570 Training loss 0.008991466835141182 Validation loss 0.013075520284473896 Accuracy 0.8639000058174133 Accuracies by class [array(0.775, dtype=float32), array(0.965, dtype=float32), array(0.763, dtype=float32), array(0.903, dtype=float32), array(0.843, dtype=float32), array(0.948, dtype=float32), array(0.619, dtype=float32), array(0.911, dtype=float32), array(0.951, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 35580 Training loss 0.009487641975283623 Validation loss 0.013428975827991962 Accuracy 0.8586999773979187 Accuracies by class [array(0.817, dtype=float32), array(0.972, dtype=float32), array(0.652, dtype=float32), array(0.832, dtype=float32), array(0.875, dtype=float32), array(0.939, dtype=float32), array(0.65, dtype=float32), array(0.929, dtype=float32), array(0.964, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 35590 Training loss 0.008560179732739925 Validation loss 0.013071318157017231 Accuracy 0.8628000020980835 Accuracies by class [array(0.733, dtype=float32), array(0.97, dtype=float32), array(0.805, dtype=float32), array(0.875, dtype=float32), array(0.774, dtype=float32), array(0.953, dtype=float32), array(0.713, dtype=float32), array(0.88, dtype=float32), array(0.962, dtype=float32), array(0.963, dtype=float32)]\n",
      "Iteration 35600 Training loss 0.009554221294820309 Validation loss 0.012739855796098709 Accuracy 0.8675000071525574 Accuracies by class [array(0.836, dtype=float32), array(0.972, dtype=float32), array(0.758, dtype=float32), array(0.88, dtype=float32), array(0.849, dtype=float32), array(0.947, dtype=float32), array(0.59, dtype=float32), array(0.938, dtype=float32), array(0.964, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 35610 Training loss 0.008430199697613716 Validation loss 0.013409718871116638 Accuracy 0.859499990940094 Accuracies by class [array(0.75, dtype=float32), array(0.967, dtype=float32), array(0.852, dtype=float32), array(0.922, dtype=float32), array(0.784, dtype=float32), array(0.943, dtype=float32), array(0.527, dtype=float32), array(0.943, dtype=float32), array(0.96, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 35620 Training loss 0.008022683672606945 Validation loss 0.01317888405174017 Accuracy 0.8618999719619751 Accuracies by class [array(0.729, dtype=float32), array(0.966, dtype=float32), array(0.765, dtype=float32), array(0.927, dtype=float32), array(0.821, dtype=float32), array(0.942, dtype=float32), array(0.619, dtype=float32), array(0.944, dtype=float32), array(0.966, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 35630 Training loss 0.01019306667149067 Validation loss 0.012713386677205563 Accuracy 0.8664000034332275 Accuracies by class [array(0.813, dtype=float32), array(0.973, dtype=float32), array(0.843, dtype=float32), array(0.894, dtype=float32), array(0.777, dtype=float32), array(0.942, dtype=float32), array(0.582, dtype=float32), array(0.929, dtype=float32), array(0.965, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 35640 Training loss 0.009841260500252247 Validation loss 0.012666638009250164 Accuracy 0.8682000041007996 Accuracies by class [array(0.78, dtype=float32), array(0.968, dtype=float32), array(0.806, dtype=float32), array(0.91, dtype=float32), array(0.807, dtype=float32), array(0.938, dtype=float32), array(0.617, dtype=float32), array(0.954, dtype=float32), array(0.966, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 35650 Training loss 0.00859888456761837 Validation loss 0.012953249737620354 Accuracy 0.8646000027656555 Accuracies by class [array(0.741, dtype=float32), array(0.968, dtype=float32), array(0.82, dtype=float32), array(0.889, dtype=float32), array(0.795, dtype=float32), array(0.948, dtype=float32), array(0.649, dtype=float32), array(0.918, dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 35660 Training loss 0.009207411669194698 Validation loss 0.01294223964214325 Accuracy 0.8648999929428101 Accuracies by class [array(0.811, dtype=float32), array(0.967, dtype=float32), array(0.779, dtype=float32), array(0.844, dtype=float32), array(0.859, dtype=float32), array(0.939, dtype=float32), array(0.598, dtype=float32), array(0.951, dtype=float32), array(0.965, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 35670 Training loss 0.01042865589261055 Validation loss 0.013625210151076317 Accuracy 0.8583999872207642 Accuracies by class [array(0.729, dtype=float32), array(0.964, dtype=float32), array(0.73, dtype=float32), array(0.862, dtype=float32), array(0.881, dtype=float32), array(0.948, dtype=float32), array(0.637, dtype=float32), array(0.926, dtype=float32), array(0.963, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 35680 Training loss 0.010411175899207592 Validation loss 0.012759238481521606 Accuracy 0.866599977016449 Accuracies by class [array(0.866, dtype=float32), array(0.966, dtype=float32), array(0.823, dtype=float32), array(0.854, dtype=float32), array(0.787, dtype=float32), array(0.943, dtype=float32), array(0.587, dtype=float32), array(0.94, dtype=float32), array(0.96, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 35690 Training loss 0.008260160684585571 Validation loss 0.012707439251244068 Accuracy 0.8659999966621399 Accuracies by class [array(0.874, dtype=float32), array(0.966, dtype=float32), array(0.803, dtype=float32), array(0.882, dtype=float32), array(0.815, dtype=float32), array(0.944, dtype=float32), array(0.537, dtype=float32), array(0.938, dtype=float32), array(0.961, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 35700 Training loss 0.00820607878267765 Validation loss 0.01243623811751604 Accuracy 0.8691999912261963 Accuracies by class [array(0.823, dtype=float32), array(0.965, dtype=float32), array(0.823, dtype=float32), array(0.887, dtype=float32), array(0.801, dtype=float32), array(0.945, dtype=float32), array(0.615, dtype=float32), array(0.948, dtype=float32), array(0.959, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 35710 Training loss 0.008284058421850204 Validation loss 0.01300189457833767 Accuracy 0.8637999892234802 Accuracies by class [array(0.822, dtype=float32), array(0.969, dtype=float32), array(0.831, dtype=float32), array(0.882, dtype=float32), array(0.701, dtype=float32), array(0.95, dtype=float32), array(0.65, dtype=float32), array(0.922, dtype=float32), array(0.96, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 35720 Training loss 0.008669781498610973 Validation loss 0.012541836127638817 Accuracy 0.8687000274658203 Accuracies by class [array(0.817, dtype=float32), array(0.963, dtype=float32), array(0.798, dtype=float32), array(0.91, dtype=float32), array(0.801, dtype=float32), array(0.927, dtype=float32), array(0.614, dtype=float32), array(0.944, dtype=float32), array(0.956, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 35730 Training loss 0.008820651099085808 Validation loss 0.012621982023119926 Accuracy 0.8679999709129333 Accuracies by class [array(0.847, dtype=float32), array(0.965, dtype=float32), array(0.786, dtype=float32), array(0.9, dtype=float32), array(0.807, dtype=float32), array(0.931, dtype=float32), array(0.589, dtype=float32), array(0.937, dtype=float32), array(0.963, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 35740 Training loss 0.009115989319980145 Validation loss 0.012620672583580017 Accuracy 0.8675000071525574 Accuracies by class [array(0.813, dtype=float32), array(0.967, dtype=float32), array(0.794, dtype=float32), array(0.872, dtype=float32), array(0.763, dtype=float32), array(0.943, dtype=float32), array(0.678, dtype=float32), array(0.931, dtype=float32), array(0.964, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 35750 Training loss 0.007867570035159588 Validation loss 0.012793698348104954 Accuracy 0.8655999898910522 Accuracies by class [array(0.793, dtype=float32), array(0.968, dtype=float32), array(0.729, dtype=float32), array(0.872, dtype=float32), array(0.844, dtype=float32), array(0.941, dtype=float32), array(0.663, dtype=float32), array(0.956, dtype=float32), array(0.96, dtype=float32), array(0.93, dtype=float32)]\n",
      "Iteration 35760 Training loss 0.010496444068849087 Validation loss 0.012678420171141624 Accuracy 0.867900013923645 Accuracies by class [array(0.85, dtype=float32), array(0.965, dtype=float32), array(0.819, dtype=float32), array(0.893, dtype=float32), array(0.816, dtype=float32), array(0.946, dtype=float32), array(0.547, dtype=float32), array(0.924, dtype=float32), array(0.966, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 35770 Training loss 0.009519091807305813 Validation loss 0.012965385802090168 Accuracy 0.8648999929428101 Accuracies by class [array(0.839, dtype=float32), array(0.962, dtype=float32), array(0.841, dtype=float32), array(0.907, dtype=float32), array(0.762, dtype=float32), array(0.943, dtype=float32), array(0.574, dtype=float32), array(0.957, dtype=float32), array(0.964, dtype=float32), array(0.9, dtype=float32)]\n",
      "Iteration 35780 Training loss 0.011940520256757736 Validation loss 0.012658761814236641 Accuracy 0.8662999868392944 Accuracies by class [array(0.845, dtype=float32), array(0.967, dtype=float32), array(0.826, dtype=float32), array(0.91, dtype=float32), array(0.751, dtype=float32), array(0.939, dtype=float32), array(0.578, dtype=float32), array(0.945, dtype=float32), array(0.964, dtype=float32), array(0.938, dtype=float32)]\n",
      "Iteration 35790 Training loss 0.00956135056912899 Validation loss 0.012966292910277843 Accuracy 0.8641999959945679 Accuracies by class [array(0.772, dtype=float32), array(0.97, dtype=float32), array(0.808, dtype=float32), array(0.863, dtype=float32), array(0.82, dtype=float32), array(0.951, dtype=float32), array(0.637, dtype=float32), array(0.897, dtype=float32), array(0.969, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 35800 Training loss 0.008485765196383 Validation loss 0.01297675259411335 Accuracy 0.8636999726295471 Accuracies by class [array(0.824, dtype=float32), array(0.971, dtype=float32), array(0.807, dtype=float32), array(0.868, dtype=float32), array(0.832, dtype=float32), array(0.943, dtype=float32), array(0.544, dtype=float32), array(0.936, dtype=float32), array(0.963, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 35810 Training loss 0.010760842822492123 Validation loss 0.01346584316343069 Accuracy 0.8587999939918518 Accuracies by class [array(0.698, dtype=float32), array(0.967, dtype=float32), array(0.81, dtype=float32), array(0.897, dtype=float32), array(0.774, dtype=float32), array(0.941, dtype=float32), array(0.686, dtype=float32), array(0.88, dtype=float32), array(0.961, dtype=float32), array(0.974, dtype=float32)]\n",
      "Iteration 35820 Training loss 0.007715434301644564 Validation loss 0.013571007177233696 Accuracy 0.8568999767303467 Accuracies by class [array(0.745, dtype=float32), array(0.969, dtype=float32), array(0.647, dtype=float32), array(0.888, dtype=float32), array(0.871, dtype=float32), array(0.949, dtype=float32), array(0.67, dtype=float32), array(0.924, dtype=float32), array(0.966, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 35830 Training loss 0.009354106150567532 Validation loss 0.013399794697761536 Accuracy 0.8597999811172485 Accuracies by class [array(0.715, dtype=float32), array(0.968, dtype=float32), array(0.726, dtype=float32), array(0.891, dtype=float32), array(0.872, dtype=float32), array(0.937, dtype=float32), array(0.64, dtype=float32), array(0.942, dtype=float32), array(0.958, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 35840 Training loss 0.009111704304814339 Validation loss 0.012738245539367199 Accuracy 0.8661999702453613 Accuracies by class [array(0.813, dtype=float32), array(0.968, dtype=float32), array(0.763, dtype=float32), array(0.887, dtype=float32), array(0.817, dtype=float32), array(0.946, dtype=float32), array(0.641, dtype=float32), array(0.907, dtype=float32), array(0.963, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 35850 Training loss 0.010757052339613438 Validation loss 0.012733880430459976 Accuracy 0.8665000200271606 Accuracies by class [array(0.83, dtype=float32), array(0.968, dtype=float32), array(0.793, dtype=float32), array(0.881, dtype=float32), array(0.769, dtype=float32), array(0.934, dtype=float32), array(0.649, dtype=float32), array(0.923, dtype=float32), array(0.96, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 35860 Training loss 0.008911007083952427 Validation loss 0.012858337722718716 Accuracy 0.864799976348877 Accuracies by class [array(0.844, dtype=float32), array(0.964, dtype=float32), array(0.756, dtype=float32), array(0.897, dtype=float32), array(0.818, dtype=float32), array(0.938, dtype=float32), array(0.582, dtype=float32), array(0.948, dtype=float32), array(0.963, dtype=float32), array(0.938, dtype=float32)]\n",
      "Iteration 35870 Training loss 0.008032817393541336 Validation loss 0.013113400898873806 Accuracy 0.8626000285148621 Accuracies by class [array(0.797, dtype=float32), array(0.967, dtype=float32), array(0.841, dtype=float32), array(0.909, dtype=float32), array(0.75, dtype=float32), array(0.94, dtype=float32), array(0.576, dtype=float32), array(0.93, dtype=float32), array(0.96, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 35880 Training loss 0.008335456252098083 Validation loss 0.012806640937924385 Accuracy 0.8655999898910522 Accuracies by class [array(0.783, dtype=float32), array(0.966, dtype=float32), array(0.746, dtype=float32), array(0.893, dtype=float32), array(0.803, dtype=float32), array(0.943, dtype=float32), array(0.685, dtype=float32), array(0.934, dtype=float32), array(0.955, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 35890 Training loss 0.013132364489138126 Validation loss 0.01325419545173645 Accuracy 0.86080002784729 Accuracies by class [array(0.826, dtype=float32), array(0.961, dtype=float32), array(0.804, dtype=float32), array(0.904, dtype=float32), array(0.801, dtype=float32), array(0.93, dtype=float32), array(0.581, dtype=float32), array(0.972, dtype=float32), array(0.959, dtype=float32), array(0.87, dtype=float32)]\n",
      "Iteration 35900 Training loss 0.010711868293583393 Validation loss 0.01313895732164383 Accuracy 0.8618000149726868 Accuracies by class [array(0.784, dtype=float32), array(0.964, dtype=float32), array(0.818, dtype=float32), array(0.888, dtype=float32), array(0.756, dtype=float32), array(0.926, dtype=float32), array(0.662, dtype=float32), array(0.964, dtype=float32), array(0.961, dtype=float32), array(0.895, dtype=float32)]\n",
      "Iteration 35910 Training loss 0.009124436415731907 Validation loss 0.01306779496371746 Accuracy 0.862500011920929 Accuracies by class [array(0.844, dtype=float32), array(0.963, dtype=float32), array(0.656, dtype=float32), array(0.897, dtype=float32), array(0.828, dtype=float32), array(0.938, dtype=float32), array(0.642, dtype=float32), array(0.947, dtype=float32), array(0.973, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 35920 Training loss 0.008328999392688274 Validation loss 0.01291030552238226 Accuracy 0.8650000095367432 Accuracies by class [array(0.811, dtype=float32), array(0.962, dtype=float32), array(0.751, dtype=float32), array(0.909, dtype=float32), array(0.771, dtype=float32), array(0.946, dtype=float32), array(0.663, dtype=float32), array(0.925, dtype=float32), array(0.966, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 35930 Training loss 0.009060905314981937 Validation loss 0.012886431999504566 Accuracy 0.8651999831199646 Accuracies by class [array(0.797, dtype=float32), array(0.967, dtype=float32), array(0.832, dtype=float32), array(0.897, dtype=float32), array(0.749, dtype=float32), array(0.946, dtype=float32), array(0.628, dtype=float32), array(0.937, dtype=float32), array(0.965, dtype=float32), array(0.934, dtype=float32)]\n",
      "Iteration 35940 Training loss 0.009509236551821232 Validation loss 0.012683302164077759 Accuracy 0.8669999837875366 Accuracies by class [array(0.829, dtype=float32), array(0.968, dtype=float32), array(0.805, dtype=float32), array(0.89, dtype=float32), array(0.814, dtype=float32), array(0.942, dtype=float32), array(0.573, dtype=float32), array(0.939, dtype=float32), array(0.969, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 35950 Training loss 0.009784295223653316 Validation loss 0.012554931454360485 Accuracy 0.8687000274658203 Accuracies by class [array(0.79, dtype=float32), array(0.97, dtype=float32), array(0.805, dtype=float32), array(0.909, dtype=float32), array(0.802, dtype=float32), array(0.95, dtype=float32), array(0.622, dtype=float32), array(0.925, dtype=float32), array(0.971, dtype=float32), array(0.943, dtype=float32)]\n",
      "Iteration 35960 Training loss 0.009348073974251747 Validation loss 0.01347088348120451 Accuracy 0.8580999970436096 Accuracies by class [array(0.808, dtype=float32), array(0.967, dtype=float32), array(0.844, dtype=float32), array(0.914, dtype=float32), array(0.64, dtype=float32), array(0.942, dtype=float32), array(0.63, dtype=float32), array(0.937, dtype=float32), array(0.956, dtype=float32), array(0.943, dtype=float32)]\n",
      "Iteration 35970 Training loss 0.008995658718049526 Validation loss 0.012938570231199265 Accuracy 0.864799976348877 Accuracies by class [array(0.842, dtype=float32), array(0.968, dtype=float32), array(0.833, dtype=float32), array(0.885, dtype=float32), array(0.787, dtype=float32), array(0.932, dtype=float32), array(0.546, dtype=float32), array(0.927, dtype=float32), array(0.97, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 35980 Training loss 0.007929063402116299 Validation loss 0.013154633343219757 Accuracy 0.861299991607666 Accuracies by class [array(0.775, dtype=float32), array(0.969, dtype=float32), array(0.713, dtype=float32), array(0.869, dtype=float32), array(0.869, dtype=float32), array(0.938, dtype=float32), array(0.628, dtype=float32), array(0.937, dtype=float32), array(0.965, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 35990 Training loss 0.008190959692001343 Validation loss 0.013284001499414444 Accuracy 0.8608999848365784 Accuracies by class [array(0.762, dtype=float32), array(0.971, dtype=float32), array(0.845, dtype=float32), array(0.867, dtype=float32), array(0.669, dtype=float32), array(0.949, dtype=float32), array(0.707, dtype=float32), array(0.918, dtype=float32), array(0.965, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 36000 Training loss 0.008719337172806263 Validation loss 0.013048817403614521 Accuracy 0.862500011920929 Accuracies by class [array(0.842, dtype=float32), array(0.968, dtype=float32), array(0.827, dtype=float32), array(0.894, dtype=float32), array(0.805, dtype=float32), array(0.933, dtype=float32), array(0.506, dtype=float32), array(0.919, dtype=float32), array(0.967, dtype=float32), array(0.964, dtype=float32)]\n",
      "Iteration 36010 Training loss 0.0077414377592504025 Validation loss 0.013219228945672512 Accuracy 0.8605999946594238 Accuracies by class [array(0.758, dtype=float32), array(0.966, dtype=float32), array(0.73, dtype=float32), array(0.911, dtype=float32), array(0.834, dtype=float32), array(0.941, dtype=float32), array(0.618, dtype=float32), array(0.936, dtype=float32), array(0.969, dtype=float32), array(0.943, dtype=float32)]\n",
      "Iteration 36020 Training loss 0.010806453414261341 Validation loss 0.012837523594498634 Accuracy 0.8651000261306763 Accuracies by class [array(0.791, dtype=float32), array(0.966, dtype=float32), array(0.779, dtype=float32), array(0.896, dtype=float32), array(0.781, dtype=float32), array(0.933, dtype=float32), array(0.661, dtype=float32), array(0.929, dtype=float32), array(0.957, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 36030 Training loss 0.007144132629036903 Validation loss 0.012679959647357464 Accuracy 0.8672000169754028 Accuracies by class [array(0.783, dtype=float32), array(0.963, dtype=float32), array(0.749, dtype=float32), array(0.912, dtype=float32), array(0.806, dtype=float32), array(0.939, dtype=float32), array(0.671, dtype=float32), array(0.928, dtype=float32), array(0.965, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 36040 Training loss 0.009256536141037941 Validation loss 0.013025552034378052 Accuracy 0.8634999990463257 Accuracies by class [array(0.854, dtype=float32), array(0.963, dtype=float32), array(0.833, dtype=float32), array(0.916, dtype=float32), array(0.784, dtype=float32), array(0.939, dtype=float32), array(0.492, dtype=float32), array(0.938, dtype=float32), array(0.967, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 36050 Training loss 0.009668360464274883 Validation loss 0.013964028097689152 Accuracy 0.8533999919891357 Accuracies by class [array(0.684, dtype=float32), array(0.97, dtype=float32), array(0.664, dtype=float32), array(0.882, dtype=float32), array(0.793, dtype=float32), array(0.943, dtype=float32), array(0.756, dtype=float32), array(0.936, dtype=float32), array(0.964, dtype=float32), array(0.942, dtype=float32)]\n",
      "Iteration 36060 Training loss 0.009174823760986328 Validation loss 0.013261804357171059 Accuracy 0.8605999946594238 Accuracies by class [array(0.823, dtype=float32), array(0.972, dtype=float32), array(0.857, dtype=float32), array(0.881, dtype=float32), array(0.776, dtype=float32), array(0.93, dtype=float32), array(0.517, dtype=float32), array(0.937, dtype=float32), array(0.965, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 36070 Training loss 0.008281026966869831 Validation loss 0.01291333232074976 Accuracy 0.8640000224113464 Accuracies by class [array(0.842, dtype=float32), array(0.967, dtype=float32), array(0.736, dtype=float32), array(0.912, dtype=float32), array(0.758, dtype=float32), array(0.947, dtype=float32), array(0.637, dtype=float32), array(0.932, dtype=float32), array(0.968, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 36080 Training loss 0.009724785573780537 Validation loss 0.01283092051744461 Accuracy 0.8651999831199646 Accuracies by class [array(0.805, dtype=float32), array(0.967, dtype=float32), array(0.723, dtype=float32), array(0.88, dtype=float32), array(0.858, dtype=float32), array(0.946, dtype=float32), array(0.636, dtype=float32), array(0.919, dtype=float32), array(0.969, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 36090 Training loss 0.010333406738936901 Validation loss 0.01301138661801815 Accuracy 0.8634999990463257 Accuracies by class [array(0.838, dtype=float32), array(0.967, dtype=float32), array(0.779, dtype=float32), array(0.894, dtype=float32), array(0.851, dtype=float32), array(0.932, dtype=float32), array(0.522, dtype=float32), array(0.962, dtype=float32), array(0.968, dtype=float32), array(0.922, dtype=float32)]\n",
      "Iteration 36100 Training loss 0.011097821407020092 Validation loss 0.013043120503425598 Accuracy 0.8626999855041504 Accuracies by class [array(0.796, dtype=float32), array(0.964, dtype=float32), array(0.826, dtype=float32), array(0.909, dtype=float32), array(0.676, dtype=float32), array(0.949, dtype=float32), array(0.677, dtype=float32), array(0.914, dtype=float32), array(0.966, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 36110 Training loss 0.01012035459280014 Validation loss 0.013345676474273205 Accuracy 0.86080002784729 Accuracies by class [array(0.789, dtype=float32), array(0.966, dtype=float32), array(0.619, dtype=float32), array(0.895, dtype=float32), array(0.808, dtype=float32), array(0.939, dtype=float32), array(0.73, dtype=float32), array(0.94, dtype=float32), array(0.971, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 36120 Training loss 0.010013435035943985 Validation loss 0.012684871442615986 Accuracy 0.8664000034332275 Accuracies by class [array(0.732, dtype=float32), array(0.965, dtype=float32), array(0.777, dtype=float32), array(0.91, dtype=float32), array(0.81, dtype=float32), array(0.932, dtype=float32), array(0.695, dtype=float32), array(0.951, dtype=float32), array(0.956, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 36130 Training loss 0.009137876331806183 Validation loss 0.012899313122034073 Accuracy 0.8646000027656555 Accuracies by class [array(0.823, dtype=float32), array(0.959, dtype=float32), array(0.706, dtype=float32), array(0.872, dtype=float32), array(0.843, dtype=float32), array(0.941, dtype=float32), array(0.652, dtype=float32), array(0.932, dtype=float32), array(0.972, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 36140 Training loss 0.008875490166246891 Validation loss 0.012514437548816204 Accuracy 0.8679999709129333 Accuracies by class [array(0.798, dtype=float32), array(0.963, dtype=float32), array(0.802, dtype=float32), array(0.918, dtype=float32), array(0.788, dtype=float32), array(0.942, dtype=float32), array(0.629, dtype=float32), array(0.919, dtype=float32), array(0.967, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 36150 Training loss 0.007871679961681366 Validation loss 0.013082255609333515 Accuracy 0.862500011920929 Accuracies by class [array(0.823, dtype=float32), array(0.966, dtype=float32), array(0.845, dtype=float32), array(0.899, dtype=float32), array(0.764, dtype=float32), array(0.935, dtype=float32), array(0.546, dtype=float32), array(0.925, dtype=float32), array(0.967, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 36160 Training loss 0.010644026100635529 Validation loss 0.01297750510275364 Accuracy 0.8633999824523926 Accuracies by class [array(0.844, dtype=float32), array(0.969, dtype=float32), array(0.794, dtype=float32), array(0.922, dtype=float32), array(0.826, dtype=float32), array(0.938, dtype=float32), array(0.498, dtype=float32), array(0.919, dtype=float32), array(0.966, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 36170 Training loss 0.008594803512096405 Validation loss 0.01268756203353405 Accuracy 0.8664000034332275 Accuracies by class [array(0.802, dtype=float32), array(0.969, dtype=float32), array(0.803, dtype=float32), array(0.905, dtype=float32), array(0.747, dtype=float32), array(0.934, dtype=float32), array(0.669, dtype=float32), array(0.936, dtype=float32), array(0.955, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 36180 Training loss 0.009849971160292625 Validation loss 0.01264326274394989 Accuracy 0.8676000237464905 Accuracies by class [array(0.85, dtype=float32), array(0.972, dtype=float32), array(0.779, dtype=float32), array(0.883, dtype=float32), array(0.855, dtype=float32), array(0.945, dtype=float32), array(0.543, dtype=float32), array(0.943, dtype=float32), array(0.966, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 36190 Training loss 0.008527698926627636 Validation loss 0.012830332852900028 Accuracy 0.8654000163078308 Accuracies by class [array(0.788, dtype=float32), array(0.963, dtype=float32), array(0.774, dtype=float32), array(0.888, dtype=float32), array(0.732, dtype=float32), array(0.939, dtype=float32), array(0.727, dtype=float32), array(0.952, dtype=float32), array(0.965, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 36200 Training loss 0.007885701023042202 Validation loss 0.012852146290242672 Accuracy 0.8650000095367432 Accuracies by class [array(0.806, dtype=float32), array(0.969, dtype=float32), array(0.837, dtype=float32), array(0.904, dtype=float32), array(0.757, dtype=float32), array(0.943, dtype=float32), array(0.595, dtype=float32), array(0.919, dtype=float32), array(0.966, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 36210 Training loss 0.011382749304175377 Validation loss 0.0130201680585742 Accuracy 0.8636000156402588 Accuracies by class [array(0.882, dtype=float32), array(0.964, dtype=float32), array(0.721, dtype=float32), array(0.863, dtype=float32), array(0.761, dtype=float32), array(0.947, dtype=float32), array(0.652, dtype=float32), array(0.947, dtype=float32), array(0.968, dtype=float32), array(0.931, dtype=float32)]\n",
      "Iteration 36220 Training loss 0.008582585491240025 Validation loss 0.01306216511875391 Accuracy 0.8626999855041504 Accuracies by class [array(0.784, dtype=float32), array(0.968, dtype=float32), array(0.779, dtype=float32), array(0.9, dtype=float32), array(0.836, dtype=float32), array(0.95, dtype=float32), array(0.582, dtype=float32), array(0.907, dtype=float32), array(0.966, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 36230 Training loss 0.008598525077104568 Validation loss 0.012673549354076385 Accuracy 0.8679999709129333 Accuracies by class [array(0.844, dtype=float32), array(0.965, dtype=float32), array(0.791, dtype=float32), array(0.915, dtype=float32), array(0.782, dtype=float32), array(0.945, dtype=float32), array(0.591, dtype=float32), array(0.925, dtype=float32), array(0.967, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 36240 Training loss 0.011379750445485115 Validation loss 0.0132191963493824 Accuracy 0.8615000247955322 Accuracies by class [array(0.671, dtype=float32), array(0.966, dtype=float32), array(0.795, dtype=float32), array(0.894, dtype=float32), array(0.774, dtype=float32), array(0.938, dtype=float32), array(0.728, dtype=float32), array(0.937, dtype=float32), array(0.964, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 36250 Training loss 0.009494281373918056 Validation loss 0.012854554690420628 Accuracy 0.8650000095367432 Accuracies by class [array(0.842, dtype=float32), array(0.968, dtype=float32), array(0.756, dtype=float32), array(0.871, dtype=float32), array(0.794, dtype=float32), array(0.95, dtype=float32), array(0.63, dtype=float32), array(0.91, dtype=float32), array(0.973, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 36260 Training loss 0.009184392169117928 Validation loss 0.013101323507726192 Accuracy 0.8629000186920166 Accuracies by class [array(0.834, dtype=float32), array(0.972, dtype=float32), array(0.795, dtype=float32), array(0.845, dtype=float32), array(0.846, dtype=float32), array(0.941, dtype=float32), array(0.551, dtype=float32), array(0.925, dtype=float32), array(0.966, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 36270 Training loss 0.008210372179746628 Validation loss 0.013088811188936234 Accuracy 0.8626000285148621 Accuracies by class [array(0.85, dtype=float32), array(0.967, dtype=float32), array(0.84, dtype=float32), array(0.876, dtype=float32), array(0.771, dtype=float32), array(0.935, dtype=float32), array(0.543, dtype=float32), array(0.966, dtype=float32), array(0.97, dtype=float32), array(0.908, dtype=float32)]\n",
      "Iteration 36280 Training loss 0.008824972435832024 Validation loss 0.01304441038519144 Accuracy 0.8633000254631042 Accuracies by class [array(0.843, dtype=float32), array(0.964, dtype=float32), array(0.829, dtype=float32), array(0.886, dtype=float32), array(0.721, dtype=float32), array(0.934, dtype=float32), array(0.607, dtype=float32), array(0.914, dtype=float32), array(0.966, dtype=float32), array(0.969, dtype=float32)]\n",
      "Iteration 36290 Training loss 0.008437559008598328 Validation loss 0.013176806271076202 Accuracy 0.8626000285148621 Accuracies by class [array(0.741, dtype=float32), array(0.969, dtype=float32), array(0.834, dtype=float32), array(0.878, dtype=float32), array(0.814, dtype=float32), array(0.935, dtype=float32), array(0.602, dtype=float32), array(0.954, dtype=float32), array(0.963, dtype=float32), array(0.936, dtype=float32)]\n",
      "Iteration 36300 Training loss 0.010168522596359253 Validation loss 0.01293726172298193 Accuracy 0.8634999990463257 Accuracies by class [array(0.753, dtype=float32), array(0.966, dtype=float32), array(0.82, dtype=float32), array(0.905, dtype=float32), array(0.741, dtype=float32), array(0.943, dtype=float32), array(0.664, dtype=float32), array(0.92, dtype=float32), array(0.967, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 36310 Training loss 0.009645569138228893 Validation loss 0.012545906007289886 Accuracy 0.8684999942779541 Accuracies by class [array(0.852, dtype=float32), array(0.968, dtype=float32), array(0.837, dtype=float32), array(0.891, dtype=float32), array(0.782, dtype=float32), array(0.937, dtype=float32), array(0.575, dtype=float32), array(0.919, dtype=float32), array(0.965, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 36320 Training loss 0.008525185286998749 Validation loss 0.012811427935957909 Accuracy 0.8646000027656555 Accuracies by class [array(0.843, dtype=float32), array(0.968, dtype=float32), array(0.797, dtype=float32), array(0.907, dtype=float32), array(0.783, dtype=float32), array(0.924, dtype=float32), array(0.587, dtype=float32), array(0.961, dtype=float32), array(0.959, dtype=float32), array(0.917, dtype=float32)]\n",
      "Iteration 36330 Training loss 0.009663893841207027 Validation loss 0.012843585573136806 Accuracy 0.864799976348877 Accuracies by class [array(0.766, dtype=float32), array(0.963, dtype=float32), array(0.725, dtype=float32), array(0.896, dtype=float32), array(0.806, dtype=float32), array(0.936, dtype=float32), array(0.713, dtype=float32), array(0.938, dtype=float32), array(0.958, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 36340 Training loss 0.009814235381782055 Validation loss 0.012447275221347809 Accuracy 0.8700000047683716 Accuracies by class [array(0.801, dtype=float32), array(0.97, dtype=float32), array(0.801, dtype=float32), array(0.899, dtype=float32), array(0.775, dtype=float32), array(0.949, dtype=float32), array(0.665, dtype=float32), array(0.924, dtype=float32), array(0.963, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 36350 Training loss 0.008672844618558884 Validation loss 0.012623121030628681 Accuracy 0.8675000071525574 Accuracies by class [array(0.827, dtype=float32), array(0.961, dtype=float32), array(0.824, dtype=float32), array(0.913, dtype=float32), array(0.779, dtype=float32), array(0.928, dtype=float32), array(0.593, dtype=float32), array(0.95, dtype=float32), array(0.965, dtype=float32), array(0.935, dtype=float32)]\n",
      "Iteration 36360 Training loss 0.010194974020123482 Validation loss 0.012619733810424805 Accuracy 0.8676999807357788 Accuracies by class [array(0.812, dtype=float32), array(0.965, dtype=float32), array(0.814, dtype=float32), array(0.904, dtype=float32), array(0.771, dtype=float32), array(0.938, dtype=float32), array(0.626, dtype=float32), array(0.919, dtype=float32), array(0.967, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 36370 Training loss 0.008333167061209679 Validation loss 0.012565801851451397 Accuracy 0.8677999973297119 Accuracies by class [array(0.813, dtype=float32), array(0.965, dtype=float32), array(0.736, dtype=float32), array(0.888, dtype=float32), array(0.807, dtype=float32), array(0.939, dtype=float32), array(0.679, dtype=float32), array(0.941, dtype=float32), array(0.958, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 36380 Training loss 0.01019079890102148 Validation loss 0.013227680698037148 Accuracy 0.8610000014305115 Accuracies by class [array(0.78, dtype=float32), array(0.964, dtype=float32), array(0.656, dtype=float32), array(0.896, dtype=float32), array(0.88, dtype=float32), array(0.94, dtype=float32), array(0.645, dtype=float32), array(0.938, dtype=float32), array(0.961, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 36390 Training loss 0.010914924554526806 Validation loss 0.012779169715940952 Accuracy 0.8659999966621399 Accuracies by class [array(0.848, dtype=float32), array(0.96, dtype=float32), array(0.715, dtype=float32), array(0.922, dtype=float32), array(0.803, dtype=float32), array(0.937, dtype=float32), array(0.626, dtype=float32), array(0.918, dtype=float32), array(0.965, dtype=float32), array(0.966, dtype=float32)]\n",
      "Iteration 36400 Training loss 0.008340726606547832 Validation loss 0.012622636742889881 Accuracy 0.8679999709129333 Accuracies by class [array(0.844, dtype=float32), array(0.961, dtype=float32), array(0.721, dtype=float32), array(0.895, dtype=float32), array(0.778, dtype=float32), array(0.937, dtype=float32), array(0.677, dtype=float32), array(0.948, dtype=float32), array(0.969, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 36410 Training loss 0.008462668396532536 Validation loss 0.012838684022426605 Accuracy 0.8652999997138977 Accuracies by class [array(0.767, dtype=float32), array(0.96, dtype=float32), array(0.784, dtype=float32), array(0.923, dtype=float32), array(0.812, dtype=float32), array(0.936, dtype=float32), array(0.629, dtype=float32), array(0.917, dtype=float32), array(0.962, dtype=float32), array(0.963, dtype=float32)]\n",
      "Iteration 36420 Training loss 0.009114725515246391 Validation loss 0.012531677260994911 Accuracy 0.8669999837875366 Accuracies by class [array(0.836, dtype=float32), array(0.965, dtype=float32), array(0.783, dtype=float32), array(0.904, dtype=float32), array(0.804, dtype=float32), array(0.939, dtype=float32), array(0.585, dtype=float32), array(0.939, dtype=float32), array(0.965, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 36430 Training loss 0.010673019103705883 Validation loss 0.013448376208543777 Accuracy 0.8583999872207642 Accuracies by class [array(0.66, dtype=float32), array(0.962, dtype=float32), array(0.796, dtype=float32), array(0.899, dtype=float32), array(0.763, dtype=float32), array(0.947, dtype=float32), array(0.737, dtype=float32), array(0.898, dtype=float32), array(0.962, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 36440 Training loss 0.008646332658827305 Validation loss 0.012364521622657776 Accuracy 0.8698999881744385 Accuracies by class [array(0.833, dtype=float32), array(0.964, dtype=float32), array(0.789, dtype=float32), array(0.911, dtype=float32), array(0.789, dtype=float32), array(0.94, dtype=float32), array(0.623, dtype=float32), array(0.962, dtype=float32), array(0.965, dtype=float32), array(0.923, dtype=float32)]\n",
      "Iteration 36450 Training loss 0.008364571258425713 Validation loss 0.012681243941187859 Accuracy 0.8671000003814697 Accuracies by class [array(0.845, dtype=float32), array(0.972, dtype=float32), array(0.749, dtype=float32), array(0.83, dtype=float32), array(0.85, dtype=float32), array(0.947, dtype=float32), array(0.629, dtype=float32), array(0.917, dtype=float32), array(0.969, dtype=float32), array(0.963, dtype=float32)]\n",
      "Iteration 36460 Training loss 0.009039501659572124 Validation loss 0.013145895674824715 Accuracy 0.8626999855041504 Accuracies by class [array(0.883, dtype=float32), array(0.971, dtype=float32), array(0.785, dtype=float32), array(0.886, dtype=float32), array(0.831, dtype=float32), array(0.939, dtype=float32), array(0.498, dtype=float32), array(0.895, dtype=float32), array(0.97, dtype=float32), array(0.969, dtype=float32)]\n",
      "Iteration 36470 Training loss 0.007747477386146784 Validation loss 0.01256065908819437 Accuracy 0.8680999875068665 Accuracies by class [array(0.84, dtype=float32), array(0.965, dtype=float32), array(0.794, dtype=float32), array(0.915, dtype=float32), array(0.817, dtype=float32), array(0.932, dtype=float32), array(0.548, dtype=float32), array(0.939, dtype=float32), array(0.967, dtype=float32), array(0.964, dtype=float32)]\n",
      "Iteration 36480 Training loss 0.00698036840185523 Validation loss 0.012512207962572575 Accuracy 0.8673999905586243 Accuracies by class [array(0.803, dtype=float32), array(0.965, dtype=float32), array(0.791, dtype=float32), array(0.916, dtype=float32), array(0.808, dtype=float32), array(0.931, dtype=float32), array(0.612, dtype=float32), array(0.956, dtype=float32), array(0.963, dtype=float32), array(0.929, dtype=float32)]\n",
      "Iteration 36490 Training loss 0.010463579557836056 Validation loss 0.012855807319283485 Accuracy 0.8644999861717224 Accuracies by class [array(0.81, dtype=float32), array(0.97, dtype=float32), array(0.712, dtype=float32), array(0.883, dtype=float32), array(0.798, dtype=float32), array(0.943, dtype=float32), array(0.683, dtype=float32), array(0.92, dtype=float32), array(0.965, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 36500 Training loss 0.010159549303352833 Validation loss 0.012786968611180782 Accuracy 0.8654999732971191 Accuracies by class [array(0.868, dtype=float32), array(0.967, dtype=float32), array(0.844, dtype=float32), array(0.894, dtype=float32), array(0.745, dtype=float32), array(0.934, dtype=float32), array(0.545, dtype=float32), array(0.936, dtype=float32), array(0.966, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 36510 Training loss 0.008242569863796234 Validation loss 0.012767678126692772 Accuracy 0.8661999702453613 Accuracies by class [array(0.84, dtype=float32), array(0.969, dtype=float32), array(0.804, dtype=float32), array(0.9, dtype=float32), array(0.802, dtype=float32), array(0.937, dtype=float32), array(0.568, dtype=float32), array(0.936, dtype=float32), array(0.957, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 36520 Training loss 0.008067607879638672 Validation loss 0.012672074139118195 Accuracy 0.8672000169754028 Accuracies by class [array(0.843, dtype=float32), array(0.972, dtype=float32), array(0.807, dtype=float32), array(0.861, dtype=float32), array(0.817, dtype=float32), array(0.949, dtype=float32), array(0.579, dtype=float32), array(0.927, dtype=float32), array(0.965, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 36530 Training loss 0.006495553534477949 Validation loss 0.012951673939824104 Accuracy 0.8633999824523926 Accuracies by class [array(0.774, dtype=float32), array(0.967, dtype=float32), array(0.739, dtype=float32), array(0.908, dtype=float32), array(0.859, dtype=float32), array(0.937, dtype=float32), array(0.588, dtype=float32), array(0.95, dtype=float32), array(0.966, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 36540 Training loss 0.009670451283454895 Validation loss 0.012729919515550137 Accuracy 0.866599977016449 Accuracies by class [array(0.829, dtype=float32), array(0.965, dtype=float32), array(0.785, dtype=float32), array(0.899, dtype=float32), array(0.819, dtype=float32), array(0.935, dtype=float32), array(0.585, dtype=float32), array(0.925, dtype=float32), array(0.964, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 36550 Training loss 0.009048438630998135 Validation loss 0.012629374861717224 Accuracy 0.8676000237464905 Accuracies by class [array(0.839, dtype=float32), array(0.966, dtype=float32), array(0.782, dtype=float32), array(0.881, dtype=float32), array(0.798, dtype=float32), array(0.94, dtype=float32), array(0.618, dtype=float32), array(0.938, dtype=float32), array(0.963, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 36560 Training loss 0.00790031161159277 Validation loss 0.012482581660151482 Accuracy 0.8690999746322632 Accuracies by class [array(0.76, dtype=float32), array(0.968, dtype=float32), array(0.78, dtype=float32), array(0.891, dtype=float32), array(0.831, dtype=float32), array(0.943, dtype=float32), array(0.673, dtype=float32), array(0.928, dtype=float32), array(0.96, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 36570 Training loss 0.008668513968586922 Validation loss 0.013073891401290894 Accuracy 0.8629999756813049 Accuracies by class [array(0.864, dtype=float32), array(0.965, dtype=float32), array(0.722, dtype=float32), array(0.854, dtype=float32), array(0.785, dtype=float32), array(0.937, dtype=float32), array(0.663, dtype=float32), array(0.908, dtype=float32), array(0.964, dtype=float32), array(0.968, dtype=float32)]\n",
      "Iteration 36580 Training loss 0.008281508460640907 Validation loss 0.012711804360151291 Accuracy 0.866599977016449 Accuracies by class [array(0.827, dtype=float32), array(0.961, dtype=float32), array(0.83, dtype=float32), array(0.917, dtype=float32), array(0.777, dtype=float32), array(0.938, dtype=float32), array(0.556, dtype=float32), array(0.94, dtype=float32), array(0.968, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 36590 Training loss 0.008834222331643105 Validation loss 0.012928959913551807 Accuracy 0.8637999892234802 Accuracies by class [array(0.839, dtype=float32), array(0.968, dtype=float32), array(0.738, dtype=float32), array(0.868, dtype=float32), array(0.828, dtype=float32), array(0.938, dtype=float32), array(0.643, dtype=float32), array(0.883, dtype=float32), array(0.964, dtype=float32), array(0.969, dtype=float32)]\n",
      "Iteration 36600 Training loss 0.009189575910568237 Validation loss 0.01265888661146164 Accuracy 0.8669000267982483 Accuracies by class [array(0.777, dtype=float32), array(0.967, dtype=float32), array(0.769, dtype=float32), array(0.902, dtype=float32), array(0.779, dtype=float32), array(0.948, dtype=float32), array(0.697, dtype=float32), array(0.934, dtype=float32), array(0.954, dtype=float32), array(0.942, dtype=float32)]\n",
      "Iteration 36610 Training loss 0.008539354428648949 Validation loss 0.012774844653904438 Accuracy 0.8651000261306763 Accuracies by class [array(0.805, dtype=float32), array(0.965, dtype=float32), array(0.791, dtype=float32), array(0.912, dtype=float32), array(0.759, dtype=float32), array(0.936, dtype=float32), array(0.632, dtype=float32), array(0.941, dtype=float32), array(0.962, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 36620 Training loss 0.007781561464071274 Validation loss 0.013205920346081257 Accuracy 0.8610000014305115 Accuracies by class [array(0.849, dtype=float32), array(0.965, dtype=float32), array(0.71, dtype=float32), array(0.83, dtype=float32), array(0.865, dtype=float32), array(0.965, dtype=float32), array(0.621, dtype=float32), array(0.891, dtype=float32), array(0.973, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 36630 Training loss 0.00830344669520855 Validation loss 0.012685822322964668 Accuracy 0.8672999739646912 Accuracies by class [array(0.859, dtype=float32), array(0.967, dtype=float32), array(0.808, dtype=float32), array(0.874, dtype=float32), array(0.787, dtype=float32), array(0.947, dtype=float32), array(0.584, dtype=float32), array(0.918, dtype=float32), array(0.966, dtype=float32), array(0.963, dtype=float32)]\n",
      "Iteration 36640 Training loss 0.009945274330675602 Validation loss 0.01288447342813015 Accuracy 0.8640000224113464 Accuracies by class [array(0.856, dtype=float32), array(0.966, dtype=float32), array(0.839, dtype=float32), array(0.884, dtype=float32), array(0.782, dtype=float32), array(0.946, dtype=float32), array(0.529, dtype=float32), array(0.917, dtype=float32), array(0.966, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 36650 Training loss 0.010972513817250729 Validation loss 0.012866678647696972 Accuracy 0.864799976348877 Accuracies by class [array(0.782, dtype=float32), array(0.965, dtype=float32), array(0.777, dtype=float32), array(0.908, dtype=float32), array(0.84, dtype=float32), array(0.946, dtype=float32), array(0.599, dtype=float32), array(0.955, dtype=float32), array(0.966, dtype=float32), array(0.91, dtype=float32)]\n",
      "Iteration 36660 Training loss 0.00916888564825058 Validation loss 0.012914094142615795 Accuracy 0.8640000224113464 Accuracies by class [array(0.872, dtype=float32), array(0.967, dtype=float32), array(0.858, dtype=float32), array(0.88, dtype=float32), array(0.715, dtype=float32), array(0.942, dtype=float32), array(0.56, dtype=float32), array(0.953, dtype=float32), array(0.967, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 36670 Training loss 0.007094207219779491 Validation loss 0.012382004410028458 Accuracy 0.8702999949455261 Accuracies by class [array(0.811, dtype=float32), array(0.969, dtype=float32), array(0.791, dtype=float32), array(0.898, dtype=float32), array(0.782, dtype=float32), array(0.955, dtype=float32), array(0.661, dtype=float32), array(0.939, dtype=float32), array(0.967, dtype=float32), array(0.93, dtype=float32)]\n",
      "Iteration 36680 Training loss 0.007540192920714617 Validation loss 0.012970732524991035 Accuracy 0.8626000285148621 Accuracies by class [array(0.789, dtype=float32), array(0.964, dtype=float32), array(0.853, dtype=float32), array(0.921, dtype=float32), array(0.683, dtype=float32), array(0.943, dtype=float32), array(0.645, dtype=float32), array(0.961, dtype=float32), array(0.958, dtype=float32), array(0.909, dtype=float32)]\n",
      "Iteration 36690 Training loss 0.008797303773462772 Validation loss 0.012750090099871159 Accuracy 0.864799976348877 Accuracies by class [array(0.818, dtype=float32), array(0.967, dtype=float32), array(0.824, dtype=float32), array(0.898, dtype=float32), array(0.716, dtype=float32), array(0.956, dtype=float32), array(0.657, dtype=float32), array(0.883, dtype=float32), array(0.965, dtype=float32), array(0.964, dtype=float32)]\n",
      "Iteration 36700 Training loss 0.0069147865287959576 Validation loss 0.012876595370471478 Accuracy 0.8648999929428101 Accuracies by class [array(0.822, dtype=float32), array(0.97, dtype=float32), array(0.759, dtype=float32), array(0.88, dtype=float32), array(0.872, dtype=float32), array(0.938, dtype=float32), array(0.566, dtype=float32), array(0.91, dtype=float32), array(0.966, dtype=float32), array(0.966, dtype=float32)]\n",
      "Iteration 36710 Training loss 0.007060034666210413 Validation loss 0.012403403408825397 Accuracy 0.8694999814033508 Accuracies by class [array(0.796, dtype=float32), array(0.966, dtype=float32), array(0.819, dtype=float32), array(0.889, dtype=float32), array(0.767, dtype=float32), array(0.939, dtype=float32), array(0.682, dtype=float32), array(0.955, dtype=float32), array(0.945, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 36720 Training loss 0.00820627249777317 Validation loss 0.012657330371439457 Accuracy 0.8661999702453613 Accuracies by class [array(0.76, dtype=float32), array(0.97, dtype=float32), array(0.829, dtype=float32), array(0.874, dtype=float32), array(0.773, dtype=float32), array(0.944, dtype=float32), array(0.674, dtype=float32), array(0.936, dtype=float32), array(0.951, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 36730 Training loss 0.008040501736104488 Validation loss 0.012350465171039104 Accuracy 0.8701000213623047 Accuracies by class [array(0.854, dtype=float32), array(0.966, dtype=float32), array(0.776, dtype=float32), array(0.889, dtype=float32), array(0.8, dtype=float32), array(0.941, dtype=float32), array(0.633, dtype=float32), array(0.936, dtype=float32), array(0.952, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 36740 Training loss 0.007943757809698582 Validation loss 0.012570028193295002 Accuracy 0.867900013923645 Accuracies by class [array(0.829, dtype=float32), array(0.97, dtype=float32), array(0.757, dtype=float32), array(0.892, dtype=float32), array(0.855, dtype=float32), array(0.94, dtype=float32), array(0.588, dtype=float32), array(0.955, dtype=float32), array(0.964, dtype=float32), array(0.929, dtype=float32)]\n",
      "Iteration 36750 Training loss 0.008916187100112438 Validation loss 0.012614964507520199 Accuracy 0.8661999702453613 Accuracies by class [array(0.79, dtype=float32), array(0.969, dtype=float32), array(0.757, dtype=float32), array(0.886, dtype=float32), array(0.826, dtype=float32), array(0.933, dtype=float32), array(0.645, dtype=float32), array(0.943, dtype=float32), array(0.962, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 36760 Training loss 0.007702857255935669 Validation loss 0.012376791797578335 Accuracy 0.8684999942779541 Accuracies by class [array(0.821, dtype=float32), array(0.97, dtype=float32), array(0.76, dtype=float32), array(0.878, dtype=float32), array(0.839, dtype=float32), array(0.946, dtype=float32), array(0.623, dtype=float32), array(0.942, dtype=float32), array(0.965, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 36770 Training loss 0.009699912741780281 Validation loss 0.013170519843697548 Accuracy 0.8614000082015991 Accuracies by class [array(0.763, dtype=float32), array(0.97, dtype=float32), array(0.721, dtype=float32), array(0.889, dtype=float32), array(0.713, dtype=float32), array(0.946, dtype=float32), array(0.766, dtype=float32), array(0.938, dtype=float32), array(0.963, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 36780 Training loss 0.008183873258531094 Validation loss 0.012358266860246658 Accuracy 0.870199978351593 Accuracies by class [array(0.801, dtype=float32), array(0.968, dtype=float32), array(0.784, dtype=float32), array(0.894, dtype=float32), array(0.771, dtype=float32), array(0.934, dtype=float32), array(0.693, dtype=float32), array(0.94, dtype=float32), array(0.963, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 36790 Training loss 0.008087188005447388 Validation loss 0.012399794533848763 Accuracy 0.869700014591217 Accuracies by class [array(0.804, dtype=float32), array(0.961, dtype=float32), array(0.79, dtype=float32), array(0.907, dtype=float32), array(0.828, dtype=float32), array(0.941, dtype=float32), array(0.614, dtype=float32), array(0.949, dtype=float32), array(0.964, dtype=float32), array(0.939, dtype=float32)]\n",
      "Iteration 36800 Training loss 0.007449820637702942 Validation loss 0.01305244117975235 Accuracy 0.8615000247955322 Accuracies by class [array(0.817, dtype=float32), array(0.964, dtype=float32), array(0.655, dtype=float32), array(0.889, dtype=float32), array(0.805, dtype=float32), array(0.942, dtype=float32), array(0.702, dtype=float32), array(0.917, dtype=float32), array(0.965, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 36810 Training loss 0.008974174037575722 Validation loss 0.012839579954743385 Accuracy 0.8640999794006348 Accuracies by class [array(0.865, dtype=float32), array(0.972, dtype=float32), array(0.802, dtype=float32), array(0.855, dtype=float32), array(0.818, dtype=float32), array(0.936, dtype=float32), array(0.532, dtype=float32), array(0.955, dtype=float32), array(0.962, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 36820 Training loss 0.008920497260987759 Validation loss 0.0124458447098732 Accuracy 0.8697999715805054 Accuracies by class [array(0.813, dtype=float32), array(0.971, dtype=float32), array(0.82, dtype=float32), array(0.892, dtype=float32), array(0.826, dtype=float32), array(0.949, dtype=float32), array(0.583, dtype=float32), array(0.927, dtype=float32), array(0.962, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 36830 Training loss 0.009604392573237419 Validation loss 0.01258020754903555 Accuracy 0.8676999807357788 Accuracies by class [array(0.809, dtype=float32), array(0.967, dtype=float32), array(0.807, dtype=float32), array(0.908, dtype=float32), array(0.822, dtype=float32), array(0.938, dtype=float32), array(0.576, dtype=float32), array(0.927, dtype=float32), array(0.964, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 36840 Training loss 0.009662316180765629 Validation loss 0.012895463965833187 Accuracy 0.8629999756813049 Accuracies by class [array(0.833, dtype=float32), array(0.971, dtype=float32), array(0.792, dtype=float32), array(0.878, dtype=float32), array(0.833, dtype=float32), array(0.941, dtype=float32), array(0.559, dtype=float32), array(0.892, dtype=float32), array(0.963, dtype=float32), array(0.968, dtype=float32)]\n",
      "Iteration 36850 Training loss 0.009229777380824089 Validation loss 0.012498456053435802 Accuracy 0.8676000237464905 Accuracies by class [array(0.809, dtype=float32), array(0.97, dtype=float32), array(0.738, dtype=float32), array(0.894, dtype=float32), array(0.81, dtype=float32), array(0.945, dtype=float32), array(0.663, dtype=float32), array(0.929, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 36860 Training loss 0.006761109456419945 Validation loss 0.012852638959884644 Accuracy 0.8644999861717224 Accuracies by class [array(0.818, dtype=float32), array(0.971, dtype=float32), array(0.857, dtype=float32), array(0.877, dtype=float32), array(0.746, dtype=float32), array(0.944, dtype=float32), array(0.596, dtype=float32), array(0.919, dtype=float32), array(0.962, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 36870 Training loss 0.007339331321418285 Validation loss 0.0129036670550704 Accuracy 0.8629999756813049 Accuracies by class [array(0.758, dtype=float32), array(0.97, dtype=float32), array(0.773, dtype=float32), array(0.856, dtype=float32), array(0.772, dtype=float32), array(0.94, dtype=float32), array(0.731, dtype=float32), array(0.937, dtype=float32), array(0.947, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 36880 Training loss 0.007441104389727116 Validation loss 0.012509675696492195 Accuracy 0.8689000010490417 Accuracies by class [array(0.889, dtype=float32), array(0.971, dtype=float32), array(0.791, dtype=float32), array(0.864, dtype=float32), array(0.812, dtype=float32), array(0.942, dtype=float32), array(0.559, dtype=float32), array(0.941, dtype=float32), array(0.969, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 36890 Training loss 0.008044061250984669 Validation loss 0.012182890437543392 Accuracy 0.8712000250816345 Accuracies by class [array(0.828, dtype=float32), array(0.962, dtype=float32), array(0.777, dtype=float32), array(0.908, dtype=float32), array(0.773, dtype=float32), array(0.952, dtype=float32), array(0.667, dtype=float32), array(0.94, dtype=float32), array(0.963, dtype=float32), array(0.942, dtype=float32)]\n",
      "Iteration 36900 Training loss 0.009029573760926723 Validation loss 0.012838358990848064 Accuracy 0.8655999898910522 Accuracies by class [array(0.888, dtype=float32), array(0.971, dtype=float32), array(0.826, dtype=float32), array(0.86, dtype=float32), array(0.739, dtype=float32), array(0.941, dtype=float32), array(0.593, dtype=float32), array(0.928, dtype=float32), array(0.952, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 36910 Training loss 0.010264972224831581 Validation loss 0.01316965464502573 Accuracy 0.8621000051498413 Accuracies by class [array(0.881, dtype=float32), array(0.974, dtype=float32), array(0.797, dtype=float32), array(0.881, dtype=float32), array(0.825, dtype=float32), array(0.934, dtype=float32), array(0.49, dtype=float32), array(0.968, dtype=float32), array(0.961, dtype=float32), array(0.91, dtype=float32)]\n",
      "Iteration 36920 Training loss 0.010113230906426907 Validation loss 0.012313964776694775 Accuracy 0.870199978351593 Accuracies by class [array(0.794, dtype=float32), array(0.971, dtype=float32), array(0.799, dtype=float32), array(0.894, dtype=float32), array(0.787, dtype=float32), array(0.951, dtype=float32), array(0.654, dtype=float32), array(0.946, dtype=float32), array(0.966, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 36930 Training loss 0.009283922612667084 Validation loss 0.01224032323807478 Accuracy 0.8708000183105469 Accuracies by class [array(0.798, dtype=float32), array(0.971, dtype=float32), array(0.789, dtype=float32), array(0.893, dtype=float32), array(0.756, dtype=float32), array(0.945, dtype=float32), array(0.698, dtype=float32), array(0.942, dtype=float32), array(0.968, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 36940 Training loss 0.008052340708673 Validation loss 0.012486003339290619 Accuracy 0.868399977684021 Accuracies by class [array(0.8, dtype=float32), array(0.969, dtype=float32), array(0.783, dtype=float32), array(0.908, dtype=float32), array(0.798, dtype=float32), array(0.94, dtype=float32), array(0.631, dtype=float32), array(0.954, dtype=float32), array(0.963, dtype=float32), array(0.938, dtype=float32)]\n",
      "Iteration 36950 Training loss 0.007672253996133804 Validation loss 0.013117438182234764 Accuracy 0.8611000180244446 Accuracies by class [array(0.757, dtype=float32), array(0.975, dtype=float32), array(0.746, dtype=float32), array(0.881, dtype=float32), array(0.736, dtype=float32), array(0.951, dtype=float32), array(0.727, dtype=float32), array(0.925, dtype=float32), array(0.97, dtype=float32), array(0.943, dtype=float32)]\n",
      "Iteration 36960 Training loss 0.010006153024733067 Validation loss 0.012026002630591393 Accuracy 0.8738999962806702 Accuracies by class [array(0.839, dtype=float32), array(0.973, dtype=float32), array(0.801, dtype=float32), array(0.887, dtype=float32), array(0.806, dtype=float32), array(0.943, dtype=float32), array(0.628, dtype=float32), array(0.944, dtype=float32), array(0.965, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 36970 Training loss 0.008999818935990334 Validation loss 0.012147882021963596 Accuracy 0.871999979019165 Accuracies by class [array(0.819, dtype=float32), array(0.971, dtype=float32), array(0.798, dtype=float32), array(0.876, dtype=float32), array(0.795, dtype=float32), array(0.938, dtype=float32), array(0.668, dtype=float32), array(0.94, dtype=float32), array(0.96, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 36980 Training loss 0.007950086146593094 Validation loss 0.012200025841593742 Accuracy 0.871399998664856 Accuracies by class [array(0.826, dtype=float32), array(0.971, dtype=float32), array(0.766, dtype=float32), array(0.889, dtype=float32), array(0.838, dtype=float32), array(0.944, dtype=float32), array(0.639, dtype=float32), array(0.955, dtype=float32), array(0.958, dtype=float32), array(0.928, dtype=float32)]\n",
      "Iteration 36990 Training loss 0.008924218825995922 Validation loss 0.01254957914352417 Accuracy 0.8682000041007996 Accuracies by class [array(0.863, dtype=float32), array(0.969, dtype=float32), array(0.822, dtype=float32), array(0.899, dtype=float32), array(0.788, dtype=float32), array(0.943, dtype=float32), array(0.551, dtype=float32), array(0.934, dtype=float32), array(0.957, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37000 Training loss 0.00873782392591238 Validation loss 0.01229344867169857 Accuracy 0.8704000115394592 Accuracies by class [array(0.81, dtype=float32), array(0.967, dtype=float32), array(0.804, dtype=float32), array(0.913, dtype=float32), array(0.758, dtype=float32), array(0.949, dtype=float32), array(0.661, dtype=float32), array(0.947, dtype=float32), array(0.961, dtype=float32), array(0.934, dtype=float32)]\n",
      "Iteration 37010 Training loss 0.009252030402421951 Validation loss 0.012729576788842678 Accuracy 0.8657000064849854 Accuracies by class [array(0.737, dtype=float32), array(0.965, dtype=float32), array(0.819, dtype=float32), array(0.897, dtype=float32), array(0.732, dtype=float32), array(0.944, dtype=float32), array(0.721, dtype=float32), array(0.933, dtype=float32), array(0.96, dtype=float32), array(0.949, dtype=float32)]\n",
      "Iteration 37020 Training loss 0.010831421241164207 Validation loss 0.013036726973950863 Accuracy 0.863099992275238 Accuracies by class [array(0.84, dtype=float32), array(0.96, dtype=float32), array(0.749, dtype=float32), array(0.905, dtype=float32), array(0.701, dtype=float32), array(0.949, dtype=float32), array(0.699, dtype=float32), array(0.903, dtype=float32), array(0.963, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 37030 Training loss 0.005445095244795084 Validation loss 0.012122821062803268 Accuracy 0.8725000023841858 Accuracies by class [array(0.817, dtype=float32), array(0.968, dtype=float32), array(0.797, dtype=float32), array(0.897, dtype=float32), array(0.815, dtype=float32), array(0.942, dtype=float32), array(0.64, dtype=float32), array(0.935, dtype=float32), array(0.964, dtype=float32), array(0.95, dtype=float32)]\n",
      "Iteration 37040 Training loss 0.010296422056853771 Validation loss 0.01242950651794672 Accuracy 0.8684999942779541 Accuracies by class [array(0.78, dtype=float32), array(0.964, dtype=float32), array(0.774, dtype=float32), array(0.903, dtype=float32), array(0.833, dtype=float32), array(0.945, dtype=float32), array(0.659, dtype=float32), array(0.901, dtype=float32), array(0.965, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 37050 Training loss 0.012417900376021862 Validation loss 0.012776975519955158 Accuracy 0.8652999997138977 Accuracies by class [array(0.844, dtype=float32), array(0.975, dtype=float32), array(0.754, dtype=float32), array(0.812, dtype=float32), array(0.863, dtype=float32), array(0.941, dtype=float32), array(0.62, dtype=float32), array(0.935, dtype=float32), array(0.962, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 37060 Training loss 0.010743746533989906 Validation loss 0.012899628840386868 Accuracy 0.8644000291824341 Accuracies by class [array(0.722, dtype=float32), array(0.97, dtype=float32), array(0.823, dtype=float32), array(0.905, dtype=float32), array(0.781, dtype=float32), array(0.946, dtype=float32), array(0.668, dtype=float32), array(0.961, dtype=float32), array(0.963, dtype=float32), array(0.905, dtype=float32)]\n",
      "Iteration 37070 Training loss 0.008569742552936077 Validation loss 0.012400508858263493 Accuracy 0.8691999912261963 Accuracies by class [array(0.833, dtype=float32), array(0.971, dtype=float32), array(0.808, dtype=float32), array(0.886, dtype=float32), array(0.771, dtype=float32), array(0.942, dtype=float32), array(0.638, dtype=float32), array(0.913, dtype=float32), array(0.969, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 37080 Training loss 0.010728627443313599 Validation loss 0.012787790969014168 Accuracy 0.8639000058174133 Accuracies by class [array(0.827, dtype=float32), array(0.964, dtype=float32), array(0.782, dtype=float32), array(0.907, dtype=float32), array(0.676, dtype=float32), array(0.941, dtype=float32), array(0.691, dtype=float32), array(0.938, dtype=float32), array(0.962, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 37090 Training loss 0.00832726713269949 Validation loss 0.012427241541445255 Accuracy 0.8690999746322632 Accuracies by class [array(0.81, dtype=float32), array(0.967, dtype=float32), array(0.744, dtype=float32), array(0.869, dtype=float32), array(0.834, dtype=float32), array(0.939, dtype=float32), array(0.676, dtype=float32), array(0.949, dtype=float32), array(0.963, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 37100 Training loss 0.007126804906874895 Validation loss 0.012357877567410469 Accuracy 0.8690999746322632 Accuracies by class [array(0.793, dtype=float32), array(0.971, dtype=float32), array(0.775, dtype=float32), array(0.883, dtype=float32), array(0.815, dtype=float32), array(0.946, dtype=float32), array(0.663, dtype=float32), array(0.945, dtype=float32), array(0.965, dtype=float32), array(0.935, dtype=float32)]\n",
      "Iteration 37110 Training loss 0.008634236641228199 Validation loss 0.012653355486690998 Accuracy 0.8657000064849854 Accuracies by class [array(0.773, dtype=float32), array(0.967, dtype=float32), array(0.79, dtype=float32), array(0.886, dtype=float32), array(0.711, dtype=float32), array(0.94, dtype=float32), array(0.746, dtype=float32), array(0.945, dtype=float32), array(0.959, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 37120 Training loss 0.010790439322590828 Validation loss 0.013090345077216625 Accuracy 0.8622000217437744 Accuracies by class [array(0.833, dtype=float32), array(0.973, dtype=float32), array(0.746, dtype=float32), array(0.897, dtype=float32), array(0.856, dtype=float32), array(0.942, dtype=float32), array(0.533, dtype=float32), array(0.921, dtype=float32), array(0.965, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37130 Training loss 0.00870840810239315 Validation loss 0.012623291462659836 Accuracy 0.8665000200271606 Accuracies by class [array(0.801, dtype=float32), array(0.969, dtype=float32), array(0.768, dtype=float32), array(0.883, dtype=float32), array(0.754, dtype=float32), array(0.942, dtype=float32), array(0.705, dtype=float32), array(0.943, dtype=float32), array(0.959, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 37140 Training loss 0.010769798420369625 Validation loss 0.01242347527295351 Accuracy 0.8695999979972839 Accuracies by class [array(0.846, dtype=float32), array(0.969, dtype=float32), array(0.814, dtype=float32), array(0.873, dtype=float32), array(0.799, dtype=float32), array(0.939, dtype=float32), array(0.6, dtype=float32), array(0.942, dtype=float32), array(0.968, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 37150 Training loss 0.007899253629148006 Validation loss 0.012690749950706959 Accuracy 0.8669999837875366 Accuracies by class [array(0.742, dtype=float32), array(0.967, dtype=float32), array(0.774, dtype=float32), array(0.893, dtype=float32), array(0.786, dtype=float32), array(0.939, dtype=float32), array(0.718, dtype=float32), array(0.949, dtype=float32), array(0.962, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 37160 Training loss 0.009404022246599197 Validation loss 0.013134678825736046 Accuracy 0.8610000014305115 Accuracies by class [array(0.858, dtype=float32), array(0.971, dtype=float32), array(0.82, dtype=float32), array(0.861, dtype=float32), array(0.825, dtype=float32), array(0.954, dtype=float32), array(0.503, dtype=float32), array(0.906, dtype=float32), array(0.966, dtype=float32), array(0.946, dtype=float32)]\n",
      "Iteration 37170 Training loss 0.008420620113611221 Validation loss 0.013476982712745667 Accuracy 0.858299970626831 Accuracies by class [array(0.787, dtype=float32), array(0.969, dtype=float32), array(0.692, dtype=float32), array(0.888, dtype=float32), array(0.715, dtype=float32), array(0.947, dtype=float32), array(0.741, dtype=float32), array(0.943, dtype=float32), array(0.966, dtype=float32), array(0.935, dtype=float32)]\n",
      "Iteration 37180 Training loss 0.010128958150744438 Validation loss 0.012832390144467354 Accuracy 0.8652999997138977 Accuracies by class [array(0.741, dtype=float32), array(0.967, dtype=float32), array(0.863, dtype=float32), array(0.901, dtype=float32), array(0.773, dtype=float32), array(0.939, dtype=float32), array(0.619, dtype=float32), array(0.943, dtype=float32), array(0.959, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 37190 Training loss 0.009745964780449867 Validation loss 0.012305651791393757 Accuracy 0.8697999715805054 Accuracies by class [array(0.845, dtype=float32), array(0.966, dtype=float32), array(0.82, dtype=float32), array(0.879, dtype=float32), array(0.776, dtype=float32), array(0.946, dtype=float32), array(0.623, dtype=float32), array(0.921, dtype=float32), array(0.966, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37200 Training loss 0.00914724636822939 Validation loss 0.012304945848882198 Accuracy 0.8694999814033508 Accuracies by class [array(0.78, dtype=float32), array(0.968, dtype=float32), array(0.824, dtype=float32), array(0.899, dtype=float32), array(0.769, dtype=float32), array(0.945, dtype=float32), array(0.665, dtype=float32), array(0.952, dtype=float32), array(0.967, dtype=float32), array(0.926, dtype=float32)]\n",
      "Iteration 37210 Training loss 0.0073887379840016365 Validation loss 0.012343856506049633 Accuracy 0.8682000041007996 Accuracies by class [array(0.788, dtype=float32), array(0.969, dtype=float32), array(0.802, dtype=float32), array(0.874, dtype=float32), array(0.809, dtype=float32), array(0.943, dtype=float32), array(0.65, dtype=float32), array(0.918, dtype=float32), array(0.966, dtype=float32), array(0.963, dtype=float32)]\n",
      "Iteration 37220 Training loss 0.008962135761976242 Validation loss 0.012985861860215664 Accuracy 0.8633000254631042 Accuracies by class [array(0.859, dtype=float32), array(0.961, dtype=float32), array(0.821, dtype=float32), array(0.882, dtype=float32), array(0.687, dtype=float32), array(0.939, dtype=float32), array(0.639, dtype=float32), array(0.962, dtype=float32), array(0.961, dtype=float32), array(0.922, dtype=float32)]\n",
      "Iteration 37230 Training loss 0.009404893033206463 Validation loss 0.012662873603403568 Accuracy 0.8658999800682068 Accuracies by class [array(0.741, dtype=float32), array(0.968, dtype=float32), array(0.763, dtype=float32), array(0.878, dtype=float32), array(0.805, dtype=float32), array(0.941, dtype=float32), array(0.713, dtype=float32), array(0.942, dtype=float32), array(0.955, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 37240 Training loss 0.009936163201928139 Validation loss 0.01271437294781208 Accuracy 0.8650000095367432 Accuracies by class [array(0.697, dtype=float32), array(0.969, dtype=float32), array(0.786, dtype=float32), array(0.905, dtype=float32), array(0.803, dtype=float32), array(0.941, dtype=float32), array(0.7, dtype=float32), array(0.934, dtype=float32), array(0.96, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 37250 Training loss 0.006941680796444416 Validation loss 0.01222848892211914 Accuracy 0.871999979019165 Accuracies by class [array(0.854, dtype=float32), array(0.967, dtype=float32), array(0.776, dtype=float32), array(0.901, dtype=float32), array(0.819, dtype=float32), array(0.943, dtype=float32), array(0.605, dtype=float32), array(0.958, dtype=float32), array(0.967, dtype=float32), array(0.93, dtype=float32)]\n",
      "Iteration 37260 Training loss 0.006684499327093363 Validation loss 0.01238241232931614 Accuracy 0.8694000244140625 Accuracies by class [array(0.828, dtype=float32), array(0.968, dtype=float32), array(0.763, dtype=float32), array(0.881, dtype=float32), array(0.783, dtype=float32), array(0.942, dtype=float32), array(0.682, dtype=float32), array(0.922, dtype=float32), array(0.967, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 37270 Training loss 0.008947072550654411 Validation loss 0.01259208470582962 Accuracy 0.8683000206947327 Accuracies by class [array(0.726, dtype=float32), array(0.967, dtype=float32), array(0.784, dtype=float32), array(0.908, dtype=float32), array(0.804, dtype=float32), array(0.944, dtype=float32), array(0.707, dtype=float32), array(0.923, dtype=float32), array(0.963, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 37280 Training loss 0.007306396961212158 Validation loss 0.012380142696201801 Accuracy 0.8704000115394592 Accuracies by class [array(0.848, dtype=float32), array(0.966, dtype=float32), array(0.753, dtype=float32), array(0.9, dtype=float32), array(0.81, dtype=float32), array(0.944, dtype=float32), array(0.627, dtype=float32), array(0.937, dtype=float32), array(0.966, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 37290 Training loss 0.009181268513202667 Validation loss 0.012640794739127159 Accuracy 0.8669000267982483 Accuracies by class [array(0.846, dtype=float32), array(0.966, dtype=float32), array(0.787, dtype=float32), array(0.908, dtype=float32), array(0.816, dtype=float32), array(0.945, dtype=float32), array(0.56, dtype=float32), array(0.919, dtype=float32), array(0.963, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 37300 Training loss 0.008010748773813248 Validation loss 0.012670893222093582 Accuracy 0.8661999702453613 Accuracies by class [array(0.839, dtype=float32), array(0.97, dtype=float32), array(0.802, dtype=float32), array(0.868, dtype=float32), array(0.739, dtype=float32), array(0.947, dtype=float32), array(0.663, dtype=float32), array(0.914, dtype=float32), array(0.965, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 37310 Training loss 0.007541080936789513 Validation loss 0.0122986463829875 Accuracy 0.8707000017166138 Accuracies by class [array(0.864, dtype=float32), array(0.967, dtype=float32), array(0.783, dtype=float32), array(0.897, dtype=float32), array(0.817, dtype=float32), array(0.946, dtype=float32), array(0.587, dtype=float32), array(0.924, dtype=float32), array(0.967, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 37320 Training loss 0.00871035736054182 Validation loss 0.01254558376967907 Accuracy 0.8676000237464905 Accuracies by class [array(0.817, dtype=float32), array(0.971, dtype=float32), array(0.808, dtype=float32), array(0.877, dtype=float32), array(0.711, dtype=float32), array(0.947, dtype=float32), array(0.702, dtype=float32), array(0.927, dtype=float32), array(0.962, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 37330 Training loss 0.009818819351494312 Validation loss 0.012611313723027706 Accuracy 0.8666999936103821 Accuracies by class [array(0.839, dtype=float32), array(0.974, dtype=float32), array(0.791, dtype=float32), array(0.889, dtype=float32), array(0.792, dtype=float32), array(0.944, dtype=float32), array(0.596, dtype=float32), array(0.922, dtype=float32), array(0.96, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 37340 Training loss 0.007853887975215912 Validation loss 0.012784701772034168 Accuracy 0.8654000163078308 Accuracies by class [array(0.841, dtype=float32), array(0.964, dtype=float32), array(0.794, dtype=float32), array(0.928, dtype=float32), array(0.805, dtype=float32), array(0.948, dtype=float32), array(0.525, dtype=float32), array(0.937, dtype=float32), array(0.964, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 37350 Training loss 0.010183867067098618 Validation loss 0.014012313447892666 Accuracy 0.852400004863739 Accuracies by class [array(0.778, dtype=float32), array(0.964, dtype=float32), array(0.799, dtype=float32), array(0.886, dtype=float32), array(0.578, dtype=float32), array(0.94, dtype=float32), array(0.73, dtype=float32), array(0.941, dtype=float32), array(0.956, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 37360 Training loss 0.008671526797115803 Validation loss 0.012366997078061104 Accuracy 0.8698999881744385 Accuracies by class [array(0.792, dtype=float32), array(0.97, dtype=float32), array(0.784, dtype=float32), array(0.889, dtype=float32), array(0.77, dtype=float32), array(0.947, dtype=float32), array(0.698, dtype=float32), array(0.947, dtype=float32), array(0.964, dtype=float32), array(0.938, dtype=float32)]\n",
      "Iteration 37370 Training loss 0.007739232853055 Validation loss 0.01356443203985691 Accuracy 0.8578000068664551 Accuracies by class [array(0.759, dtype=float32), array(0.965, dtype=float32), array(0.662, dtype=float32), array(0.87, dtype=float32), array(0.765, dtype=float32), array(0.949, dtype=float32), array(0.779, dtype=float32), array(0.904, dtype=float32), array(0.963, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 37380 Training loss 0.00946730189025402 Validation loss 0.013422946445643902 Accuracy 0.8597000241279602 Accuracies by class [array(0.87, dtype=float32), array(0.974, dtype=float32), array(0.865, dtype=float32), array(0.871, dtype=float32), array(0.758, dtype=float32), array(0.947, dtype=float32), array(0.466, dtype=float32), array(0.94, dtype=float32), array(0.961, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 37390 Training loss 0.006564772222191095 Validation loss 0.012660235166549683 Accuracy 0.8658000230789185 Accuracies by class [array(0.815, dtype=float32), array(0.97, dtype=float32), array(0.777, dtype=float32), array(0.888, dtype=float32), array(0.736, dtype=float32), array(0.944, dtype=float32), array(0.678, dtype=float32), array(0.948, dtype=float32), array(0.961, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 37400 Training loss 0.00829070433974266 Validation loss 0.012741874903440475 Accuracy 0.8658999800682068 Accuracies by class [array(0.765, dtype=float32), array(0.968, dtype=float32), array(0.73, dtype=float32), array(0.923, dtype=float32), array(0.848, dtype=float32), array(0.948, dtype=float32), array(0.638, dtype=float32), array(0.914, dtype=float32), array(0.96, dtype=float32), array(0.965, dtype=float32)]\n",
      "Iteration 37410 Training loss 0.008411992341279984 Validation loss 0.012709892354905605 Accuracy 0.8651999831199646 Accuracies by class [array(0.825, dtype=float32), array(0.968, dtype=float32), array(0.749, dtype=float32), array(0.867, dtype=float32), array(0.845, dtype=float32), array(0.94, dtype=float32), array(0.625, dtype=float32), array(0.968, dtype=float32), array(0.958, dtype=float32), array(0.907, dtype=float32)]\n",
      "Iteration 37420 Training loss 0.00889210868626833 Validation loss 0.012989495880901814 Accuracy 0.8634999990463257 Accuracies by class [array(0.834, dtype=float32), array(0.968, dtype=float32), array(0.769, dtype=float32), array(0.881, dtype=float32), array(0.863, dtype=float32), array(0.95, dtype=float32), array(0.523, dtype=float32), array(0.926, dtype=float32), array(0.964, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 37430 Training loss 0.007697277702391148 Validation loss 0.01228571031242609 Accuracy 0.8705000281333923 Accuracies by class [array(0.839, dtype=float32), array(0.968, dtype=float32), array(0.773, dtype=float32), array(0.878, dtype=float32), array(0.814, dtype=float32), array(0.946, dtype=float32), array(0.646, dtype=float32), array(0.945, dtype=float32), array(0.955, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 37440 Training loss 0.010223527438938618 Validation loss 0.012760842218995094 Accuracy 0.8658999800682068 Accuracies by class [array(0.749, dtype=float32), array(0.969, dtype=float32), array(0.802, dtype=float32), array(0.913, dtype=float32), array(0.814, dtype=float32), array(0.946, dtype=float32), array(0.619, dtype=float32), array(0.924, dtype=float32), array(0.961, dtype=float32), array(0.962, dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1054c73d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37450 Training loss 0.008315321989357471 Validation loss 0.012920036911964417 Accuracy 0.8626999855041504 Accuracies by class [array(0.761, dtype=float32), array(0.967, dtype=float32), array(0.765, dtype=float32), array(0.897, dtype=float32), array(0.7, dtype=float32), array(0.936, dtype=float32), array(0.733, dtype=float32), array(0.946, dtype=float32), array(0.966, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37460 Training loss 0.007938959635794163 Validation loss 0.01236869115382433 Accuracy 0.8695999979972839 Accuracies by class [array(0.824, dtype=float32), array(0.973, dtype=float32), array(0.753, dtype=float32), array(0.88, dtype=float32), array(0.821, dtype=float32), array(0.958, dtype=float32), array(0.655, dtype=float32), array(0.902, dtype=float32), array(0.97, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 37470 Training loss 0.00859549455344677 Validation loss 0.012344175949692726 Accuracy 0.8690999746322632 Accuracies by class [array(0.82, dtype=float32), array(0.971, dtype=float32), array(0.832, dtype=float32), array(0.857, dtype=float32), array(0.787, dtype=float32), array(0.943, dtype=float32), array(0.632, dtype=float32), array(0.94, dtype=float32), array(0.958, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 37480 Training loss 0.008849872276186943 Validation loss 0.012730115093290806 Accuracy 0.8654000163078308 Accuracies by class [array(0.733, dtype=float32), array(0.971, dtype=float32), array(0.739, dtype=float32), array(0.864, dtype=float32), array(0.812, dtype=float32), array(0.942, dtype=float32), array(0.741, dtype=float32), array(0.95, dtype=float32), array(0.957, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 37490 Training loss 0.009610365144908428 Validation loss 0.013077772222459316 Accuracy 0.8619999885559082 Accuracies by class [array(0.836, dtype=float32), array(0.974, dtype=float32), array(0.827, dtype=float32), array(0.802, dtype=float32), array(0.76, dtype=float32), array(0.945, dtype=float32), array(0.644, dtype=float32), array(0.929, dtype=float32), array(0.951, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 37500 Training loss 0.009942417033016682 Validation loss 0.012513723224401474 Accuracy 0.8675000071525574 Accuracies by class [array(0.777, dtype=float32), array(0.967, dtype=float32), array(0.757, dtype=float32), array(0.911, dtype=float32), array(0.847, dtype=float32), array(0.934, dtype=float32), array(0.634, dtype=float32), array(0.965, dtype=float32), array(0.958, dtype=float32), array(0.925, dtype=float32)]\n",
      "Iteration 37510 Training loss 0.007561855483800173 Validation loss 0.012215222232043743 Accuracy 0.8707000017166138 Accuracies by class [array(0.823, dtype=float32), array(0.965, dtype=float32), array(0.796, dtype=float32), array(0.898, dtype=float32), array(0.79, dtype=float32), array(0.944, dtype=float32), array(0.645, dtype=float32), array(0.946, dtype=float32), array(0.962, dtype=float32), array(0.938, dtype=float32)]\n",
      "Iteration 37520 Training loss 0.008116650395095348 Validation loss 0.012410445138812065 Accuracy 0.8701000213623047 Accuracies by class [array(0.814, dtype=float32), array(0.969, dtype=float32), array(0.785, dtype=float32), array(0.886, dtype=float32), array(0.786, dtype=float32), array(0.951, dtype=float32), array(0.687, dtype=float32), array(0.903, dtype=float32), array(0.959, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 37530 Training loss 0.008339173160493374 Validation loss 0.012327027507126331 Accuracy 0.8684999942779541 Accuracies by class [array(0.816, dtype=float32), array(0.972, dtype=float32), array(0.826, dtype=float32), array(0.86, dtype=float32), array(0.793, dtype=float32), array(0.942, dtype=float32), array(0.624, dtype=float32), array(0.932, dtype=float32), array(0.959, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 37540 Training loss 0.008146014995872974 Validation loss 0.012381001375615597 Accuracy 0.8689000010490417 Accuracies by class [array(0.803, dtype=float32), array(0.971, dtype=float32), array(0.751, dtype=float32), array(0.866, dtype=float32), array(0.806, dtype=float32), array(0.941, dtype=float32), array(0.702, dtype=float32), array(0.927, dtype=float32), array(0.961, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 37550 Training loss 0.007706840988248587 Validation loss 0.01264142245054245 Accuracy 0.8648999929428101 Accuracies by class [array(0.853, dtype=float32), array(0.972, dtype=float32), array(0.811, dtype=float32), array(0.845, dtype=float32), array(0.845, dtype=float32), array(0.947, dtype=float32), array(0.539, dtype=float32), array(0.938, dtype=float32), array(0.958, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 37560 Training loss 0.009471097961068153 Validation loss 0.012622182257473469 Accuracy 0.8665000200271606 Accuracies by class [array(0.859, dtype=float32), array(0.97, dtype=float32), array(0.823, dtype=float32), array(0.923, dtype=float32), array(0.748, dtype=float32), array(0.943, dtype=float32), array(0.547, dtype=float32), array(0.948, dtype=float32), array(0.959, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 37570 Training loss 0.007756417151540518 Validation loss 0.012661588378250599 Accuracy 0.866100013256073 Accuracies by class [array(0.848, dtype=float32), array(0.967, dtype=float32), array(0.853, dtype=float32), array(0.882, dtype=float32), array(0.705, dtype=float32), array(0.948, dtype=float32), array(0.617, dtype=float32), array(0.925, dtype=float32), array(0.962, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 37580 Training loss 0.008306591771543026 Validation loss 0.012289073318243027 Accuracy 0.8708999752998352 Accuracies by class [array(0.832, dtype=float32), array(0.967, dtype=float32), array(0.828, dtype=float32), array(0.904, dtype=float32), array(0.781, dtype=float32), array(0.946, dtype=float32), array(0.602, dtype=float32), array(0.935, dtype=float32), array(0.96, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 37590 Training loss 0.008840023539960384 Validation loss 0.01220910158008337 Accuracy 0.8705999851226807 Accuracies by class [array(0.834, dtype=float32), array(0.97, dtype=float32), array(0.809, dtype=float32), array(0.895, dtype=float32), array(0.742, dtype=float32), array(0.933, dtype=float32), array(0.661, dtype=float32), array(0.946, dtype=float32), array(0.96, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37600 Training loss 0.008879947476089 Validation loss 0.012350285425782204 Accuracy 0.8698999881744385 Accuracies by class [array(0.824, dtype=float32), array(0.971, dtype=float32), array(0.849, dtype=float32), array(0.902, dtype=float32), array(0.792, dtype=float32), array(0.93, dtype=float32), array(0.57, dtype=float32), array(0.956, dtype=float32), array(0.958, dtype=float32), array(0.947, dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1054c73d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37610 Training loss 0.009327318519353867 Validation loss 0.01265751477330923 Accuracy 0.8657000064849854 Accuracies by class [array(0.85, dtype=float32), array(0.97, dtype=float32), array(0.767, dtype=float32), array(0.905, dtype=float32), array(0.836, dtype=float32), array(0.944, dtype=float32), array(0.533, dtype=float32), array(0.934, dtype=float32), array(0.964, dtype=float32), array(0.954, dtype=float32)]\n",
      "Iteration 37620 Training loss 0.007713024970144033 Validation loss 0.012328684329986572 Accuracy 0.8689000010490417 Accuracies by class [array(0.773, dtype=float32), array(0.973, dtype=float32), array(0.811, dtype=float32), array(0.883, dtype=float32), array(0.747, dtype=float32), array(0.943, dtype=float32), array(0.71, dtype=float32), array(0.949, dtype=float32), array(0.96, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 37630 Training loss 0.011239462532103062 Validation loss 0.012489798478782177 Accuracy 0.8672999739646912 Accuracies by class [array(0.818, dtype=float32), array(0.971, dtype=float32), array(0.807, dtype=float32), array(0.848, dtype=float32), array(0.823, dtype=float32), array(0.942, dtype=float32), array(0.629, dtype=float32), array(0.955, dtype=float32), array(0.961, dtype=float32), array(0.919, dtype=float32)]\n",
      "Iteration 37640 Training loss 0.00867452286183834 Validation loss 0.012438159435987473 Accuracy 0.8689000010490417 Accuracies by class [array(0.814, dtype=float32), array(0.971, dtype=float32), array(0.791, dtype=float32), array(0.898, dtype=float32), array(0.837, dtype=float32), array(0.948, dtype=float32), array(0.586, dtype=float32), array(0.919, dtype=float32), array(0.964, dtype=float32), array(0.961, dtype=float32)]\n",
      "Iteration 37650 Training loss 0.007837392389774323 Validation loss 0.012231148779392242 Accuracy 0.8708000183105469 Accuracies by class [array(0.833, dtype=float32), array(0.97, dtype=float32), array(0.764, dtype=float32), array(0.884, dtype=float32), array(0.83, dtype=float32), array(0.946, dtype=float32), array(0.636, dtype=float32), array(0.924, dtype=float32), array(0.963, dtype=float32), array(0.958, dtype=float32)]\n",
      "Iteration 37660 Training loss 0.00935761071741581 Validation loss 0.012480250559747219 Accuracy 0.8679999709129333 Accuracies by class [array(0.786, dtype=float32), array(0.971, dtype=float32), array(0.837, dtype=float32), array(0.872, dtype=float32), array(0.814, dtype=float32), array(0.953, dtype=float32), array(0.609, dtype=float32), array(0.917, dtype=float32), array(0.962, dtype=float32), array(0.959, dtype=float32)]\n",
      "Iteration 37670 Training loss 0.007516276556998491 Validation loss 0.012457096017897129 Accuracy 0.8682000041007996 Accuracies by class [array(0.849, dtype=float32), array(0.969, dtype=float32), array(0.795, dtype=float32), array(0.855, dtype=float32), array(0.795, dtype=float32), array(0.948, dtype=float32), array(0.616, dtype=float32), array(0.939, dtype=float32), array(0.965, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 37680 Training loss 0.00891832821071148 Validation loss 0.012424210086464882 Accuracy 0.8690000176429749 Accuracies by class [array(0.814, dtype=float32), array(0.97, dtype=float32), array(0.804, dtype=float32), array(0.886, dtype=float32), array(0.819, dtype=float32), array(0.947, dtype=float32), array(0.602, dtype=float32), array(0.916, dtype=float32), array(0.97, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 37690 Training loss 0.006004932336509228 Validation loss 0.012522400356829166 Accuracy 0.8687000274658203 Accuracies by class [array(0.792, dtype=float32), array(0.969, dtype=float32), array(0.749, dtype=float32), array(0.885, dtype=float32), array(0.854, dtype=float32), array(0.937, dtype=float32), array(0.634, dtype=float32), array(0.95, dtype=float32), array(0.961, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37700 Training loss 0.006472822744399309 Validation loss 0.012290479615330696 Accuracy 0.8704000115394592 Accuracies by class [array(0.765, dtype=float32), array(0.968, dtype=float32), array(0.793, dtype=float32), array(0.893, dtype=float32), array(0.835, dtype=float32), array(0.943, dtype=float32), array(0.645, dtype=float32), array(0.944, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 37710 Training loss 0.0072861360386013985 Validation loss 0.01244987454265356 Accuracy 0.8691999912261963 Accuracies by class [array(0.775, dtype=float32), array(0.968, dtype=float32), array(0.779, dtype=float32), array(0.886, dtype=float32), array(0.83, dtype=float32), array(0.937, dtype=float32), array(0.665, dtype=float32), array(0.954, dtype=float32), array(0.957, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 37720 Training loss 0.006796411704272032 Validation loss 0.012543274089694023 Accuracy 0.8677999973297119 Accuracies by class [array(0.856, dtype=float32), array(0.966, dtype=float32), array(0.775, dtype=float32), array(0.855, dtype=float32), array(0.833, dtype=float32), array(0.94, dtype=float32), array(0.594, dtype=float32), array(0.94, dtype=float32), array(0.966, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 37730 Training loss 0.010037805885076523 Validation loss 0.012632861733436584 Accuracy 0.866599977016449 Accuracies by class [array(0.706, dtype=float32), array(0.965, dtype=float32), array(0.76, dtype=float32), array(0.899, dtype=float32), array(0.814, dtype=float32), array(0.948, dtype=float32), array(0.726, dtype=float32), array(0.941, dtype=float32), array(0.966, dtype=float32), array(0.941, dtype=float32)]\n",
      "Iteration 37740 Training loss 0.008221035823225975 Validation loss 0.012311147525906563 Accuracy 0.8708000183105469 Accuracies by class [array(0.831, dtype=float32), array(0.969, dtype=float32), array(0.796, dtype=float32), array(0.87, dtype=float32), array(0.79, dtype=float32), array(0.941, dtype=float32), array(0.666, dtype=float32), array(0.921, dtype=float32), array(0.962, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 37750 Training loss 0.00816116388887167 Validation loss 0.012469682842493057 Accuracy 0.8682000041007996 Accuracies by class [array(0.822, dtype=float32), array(0.97, dtype=float32), array(0.795, dtype=float32), array(0.898, dtype=float32), array(0.749, dtype=float32), array(0.944, dtype=float32), array(0.656, dtype=float32), array(0.92, dtype=float32), array(0.966, dtype=float32), array(0.962, dtype=float32)]\n",
      "Iteration 37760 Training loss 0.008970167487859726 Validation loss 0.012536635622382164 Accuracy 0.8676999807357788 Accuracies by class [array(0.804, dtype=float32), array(0.973, dtype=float32), array(0.829, dtype=float32), array(0.856, dtype=float32), array(0.814, dtype=float32), array(0.945, dtype=float32), array(0.6, dtype=float32), array(0.953, dtype=float32), array(0.963, dtype=float32), array(0.94, dtype=float32)]\n",
      "Iteration 37770 Training loss 0.009257834404706955 Validation loss 0.012621314264833927 Accuracy 0.8673999905586243 Accuracies by class [array(0.788, dtype=float32), array(0.969, dtype=float32), array(0.752, dtype=float32), array(0.884, dtype=float32), array(0.731, dtype=float32), array(0.941, dtype=float32), array(0.746, dtype=float32), array(0.953, dtype=float32), array(0.963, dtype=float32), array(0.947, dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1054c73d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37780 Training loss 0.008663647808134556 Validation loss 0.012351729907095432 Accuracy 0.8690999746322632 Accuracies by class [array(0.797, dtype=float32), array(0.971, dtype=float32), array(0.753, dtype=float32), array(0.859, dtype=float32), array(0.842, dtype=float32), array(0.948, dtype=float32), array(0.68, dtype=float32), array(0.916, dtype=float32), array(0.968, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 37790 Training loss 0.008725244551897049 Validation loss 0.013148232363164425 Accuracy 0.8607000112533569 Accuracies by class [array(0.825, dtype=float32), array(0.971, dtype=float32), array(0.702, dtype=float32), array(0.852, dtype=float32), array(0.746, dtype=float32), array(0.948, dtype=float32), array(0.723, dtype=float32), array(0.914, dtype=float32), array(0.966, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 37800 Training loss 0.007862814702093601 Validation loss 0.01279373373836279 Accuracy 0.8644999861717224 Accuracies by class [array(0.743, dtype=float32), array(0.969, dtype=float32), array(0.714, dtype=float32), array(0.885, dtype=float32), array(0.776, dtype=float32), array(0.943, dtype=float32), array(0.766, dtype=float32), array(0.922, dtype=float32), array(0.963, dtype=float32), array(0.964, dtype=float32)]\n",
      "Iteration 37810 Training loss 0.007302068639546633 Validation loss 0.012685849331319332 Accuracy 0.8662999868392944 Accuracies by class [array(0.888, dtype=float32), array(0.97, dtype=float32), array(0.697, dtype=float32), array(0.843, dtype=float32), array(0.818, dtype=float32), array(0.945, dtype=float32), array(0.647, dtype=float32), array(0.939, dtype=float32), array(0.961, dtype=float32), array(0.955, dtype=float32)]\n",
      "Iteration 37820 Training loss 0.008532559499144554 Validation loss 0.012684871442615986 Accuracy 0.8658000230789185 Accuracies by class [array(0.813, dtype=float32), array(0.97, dtype=float32), array(0.749, dtype=float32), array(0.846, dtype=float32), array(0.873, dtype=float32), array(0.937, dtype=float32), array(0.65, dtype=float32), array(0.907, dtype=float32), array(0.944, dtype=float32), array(0.969, dtype=float32)]\n",
      "Iteration 37830 Training loss 0.0073614297434687614 Validation loss 0.012399675324559212 Accuracy 0.8693000078201294 Accuracies by class [array(0.795, dtype=float32), array(0.968, dtype=float32), array(0.76, dtype=float32), array(0.875, dtype=float32), array(0.82, dtype=float32), array(0.946, dtype=float32), array(0.705, dtype=float32), array(0.9, dtype=float32), array(0.96, dtype=float32), array(0.964, dtype=float32)]\n",
      "Iteration 37840 Training loss 0.008497197180986404 Validation loss 0.011952738277614117 Accuracy 0.8748000264167786 Accuracies by class [array(0.837, dtype=float32), array(0.972, dtype=float32), array(0.83, dtype=float32), array(0.881, dtype=float32), array(0.773, dtype=float32), array(0.946, dtype=float32), array(0.651, dtype=float32), array(0.936, dtype=float32), array(0.966, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37850 Training loss 0.008538934402167797 Validation loss 0.012501454912126064 Accuracy 0.8673999905586243 Accuracies by class [array(0.839, dtype=float32), array(0.976, dtype=float32), array(0.735, dtype=float32), array(0.827, dtype=float32), array(0.829, dtype=float32), array(0.937, dtype=float32), array(0.672, dtype=float32), array(0.934, dtype=float32), array(0.965, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 37860 Training loss 0.008034085854887962 Validation loss 0.012140688486397266 Accuracy 0.8715999722480774 Accuracies by class [array(0.831, dtype=float32), array(0.972, dtype=float32), array(0.796, dtype=float32), array(0.858, dtype=float32), array(0.827, dtype=float32), array(0.94, dtype=float32), array(0.627, dtype=float32), array(0.947, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 37870 Training loss 0.008833551779389381 Validation loss 0.01270549651235342 Accuracy 0.8651000261306763 Accuracies by class [array(0.744, dtype=float32), array(0.971, dtype=float32), array(0.767, dtype=float32), array(0.838, dtype=float32), array(0.831, dtype=float32), array(0.939, dtype=float32), array(0.711, dtype=float32), array(0.936, dtype=float32), array(0.958, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37880 Training loss 0.00849972665309906 Validation loss 0.01217760518193245 Accuracy 0.8716999888420105 Accuracies by class [array(0.857, dtype=float32), array(0.965, dtype=float32), array(0.828, dtype=float32), array(0.865, dtype=float32), array(0.779, dtype=float32), array(0.934, dtype=float32), array(0.618, dtype=float32), array(0.949, dtype=float32), array(0.965, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 37890 Training loss 0.008651279844343662 Validation loss 0.012469884939491749 Accuracy 0.8679999709129333 Accuracies by class [array(0.779, dtype=float32), array(0.964, dtype=float32), array(0.714, dtype=float32), array(0.878, dtype=float32), array(0.81, dtype=float32), array(0.929, dtype=float32), array(0.739, dtype=float32), array(0.967, dtype=float32), array(0.963, dtype=float32), array(0.937, dtype=float32)]\n",
      "Iteration 37900 Training loss 0.008805019780993462 Validation loss 0.012673080898821354 Accuracy 0.8662999868392944 Accuracies by class [array(0.824, dtype=float32), array(0.968, dtype=float32), array(0.766, dtype=float32), array(0.86, dtype=float32), array(0.862, dtype=float32), array(0.945, dtype=float32), array(0.585, dtype=float32), array(0.932, dtype=float32), array(0.965, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37910 Training loss 0.0086055314168334 Validation loss 0.012363298796117306 Accuracy 0.8698999881744385 Accuracies by class [array(0.821, dtype=float32), array(0.971, dtype=float32), array(0.724, dtype=float32), array(0.863, dtype=float32), array(0.834, dtype=float32), array(0.946, dtype=float32), array(0.682, dtype=float32), array(0.939, dtype=float32), array(0.963, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37920 Training loss 0.007779431063681841 Validation loss 0.012359682470560074 Accuracy 0.8693000078201294 Accuracies by class [array(0.825, dtype=float32), array(0.971, dtype=float32), array(0.825, dtype=float32), array(0.854, dtype=float32), array(0.781, dtype=float32), array(0.948, dtype=float32), array(0.641, dtype=float32), array(0.939, dtype=float32), array(0.956, dtype=float32), array(0.953, dtype=float32)]\n",
      "Iteration 37930 Training loss 0.008976089768111706 Validation loss 0.012054206803441048 Accuracy 0.8725000023841858 Accuracies by class [array(0.857, dtype=float32), array(0.967, dtype=float32), array(0.769, dtype=float32), array(0.865, dtype=float32), array(0.798, dtype=float32), array(0.944, dtype=float32), array(0.663, dtype=float32), array(0.949, dtype=float32), array(0.963, dtype=float32), array(0.95, dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1054c73d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37940 Training loss 0.0052974107675254345 Validation loss 0.011959446594119072 Accuracy 0.8727999925613403 Accuracies by class [array(0.815, dtype=float32), array(0.967, dtype=float32), array(0.787, dtype=float32), array(0.909, dtype=float32), array(0.812, dtype=float32), array(0.942, dtype=float32), array(0.634, dtype=float32), array(0.948, dtype=float32), array(0.963, dtype=float32), array(0.951, dtype=float32)]\n",
      "Iteration 37950 Training loss 0.007375148590654135 Validation loss 0.011955764144659042 Accuracy 0.8741999864578247 Accuracies by class [array(0.796, dtype=float32), array(0.966, dtype=float32), array(0.819, dtype=float32), array(0.882, dtype=float32), array(0.804, dtype=float32), array(0.946, dtype=float32), array(0.674, dtype=float32), array(0.958, dtype=float32), array(0.959, dtype=float32), array(0.938, dtype=float32)]\n",
      "Iteration 37960 Training loss 0.006770807784050703 Validation loss 0.012051529251039028 Accuracy 0.8733999729156494 Accuracies by class [array(0.82, dtype=float32), array(0.968, dtype=float32), array(0.822, dtype=float32), array(0.851, dtype=float32), array(0.811, dtype=float32), array(0.947, dtype=float32), array(0.652, dtype=float32), array(0.95, dtype=float32), array(0.965, dtype=float32), array(0.948, dtype=float32)]\n",
      "Iteration 37970 Training loss 0.007510675583034754 Validation loss 0.012010135687887669 Accuracy 0.8730999827384949 Accuracies by class [array(0.801, dtype=float32), array(0.971, dtype=float32), array(0.813, dtype=float32), array(0.874, dtype=float32), array(0.827, dtype=float32), array(0.938, dtype=float32), array(0.652, dtype=float32), array(0.933, dtype=float32), array(0.962, dtype=float32), array(0.96, dtype=float32)]\n",
      "Iteration 37980 Training loss 0.0062328209169209 Validation loss 0.012572207488119602 Accuracy 0.8665000200271606 Accuracies by class [array(0.737, dtype=float32), array(0.972, dtype=float32), array(0.8, dtype=float32), array(0.856, dtype=float32), array(0.816, dtype=float32), array(0.948, dtype=float32), array(0.699, dtype=float32), array(0.924, dtype=float32), array(0.957, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 37990 Training loss 0.007010173052549362 Validation loss 0.012608149088919163 Accuracy 0.866100013256073 Accuracies by class [array(0.776, dtype=float32), array(0.972, dtype=float32), array(0.712, dtype=float32), array(0.865, dtype=float32), array(0.881, dtype=float32), array(0.95, dtype=float32), array(0.645, dtype=float32), array(0.932, dtype=float32), array(0.972, dtype=float32), array(0.956, dtype=float32)]\n",
      "Iteration 38000 Training loss 0.00638243043795228 Validation loss 0.012323232367634773 Accuracy 0.8695999979972839 Accuracies by class [array(0.817, dtype=float32), array(0.968, dtype=float32), array(0.8, dtype=float32), array(0.919, dtype=float32), array(0.771, dtype=float32), array(0.949, dtype=float32), array(0.632, dtype=float32), array(0.962, dtype=float32), array(0.967, dtype=float32), array(0.911, dtype=float32)]\n",
      "Iteration 38010 Training loss 0.008456261828541756 Validation loss 0.012422105297446251 Accuracy 0.8676999807357788 Accuracies by class [array(0.845, dtype=float32), array(0.975, dtype=float32), array(0.746, dtype=float32), array(0.823, dtype=float32), array(0.838, dtype=float32), array(0.944, dtype=float32), array(0.657, dtype=float32), array(0.93, dtype=float32), array(0.962, dtype=float32), array(0.957, dtype=float32)]\n",
      "Iteration 38020 Training loss 0.006677061785012484 Validation loss 0.012625134550035 Accuracy 0.8655999898910522 Accuracies by class [array(0.706, dtype=float32), array(0.971, dtype=float32), array(0.808, dtype=float32), array(0.882, dtype=float32), array(0.806, dtype=float32), array(0.947, dtype=float32), array(0.7, dtype=float32), array(0.906, dtype=float32), array(0.964, dtype=float32), array(0.966, dtype=float32)]\n",
      "Iteration 38030 Training loss 0.00873869750648737 Validation loss 0.01225979533046484 Accuracy 0.8705000281333923 Accuracies by class [array(0.867, dtype=float32), array(0.963, dtype=float32), array(0.806, dtype=float32), array(0.871, dtype=float32), array(0.8, dtype=float32), array(0.934, dtype=float32), array(0.596, dtype=float32), array(0.938, dtype=float32), array(0.965, dtype=float32), array(0.965, dtype=float32)]\n",
      "Iteration 38040 Training loss 0.006331668235361576 Validation loss 0.012358054518699646 Accuracy 0.8695999979972839 Accuracies by class [array(0.804, dtype=float32), array(0.965, dtype=float32), array(0.757, dtype=float32), array(0.884, dtype=float32), array(0.861, dtype=float32), array(0.943, dtype=float32), array(0.625, dtype=float32), array(0.95, dtype=float32), array(0.962, dtype=float32), array(0.945, dtype=float32)]\n",
      "Iteration 38050 Training loss 0.008170551620423794 Validation loss 0.011802428402006626 Accuracy 0.8751999735832214 Accuracies by class [array(0.817, dtype=float32), array(0.966, dtype=float32), array(0.785, dtype=float32), array(0.894, dtype=float32), array(0.813, dtype=float32), array(0.937, dtype=float32), array(0.677, dtype=float32), array(0.945, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 38060 Training loss 0.00896566454321146 Validation loss 0.012027624063193798 Accuracy 0.8729000091552734 Accuracies by class [array(0.821, dtype=float32), array(0.963, dtype=float32), array(0.784, dtype=float32), array(0.873, dtype=float32), array(0.815, dtype=float32), array(0.935, dtype=float32), array(0.666, dtype=float32), array(0.959, dtype=float32), array(0.966, dtype=float32), array(0.947, dtype=float32)]\n",
      "Iteration 38070 Training loss 0.009976631961762905 Validation loss 0.012108388356864452 Accuracy 0.8726999759674072 Accuracies by class [array(0.841, dtype=float32), array(0.963, dtype=float32), array(0.763, dtype=float32), array(0.906, dtype=float32), array(0.791, dtype=float32), array(0.94, dtype=float32), array(0.662, dtype=float32), array(0.953, dtype=float32), array(0.964, dtype=float32), array(0.944, dtype=float32)]\n",
      "Iteration 38080 Training loss 0.0075549534521996975 Validation loss 0.012343760579824448 Accuracy 0.8707000017166138 Accuracies by class [array(0.757, dtype=float32), array(0.957, dtype=float32), array(0.757, dtype=float32), array(0.912, dtype=float32), array(0.836, dtype=float32), array(0.94, dtype=float32), array(0.695, dtype=float32), array(0.937, dtype=float32), array(0.964, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 38090 Training loss 0.00825409684330225 Validation loss 0.012640449218451977 Accuracy 0.8664000034332275 Accuracies by class [array(0.843, dtype=float32), array(0.963, dtype=float32), array(0.859, dtype=float32), array(0.902, dtype=float32), array(0.717, dtype=float32), array(0.941, dtype=float32), array(0.595, dtype=float32), array(0.93, dtype=float32), array(0.962, dtype=float32), array(0.952, dtype=float32)]\n",
      "Iteration 38100 Training loss 0.00874111894518137 Validation loss 0.012277054600417614 Accuracy 0.8704000115394592 Accuracies by class [array(0.859, dtype=float32), array(0.965, dtype=float32), array(0.835, dtype=float32), array(0.891, dtype=float32), array(0.781, dtype=float32), array(0.945, dtype=float32), array(0.572, dtype=float32), array(0.938, dtype=float32), array(0.966, dtype=float32), array(0.952, dtype=float32)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_2_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_by_class\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mtwo_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, reg1, reg2, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, observation_by_class)\u001b[39m\n\u001b[32m    136\u001b[39m accuracy_by_class = []\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m classe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.number_of_classes): \n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     correct_class_pred = ((torch.argmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m], dim=\u001b[32m1\u001b[39m) == torch.argmax(y_valid, dim=\u001b[32m1\u001b[39m)) & (torch.argmax(y_valid, dim=\u001b[32m1\u001b[39m) == classe)).sum()\n\u001b[32m    139\u001b[39m     total_class_data = (torch.argmax(y_valid, dim=\u001b[32m1\u001b[39m) == classe).sum()\n\u001b[32m    140\u001b[39m     class_accuracy = correct_class_pred/total_class_data\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mtwo_layer_NN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     z1 = (\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.b1).t() \u001b[38;5;66;03m# shape (n_data, hidden_1_size) # logits layer 1\u001b[39;00m\n\u001b[32m     85\u001b[39m     h1 = ReLU(z1)  \u001b[38;5;66;03m# hidden neurons layer 1\u001b[39;00m\n\u001b[32m     86\u001b[39m     z2 = (torch.mm(\u001b[38;5;28mself\u001b[39m.W2, h1.t()) + \u001b[38;5;28mself\u001b[39m.b2).t() \u001b[38;5;66;03m# shape (n_data, number_of_classes ) # logits layer 2\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 1, 0.01, 10, observation_by_class = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-4, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "\n",
    "date = \"05_05_25_\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = model_2_layer\n",
    "    model_name = \"model_(784+512+10)_save_4\"\n",
    "    save_path = \"Classifiers/\" + date + model_name + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0, np.max([model.training_loss_trajectory, model.validation_loss_trajectory])+0.01)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0, np.max([model.training_loss_trajectory, model.validation_loss_trajectory])+0.01)\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    if model.observation_by_class :\n",
    "        for i in range(len(model.classes_accuracies_trajectories[0])) :\n",
    "            plt.plot(np.linspace(1,len(model.classes_accuracies_trajectories)*model.observation_rate, len(model.classes_accuracies_trajectories)), np.array(model.classes_accuracies_trajectories)[:,i], label = 'Classe ' + str(i))\n",
    "        plt.xlim(0, len(model.classes_accuracies_trajectories)*model.observation_rate)\n",
    "        plt.xlabel('Number of iterations')  \n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.title(\"Losses of the \" + model_name + ' for each class', pad = 20)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "        plt.savefig(save_path + \"figures/Accuracy_of_the_\" + model_name + \"for_each_class.png\", bbox_inches='tight')\n",
    "        plt.savefig(save_path + \"figures/Accuracy_of_the_\" + model_name + \"for_each_class.svg\", bbox_inches='tight')\n",
    "        data_validation = np.column_stack((np.linspace(1,len(model.classes_accuracies_trajectories)*model.observation_rate, len(model.classes_accuracies_trajectories)), np.array(model.classes_accuracies_trajectories)))\n",
    "        np.savetxt(save_path + \"figures/Accuracy_of_the_\" + model_name + \"for_each_class.txt\", data_validation, delimiter=\",\", header=\"N_iter, Accuracy_by_class\")\n",
    "        plt.show()\n",
    "        \n",
    "        kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.classes_accuracies_trajectories)*model.observation_rate, len(model.classes_accuracies_trajectories))]\n",
    "        for i in range(len(model.classes_accuracies_trajectories[0])) :\n",
    "            plt.plot(kappa, np.array(model.classes_accuracies_trajectories)[:, i], label = 'Classe ' + str(i))\n",
    "        plt.xlim(0, np.max(kappa))\n",
    "        plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.title(\"Accuracy of the \" + model_name + ' for each class', pad = 20)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "        plt.savefig(save_path + \"figures/kappa_accuracy_of_the_\" + model_name + \"for_each_class.png\", bbox_inches='tight')\n",
    "        plt.savefig(save_path + \"figures/kappa_accuracy_of_the_\" + model_name + \"for_each_class.svg\", bbox_inches='tight')\n",
    "        data_validation = np.column_stack((kappa, np.array(model.classes_accuracies_trajectories)))\n",
    "        np.savetxt(save_path + \"figures/kappa_accuracy_of_the_\" + model_name + \"for_each_class.txt\", data_validation, delimiter=\",\", header=\"kappa, Accuracy_by_class\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c992bf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 5)\n",
      "(130,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(model.classes_accuracies_trajectories))\n",
    "print(np.shape(np.array(model.classes_accuracies_trajectories)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "tensor(0.8692)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "model_name = \"model_2_layer_trained_save_1\"\n",
    "assessed_model = torch.load(\"datas/models/model_2_layer_save_1.pt\", weights_only=False)\n",
    "\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plots of performances\n",
    "accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(accuracy)\n",
    "kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_accuracy)\n",
    "loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(loss)\n",
    "kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_loss)\n",
    "plt.show()\n",
    "\n",
    "# Import datas\n",
    "accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, esp_init = 1, fraction_batch=0.01):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = esp_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = esp_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # # hidden neurons layer 2\n",
    "        z3 = torch.mm(self.W3, h2.t() + self.b3)\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=0.2, fraction_batch=1e-2, train_layer1=True, train_layer2=True, train_layer3=True):\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utilisés pour l'apprentissage de la première couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            # Suivi de l'apprentissage\n",
    "            if i % 100 == 0:\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2, dim=0) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                overall_training_loss = torch.mean(training_loss,dim=0)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2, dim=0)\n",
    "                overall_validation_loss = torch.mean(validation_loss,dim=0)\n",
    "                self.training_loss_trajectory.append(overall_training_loss.item())\n",
    "                self.validation_loss_trajectory.append(overall_validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", overall_training_loss.item(), \"Validation loss\", overall_validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/self.eps_init**2 + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "            self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/self.eps_init**2 + self.reg1*self.b1)\n",
    "            self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/self.eps_init**2 +self.reg2*self.W2)\n",
    "            self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/self.eps_init**2 + self.reg2*self.b2)\n",
    "        return \"Training done\"\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
